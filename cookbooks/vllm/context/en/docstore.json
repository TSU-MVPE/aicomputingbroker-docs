{"docstore/metadata": {"691180ad-7cdf-4476-b462-2714812fbd0e": {"doc_hash": "d2fdcaa05701ccb338ac0daca18c19c1b01ab6d8c7137d49bf23939437c98012"}, "47d6c819-b135-4dc0-aa6d-6edc67a647d7": {"doc_hash": "fd617125b3d48a36ed175e5436c70ada570d768ada8f236ad6795971147fb8a5"}, "44e2d0fa-d1b0-4586-931a-849602cdf3ce": {"doc_hash": "680b7ab4f2cdab058d913ee3ba86a8a8d66b9f882f06c02f7d9d46eb799ab4ca"}, "3a52219e-1667-4dfe-a30b-afbf8e704a4c": {"doc_hash": "20cef1a63d2bcbb2eaf194005f3402c9b72c073006cda9fccd48512934b77409"}, "0dece9a4-6d84-444a-9884-b3d575ccdad2": {"doc_hash": "65684c56039eec4aa34020bf16869d6ad26860c8ef03e7e2f2bb1edb7778465f"}, "cb258cd1-a5fa-4cb5-a918-b7f7e4881546": {"doc_hash": "110573d9e85093b856ed77779e90cb9119bf8acf8adf5e435d90d7b0417120ec"}, "278d5fe0-bda6-4d4b-82c8-2be316c79a18": {"doc_hash": "302d8c53eb6870ec296ce5435f37bc3e87628ddd5e2ac119cc4fd25a7bdaf0e6"}, "77d45487-e5e7-45d9-98bc-8164e984c349": {"doc_hash": "43b59b3138f00abf9ed1bb22fb76164cdb70580919200b2e0319d17472aa6640"}, "68350c4f-4b00-4ce3-a5d4-c7d08358cc3e": {"doc_hash": "2dda53a0afb9d65bb7f22fd3c0b1e6aa967b0b1860aaf25f64eb4a8718094dab"}, "42aa9509-ded0-4cda-806a-58372745ca09": {"doc_hash": "63c69228df0ec27a6ac394a780dfbf657070d97eb0233324f918d8940c88d06f"}, "a47b251f-b26e-4a12-aac7-73a0ce566c73": {"doc_hash": "b3ad754721c075ee20ca4cf9c77537a433c7a54afb9ccaa644fa4b6e2b6041b7"}, "20b36c96-53fb-4bb0-a950-7d59b22852e2": {"doc_hash": "3be03bae24ab463eed74e66f171655cc2974b34c7d39c9db109cc32d9417ec14"}, "41e6e3d2-698d-4f82-a07b-5c9cadbc75cb": {"doc_hash": "cf536d721c75f4c2ef1148db8b39da21894468c3e443a4aae50bebec470a6c76"}, "4116d067-14ed-4677-a599-5612229bff32": {"doc_hash": "b4e3183fb836cf0671bee160faf57cf3f0adb6e23a5e852656390ed032fa7110"}, "e038894b-1615-4281-9ecb-e33bbbebebfa": {"doc_hash": "eceb5a3ea32733790007c2683cce4d98658b50ba7f24d764b2b942db32c294db"}, "fdaa40cf-e999-489f-8aa2-67f42a846d70": {"doc_hash": "630a0098a9d2488382695f7a2e257650dd32bd493aa5aed8b876d2d1c6f48ce3"}, "aefc5c38-7eb2-48f2-86c4-9f7d28e01f88": {"doc_hash": "0160dc5d55d50ec703b62a6e58ab4e9a87cebc2512065c4904101015c2d97762"}, "e5927151-5713-45ad-83e4-432b2fba4efc": {"doc_hash": "0c29d1674845f8ed405a7fe091afe27d256bb1c29e33b4ed14526686c586abc1"}, "c05a48b2-a873-4943-9d72-b28dab3ee63a": {"doc_hash": "b0be2482c37a8f5c27d8d3876a1cb9ba0ddcfb72e7be588959949994b9fa7755"}, "d16ffbae-d972-4d57-9044-77eae7472cfc": {"doc_hash": "fe36f1fa03114985cc4b6101b74185438b16f9c40fe23509f737221764e89038"}, "52813620-09c3-47ef-913b-3d57785beabb": {"doc_hash": "7b82db80fa96baad9f79bb83707ea8b464c9e989cbcb55b532f4cbba5bf96722"}, "3b104bc7-67b4-4738-9cda-049342fd3c5d": {"doc_hash": "ccd0542d8756b2d0580c7f1df058a1571f5380cdf3c9524c9fe73f7567bdcba1"}, "ed83d7ca-0484-4a88-8080-20adca13d219": {"doc_hash": "0d1f7c2215fedd788079cd7d803bff64075e24a7d4441bb14fe8d8981017544c"}, "b144f4c2-946c-460f-877b-48f1eefb650b": {"doc_hash": "f854ecea181d46c630c5e0922fab11fd9a1f79530d00a080fbe3653cd17d5374"}, "98fd8831-f1fe-4eca-82e5-1ee272dc1cf9": {"doc_hash": "e9ef5f81158e373c618c41caa7fd4e73d262b36b5ecea394fd70b27f8a12018f"}, "83f890e2-6cdb-4bc1-ac16-7978f0810ae6": {"doc_hash": "a2b46013b954b7d3b934c53bff2262e4f2417ce5aeed1c1b055d1af3357a6501"}, "62c6c232-1fc0-4495-861c-f86ea7b02995": {"doc_hash": "1b099a06055a86958840d6cbf8948aae5454700a8c2d55beb910ff0f34397eb1"}, "572be251-e7be-4921-955d-9d7d5f43c7b9": {"doc_hash": "d16bf3649fadaf774f4b75d3f3fa58255f3241c87934212e83d16ddedb4a6059"}, "d5ae36ab-74bd-446b-bb39-039678271148": {"doc_hash": "5a054972b69bca6d76a39d08d3b74cc0ce16ab6f5962612affb3862e2e160d4c"}, "f5c95b2a-21cb-4229-808a-f2e5537b8ec5": {"doc_hash": "0b594b14cd94ea80292b6f9d6507fc0caaa4f5d03645e8a032561b0e41760d92"}, "fe135c7a-9559-4e21-be3c-6a2e3d0521df": {"doc_hash": "bad2d70bba2b63a2c964f8aec92e12ec6321b9442c986ba23e1ded814e10cd18"}, "2c63b837-af89-4512-a1b2-c7e5240eea66": {"doc_hash": "414b3d555577ae88a2f8460450c352f2f01bbbfe409344bdcf6057243889e352"}, "920225d9-772f-4231-88c3-606c50af4422": {"doc_hash": "828ce0f1e82ff9428a3141eb37dff54833518eb2acaf1a5dddc17ca032b94414"}, "25822ed2-3fd4-4f7a-a44c-364dc4b57f81": {"doc_hash": "22d85657bf53b8aaf296a2e90c4386aa2d0be4dce68d529841b302b489d86c28"}, "b1771bfa-ffc5-41ff-958b-ad96a2245318": {"doc_hash": "11db459d2a1d3e84e2b85834cde34538c0d014c536324703c14ecc8da3400f3c"}, "3388b4ad-8628-4a4b-973f-f2d7ccfdc36c": {"doc_hash": "b8f1bf2239e01c615872f9744a8459cd2f3f1c9380b22bd35300b90abeacd8e4"}, "ba1363df-32c9-497b-9a5d-5f64be45a202": {"doc_hash": "1ae8762a555ca1bb2e203c67006c3246fa663f92f7b2053b762a414a433efd47"}, "5f7ce314-e135-4612-b2bd-56ac4dbbf240": {"doc_hash": "1e4a134256a47135e72e86a5248192742a9105d89cb5f5150ff2f4136a2a6bbc"}, "470d1af1-7d71-4406-abed-d364be33f663": {"doc_hash": "b19484ea07a49c639c40183b821587673fb6c3b0d2b8c60e944514881ebb151a"}, "e6b31287-bcb9-4839-a2cc-f550f81c94e5": {"doc_hash": "161b0009f75ec3144850b1021ac00cfab0ef351b9810ee33968ac822f84738c0"}, "d48c1980-9844-4ce9-8c6e-8df97b4883e4": {"doc_hash": "0df21714f306e6313d3eaa0553f015c21b8ee31d139f5add0889d33e86632c67"}, "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1": {"doc_hash": "c402dfa7da29d0e01bfe03bdfaab2ff844be3dcc43a01a024cd8346899d7927d"}, "da123fb6-ee9c-45a6-8e7f-e8f9290d65d7": {"doc_hash": "947b3908e39c9f35fd129f1e989d0a6bc94f12687f42514cb5f6ed28fcf6ca70"}, "eb34edf8-c25e-46d7-a531-11f983ea4170": {"doc_hash": "278b2dc12f77b8444a099931ea9e498cbcfbd4358c823b2131b8f1927c2f13da"}, "2f54b5e3-a742-4300-b922-a452e619fa54": {"doc_hash": "c76a49f3d6f858fca1a8c5182a187ada9b0a296d37aafeeb99faf1bdca922e11"}, "c02b50c7-064b-4259-9278-a76e907ba61d": {"doc_hash": "d2ffdc3ad2eaa47c9caea88da50e06a27c9171614a334eaa17316127d81bf9ef"}, "db58c062-1e57-4dcd-bd76-db1b404f9680": {"doc_hash": "2d2cba12cce6fecc99db5e3262f403f62bc34bbc12ab40c2cc1e26a781443045"}, "ac0a55f6-c5fa-4ea3-aabe-9efba2577c94": {"doc_hash": "b04b4f29f2fa317cef8be99d27c4c60c14b08d81191766d913b86fccdd9f95b0"}, "6e58335f-84cd-49e6-b87b-866a440c29c8": {"doc_hash": "ba530e726581e17861f74f800032ed59a8d2f6a56390ae68bdd4c26f9622f5ef"}, "e3c470a4-352c-4a1f-8c71-efaee67bc11f": {"doc_hash": "c98ddcda5de9aedc9c8f9bbb1094101b5f3c5d3b75b8ddc18085553473059823"}, "95dcb29d-bae4-48ec-bbe7-0f6336652c2f": {"doc_hash": "d2fdcaa05701ccb338ac0daca18c19c1b01ab6d8c7137d49bf23939437c98012", "ref_doc_id": "691180ad-7cdf-4476-b462-2714812fbd0e"}, "38677a7e-f076-44e2-b1f3-dd3b69718dda": {"doc_hash": "fd617125b3d48a36ed175e5436c70ada570d768ada8f236ad6795971147fb8a5", "ref_doc_id": "47d6c819-b135-4dc0-aa6d-6edc67a647d7"}, "6c94d557-b880-4a05-a33c-767ab3ed7446": {"doc_hash": "680b7ab4f2cdab058d913ee3ba86a8a8d66b9f882f06c02f7d9d46eb799ab4ca", "ref_doc_id": "44e2d0fa-d1b0-4586-931a-849602cdf3ce"}, "d77a5302-11d9-4d78-85e0-964589f9e738": {"doc_hash": "20cef1a63d2bcbb2eaf194005f3402c9b72c073006cda9fccd48512934b77409", "ref_doc_id": "3a52219e-1667-4dfe-a30b-afbf8e704a4c"}, "68c9ed5e-fbac-4d9e-b806-05ba551eac6e": {"doc_hash": "65684c56039eec4aa34020bf16869d6ad26860c8ef03e7e2f2bb1edb7778465f", "ref_doc_id": "0dece9a4-6d84-444a-9884-b3d575ccdad2"}, "4901744f-39a1-4f7a-9b1f-48bcd8169715": {"doc_hash": "110573d9e85093b856ed77779e90cb9119bf8acf8adf5e435d90d7b0417120ec", "ref_doc_id": "cb258cd1-a5fa-4cb5-a918-b7f7e4881546"}, "ffd918f9-dd59-4a94-8141-a8e0ab87bbdf": {"doc_hash": "302d8c53eb6870ec296ce5435f37bc3e87628ddd5e2ac119cc4fd25a7bdaf0e6", "ref_doc_id": "278d5fe0-bda6-4d4b-82c8-2be316c79a18"}, "fe7177a3-dd92-43a9-9d4d-ee03f665a94d": {"doc_hash": "96e333c739cadea1988884edf498d4b4afd536197ccaae419451873ffc1493a8", "ref_doc_id": "77d45487-e5e7-45d9-98bc-8164e984c349"}, "41c9f3da-a8ad-452c-a688-994fec6994d3": {"doc_hash": "2dda53a0afb9d65bb7f22fd3c0b1e6aa967b0b1860aaf25f64eb4a8718094dab", "ref_doc_id": "68350c4f-4b00-4ce3-a5d4-c7d08358cc3e"}, "8ff541db-98d6-4bb8-a843-9671e9dbf4d9": {"doc_hash": "63c69228df0ec27a6ac394a780dfbf657070d97eb0233324f918d8940c88d06f", "ref_doc_id": "42aa9509-ded0-4cda-806a-58372745ca09"}, "6f5259ac-ee27-4e4d-acb3-749386b83e55": {"doc_hash": "b3ad754721c075ee20ca4cf9c77537a433c7a54afb9ccaa644fa4b6e2b6041b7", "ref_doc_id": "a47b251f-b26e-4a12-aac7-73a0ce566c73"}, "d5a86808-ba4e-4bb1-9f23-20ad29459d94": {"doc_hash": "f9659850bf401e95783ef30260b51d8ab3bc02299c46334daa669115f9b79630", "ref_doc_id": "20b36c96-53fb-4bb0-a950-7d59b22852e2"}, "01e1fb78-0777-40a9-8d7e-a3b7c5a2cb24": {"doc_hash": "3d59b1df102c81d13b6966f5ed9cf6f30ca352165db2b7bb74983fa4ffd65af0", "ref_doc_id": "41e6e3d2-698d-4f82-a07b-5c9cadbc75cb"}, "1d840031-7153-4418-b266-efc54b0772b6": {"doc_hash": "b4e3183fb836cf0671bee160faf57cf3f0adb6e23a5e852656390ed032fa7110", "ref_doc_id": "4116d067-14ed-4677-a599-5612229bff32"}, "b7d31b2e-5a23-4da7-b5a9-2df4c6fdf35c": {"doc_hash": "470ef611bc532395421508b1d3a2b43f0b2f2c2453c0a7f60e09be4979657c2a", "ref_doc_id": "e038894b-1615-4281-9ecb-e33bbbebebfa"}, "f4facc20-e577-4fff-b7f2-4bb5bb923822": {"doc_hash": "3c16653fa448b1fbe62a76d9fe60349a5fb912fc67ae5d5b0e2889db64894446", "ref_doc_id": "fdaa40cf-e999-489f-8aa2-67f42a846d70"}, "4ea0cc3c-2d27-45a0-962a-9273aa57817d": {"doc_hash": "e82af64826e6c79a019eb36053216147a245fc075004cf68cf7dff46b2e02023", "ref_doc_id": "aefc5c38-7eb2-48f2-86c4-9f7d28e01f88"}, "db430269-d8d0-46ce-8afa-fe1d14045444": {"doc_hash": "3b2d39ca30d14d21325c2eba10e527f6138611d8321debf63a566905eab23eeb", "ref_doc_id": "e5927151-5713-45ad-83e4-432b2fba4efc"}, "1636df94-ca6d-4f0f-9a73-fa2bbdc88c51": {"doc_hash": "6309d470b0ab0774df9c03b618fe51b862fd1e92f4ec126270091cb47d551a12", "ref_doc_id": "c05a48b2-a873-4943-9d72-b28dab3ee63a"}, "d77387e8-a24e-4dac-b0f9-165961280b79": {"doc_hash": "0886404174d8607ee73a327d21a4962224f692392a76a9e348fda81ed54766e6", "ref_doc_id": "d16ffbae-d972-4d57-9044-77eae7472cfc"}, "82a21c62-ea2d-4e29-8e9c-bd054dd6a5b5": {"doc_hash": "bca9afe5244e1760aadc2513b2eb51481547848db9ab386977c9fc2d4f98b8eb", "ref_doc_id": "52813620-09c3-47ef-913b-3d57785beabb"}, "1ae4c6eb-0265-4e6e-82a3-0eed2e606052": {"doc_hash": "ecb36687c0078e54212a1014025ad26d2f5111a86ae75d1231d0bffc999b6f1e", "ref_doc_id": "3b104bc7-67b4-4738-9cda-049342fd3c5d"}, "24e80683-9d05-4b37-9f1f-55eac55b23f0": {"doc_hash": "a0623cc19adceadec460a7446c0038a46cad5b85e8e56785f777bf66e89b16ee", "ref_doc_id": "ed83d7ca-0484-4a88-8080-20adca13d219"}, "3e52f508-0760-4222-bbd5-6210a0a1d47e": {"doc_hash": "9990938aab2448c1c8d700e5d937ad8b090681885218d90d3ae48a3b4a9043da", "ref_doc_id": "b144f4c2-946c-460f-877b-48f1eefb650b"}, "d7194299-65eb-4f09-aa09-91be2162a33c": {"doc_hash": "e9ef5f81158e373c618c41caa7fd4e73d262b36b5ecea394fd70b27f8a12018f", "ref_doc_id": "98fd8831-f1fe-4eca-82e5-1ee272dc1cf9"}, "f93b0a6d-bb07-48a1-bf54-60b79f725505": {"doc_hash": "d14cd8721e14ce145f3a07d4e2d1f8d4ffaedd6511cc2e7cf865de1be176cc28", "ref_doc_id": "83f890e2-6cdb-4bc1-ac16-7978f0810ae6"}, "3dd06500-42c7-4a11-a4d4-9532edd2999f": {"doc_hash": "eb3be5b2bc4d5bbae1540160f2a89aac3c81e9ae53fc9a80c4d723da6cec9844", "ref_doc_id": "62c6c232-1fc0-4495-861c-f86ea7b02995"}, "f9788c3a-5102-4c6e-baae-68bbd7842bff": {"doc_hash": "d16bf3649fadaf774f4b75d3f3fa58255f3241c87934212e83d16ddedb4a6059", "ref_doc_id": "572be251-e7be-4921-955d-9d7d5f43c7b9"}, "47332d78-290d-449f-b119-d012c50dba89": {"doc_hash": "5a054972b69bca6d76a39d08d3b74cc0ce16ab6f5962612affb3862e2e160d4c", "ref_doc_id": "d5ae36ab-74bd-446b-bb39-039678271148"}, "a871bd97-be48-4423-869e-7cf90a54ad37": {"doc_hash": "af1ffdaa92e61b3638545f642d9b3687881e14ba863521c8a96e90c91e36a46b", "ref_doc_id": "f5c95b2a-21cb-4229-808a-f2e5537b8ec5"}, "e3849e30-c6c3-4327-8300-f5904ed9e6b5": {"doc_hash": "bad2d70bba2b63a2c964f8aec92e12ec6321b9442c986ba23e1ded814e10cd18", "ref_doc_id": "fe135c7a-9559-4e21-be3c-6a2e3d0521df"}, "773a9127-14be-4b85-8280-ed1b24ccacea": {"doc_hash": "414b3d555577ae88a2f8460450c352f2f01bbbfe409344bdcf6057243889e352", "ref_doc_id": "2c63b837-af89-4512-a1b2-c7e5240eea66"}, "12df4507-f05a-414e-890e-be4767cb20da": {"doc_hash": "10278c7441d81ea9cb040692a5851e5933460926aecf81baf63830c2352f1aad", "ref_doc_id": "920225d9-772f-4231-88c3-606c50af4422"}, "e0e01798-8da1-49bb-be5a-131c9be9a216": {"doc_hash": "996294681a80588eaaa8e486c04553f66d9e0677b14c354e49eef479406da5f4", "ref_doc_id": "25822ed2-3fd4-4f7a-a44c-364dc4b57f81"}, "264b3773-fcab-4fc3-9654-9a5f9d624ec0": {"doc_hash": "11db459d2a1d3e84e2b85834cde34538c0d014c536324703c14ecc8da3400f3c", "ref_doc_id": "b1771bfa-ffc5-41ff-958b-ad96a2245318"}, "d534646f-2c79-459d-ba12-6b807ec42780": {"doc_hash": "b8f1bf2239e01c615872f9744a8459cd2f3f1c9380b22bd35300b90abeacd8e4", "ref_doc_id": "3388b4ad-8628-4a4b-973f-f2d7ccfdc36c"}, "3a7ebce8-d84f-490f-aa6f-690e84ffda2b": {"doc_hash": "aeda0c5f7e896c7b1337f4d5f89bc9e906c69cab6c3921a582564f7fad4364fd", "ref_doc_id": "ba1363df-32c9-497b-9a5d-5f64be45a202"}, "bef9d067-a125-43fa-8093-01c509efd727": {"doc_hash": "1e4a134256a47135e72e86a5248192742a9105d89cb5f5150ff2f4136a2a6bbc", "ref_doc_id": "5f7ce314-e135-4612-b2bd-56ac4dbbf240"}, "7098a2cf-fd19-46b8-b8b5-034e65ea3b47": {"doc_hash": "3c4b3e84b3dc3a9b618593b9898268598b7cc000c99ef01baf9eba0ff152d8ed", "ref_doc_id": "470d1af1-7d71-4406-abed-d364be33f663"}, "6031395d-1495-4d34-9cff-ae6430745cc7": {"doc_hash": "7b29702b79337b3f90245741969d926366658962a665a2aaddca99452903ba81", "ref_doc_id": "e6b31287-bcb9-4839-a2cc-f550f81c94e5"}, "bf9cb4b1-e1fa-4fba-9c7c-dafa5f63e101": {"doc_hash": "d397f732603841d94f73bda072b8bbccbd5a03c96721bd075935f036c30a2d52", "ref_doc_id": "d48c1980-9844-4ce9-8c6e-8df97b4883e4"}, "bcf5be8e-a206-40c8-87f6-bf04762786e0": {"doc_hash": "97216163459adeda7740efa200786c36890534e53e252aa12d9cb07455416d40", "ref_doc_id": "d48c1980-9844-4ce9-8c6e-8df97b4883e4"}, "82a580ed-4242-47c9-858e-5744f124bbc7": {"doc_hash": "66e6e8fab9d8f9dc63af173bbc83b77898cdd75f38d34e57530a14f0a9976257", "ref_doc_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1"}, "0a72e86e-e4ed-458a-b674-670a4f78cf91": {"doc_hash": "395190fd5d309f44efd645a00b4292ba6ba55d70650a167de2d29f9f52cf6aec", "ref_doc_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1"}, "9115c532-d149-49c3-b641-745a5efe420c": {"doc_hash": "672a1ef2b300435d5ecc8e67e089082f6b85c22b5677780ff3cc9ca3581cac19", "ref_doc_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1"}, "a177d4fd-bdd6-4d7d-9b52-7c7656caeb5c": {"doc_hash": "947b3908e39c9f35fd129f1e989d0a6bc94f12687f42514cb5f6ed28fcf6ca70", "ref_doc_id": "da123fb6-ee9c-45a6-8e7f-e8f9290d65d7"}, "acb70555-da42-4a40-a114-606a5345ed08": {"doc_hash": "278b2dc12f77b8444a099931ea9e498cbcfbd4358c823b2131b8f1927c2f13da", "ref_doc_id": "eb34edf8-c25e-46d7-a531-11f983ea4170"}, "65028548-74a4-4472-af54-a128df403df5": {"doc_hash": "c76a49f3d6f858fca1a8c5182a187ada9b0a296d37aafeeb99faf1bdca922e11", "ref_doc_id": "2f54b5e3-a742-4300-b922-a452e619fa54"}, "2439518b-dfdc-4052-87be-d5f47493bd16": {"doc_hash": "d2ffdc3ad2eaa47c9caea88da50e06a27c9171614a334eaa17316127d81bf9ef", "ref_doc_id": "c02b50c7-064b-4259-9278-a76e907ba61d"}, "99c3a82f-35f5-4565-b89e-4fd64e5954e9": {"doc_hash": "e615b2815b7612e73c82fe28c72ff66bfb674fa74cd0555404a162714acb72a1", "ref_doc_id": "db58c062-1e57-4dcd-bd76-db1b404f9680"}, "334c6bac-e505-4cd3-8fbf-d81c1cca6a27": {"doc_hash": "b04b4f29f2fa317cef8be99d27c4c60c14b08d81191766d913b86fccdd9f95b0", "ref_doc_id": "ac0a55f6-c5fa-4ea3-aabe-9efba2577c94"}, "a80b651c-dd5c-4df7-ad72-f828a566f456": {"doc_hash": "a07763266114f7fb680edd69b1f4c9101ceb639b722ff6f90051c9b5db1058d0", "ref_doc_id": "6e58335f-84cd-49e6-b87b-866a440c29c8"}, "21722102-7c6e-42e1-9542-f593dc1d0e0a": {"doc_hash": "c98ddcda5de9aedc9c8f9bbb1094101b5f3c5d3b75b8ddc18085553473059823", "ref_doc_id": "e3c470a4-352c-4a1f-8c71-efaee67bc11f"}}, "docstore/data": {"95dcb29d-bae4-48ec-bbe7-0f6336652c2f": {"__data__": {"id_": "95dcb29d-bae4-48ec-bbe7-0f6336652c2f", "embedding": null, "metadata": {"page_label": "1", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "691180ad-7cdf-4476-b462-2714812fbd0e", "node_type": "4", "metadata": {"page_label": "1", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d2fdcaa05701ccb338ac0daca18c19c1b01ab6d8c7137d49bf23939437c98012", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n1 / 50 Copyright 2025 Fujitsu LimitedOverview of AI Computing Broker (A CB)\nFeatures of A CB\nTechnology for switching between GPU and CPU computational processing in real-time during AI learning and\ninference.\nValue of ACB\nImpr oved GPU Utilization  \u00a0 - Dynamically controls GPU allocation according to the usage status\nduring program execution to minimize GPU idle time. \u00a0 - Enables more processing to be executed with\nthe same GPU resources, thereby reducing overall processing time.\nMaximized Concurr ent Ex ecution o f AI Pr ocesses R equir ing GPU Memor y \u00a0 - Allows sharing of GPU\namong programs while maximizing memory usage, supporting large model processing used in\ngenerative AI and LLM development.\nMinimal Changes  \u00a0 - Changes to user programs are minimal, requiring only a few lines of modification.\nTechnical Features\nAnalyzes the content of computations using the GPU and automatically allocates resources only to\nprocesses that require the GPU.\nUnlike hardware virtualization technology, programs can utilize the maximum memory capacity of the\nGPU.\nOverall S tructure and Component Description", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38677a7e-f076-44e2-b1f3-dd3b69718dda": {"__data__": {"id_": "38677a7e-f076-44e2-b1f3-dd3b69718dda", "embedding": null, "metadata": {"page_label": "2", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47d6c819-b135-4dc0-aa6d-6edc67a647d7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "fd617125b3d48a36ed175e5436c70ada570d768ada8f236ad6795971147fb8a5", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n2 / 50 Copyright 2025 Fujitsu LimitedOverall S tructure \u00a0 - Consists of driver commands, libraries, and GPU allocation servers. \u00a0 - By applying\nthe API provided by A CB's library to user programs, GPUs can be dynamically allocated by A CB. \u00a0\u00a0\u00a0 -\nExecute user programs through driver commands.\nComponents \u00a0 - User Program \u00a0\u00a0\u00a0 - AI applications using deep learning frameworks (PyT orch, PyT orch\nLightning, or T ensorFlow). \u00a0\u00a0\u00a0 - R equires the use of APIs provided by A CB within the program. \u00a0 - Driver\nCommand (agarun) \u00a0\u00a0\u00a0 - Command to execute user programs using A CB. \u00a0 - Library (Adaptive GPU\nAllocator) \u00a0\u00a0\u00a0 - Library that automatically handles device queries and settings by specifying GPU device\nusage sections within the program. \u00a0 - GPU Allocation Server (GPU Assigner) \u00a0\u00a0\u00a0 - Server that manages\nthe processes controlled by A CB and schedules the GPUs allocated to these processes.\nNotes and Limitations\nWhen executing jobs using multiple GPUs or multiple nodes, the following limitations apply: \u00a0 -\nSupports PyT orch's Distributed Data P arallel (DDP). \u00a0 - Supports multi-process but not multi-thread. \u00a0 -\nOnly supports startup via torchrun. \u00a0 - When using 2 or more GPUs per node, only\nGPUAffinityScheduler  is supported.\nDoes not support GPUs other than those manufactured by NVIDIA.\nDoes not support K eras 3. Therefore, it does not support T ensorFlow 2.16 and later versions that adopt\nKeras 3.\nACB improves GPU utilization by distributing learning and inference processes to the GPU and pre-\nprocessing and post-processing to the CPU. Therefore, if pre-processing and post-processing that can\nbe calculated without using the GPU are not included, A CB will not be effective.\nInstallation and Uninstallation Methods\nSupported V ersions\nRecommended Environment\nThe recommended environment is as follows:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c94d557-b880-4a05-a33c-767ab3ed7446": {"__data__": {"id_": "6c94d557-b880-4a05-a33c-767ab3ed7446", "embedding": null, "metadata": {"page_label": "3", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44e2d0fa-d1b0-4586-931a-849602cdf3ce", "node_type": "4", "metadata": {"page_label": "3", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "680b7ab4f2cdab058d913ee3ba86a8a8d66b9f882f06c02f7d9d46eb799ab4ca", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n3 / 50 Copyright 2025 Fujitsu LimitedOS \u00a0 - Ubuntu 22.04.04 L TS, Kernel: 6.5.0-26-generic\nGPU Driver \u00a0 - Driver V ersion: 535.171.04, CUD A Version: 12.2\nPython \u00a0 - Python V ersion: 3.10 \u00a0 - Python Library \u00a0\u00a0\u00a0 - lightning: 2.2.1 \u00a0\u00a0\u00a0 - torch: 2.2.1 \u00a0\u00a0\u00a0 - tensorflow:\n2.15.0.post1 \u00a0\u00a0\u00a0 - keras: 2.15.0\nVerified Environment\nThe environment that has been verified is as follows:\nOS \u00a0 - Ubuntu 22.04.03 L TS, Kernel: 6.2.0-39-generic \u00a0 - Ubuntu 22.04.04 L TS, Kernel: 6.5.0-26-generic \u00a0 -\nCentOS Linux 7.9.2009, K ernel: 3.10.0-957\nGPU Driver \u00a0 - Driver V ersion: 535.161.07, 535.171.04, CUD A Version: 12.2 \u00a0 - Driver V ersion: 550.54.14,\nCUDA Version: 12.4 \u00a0 - Driver V ersion: 470.57.02, CUD A Version: 11.4\nPython \u00a0 - Python V ersion: 3.10.2, 3.10.9, 3.10.12, 3.11.9, 3.12.2 \u00a0 - Python Library \u00a0\u00a0\u00a0 - lightning: 2.2.0,\n2.2.0.post0, 2.2.1, 2.2.2 \u00a0\u00a0\u00a0 - torch: 2.1.2, 2.2.1, 2.2.2+cu118 \u00a0\u00a0\u00a0 - tensorflow: 2.15.0.post1 \u00a0\u00a0\u00a0 - keras: 2.15.0\nInstallation Instructions\nIn the following steps, $WORKDIR  refers to the installation directory for the A CB component.\n1. Prepare and activate a Python virtual environment.\n2. Download adaptive-gpu-allocator.zip  and extract it into the installation directory.\ncd $WORKDIR  \nunzip adaptive-gpu-allocator.zip  \n3. Change the working directory to the adaptive-gpu-allocator  directory.\ncd adaptive-gpu-allocator  \n4. Install A CB.\npip install -e .  \nUninstallation Instructions\nFollow these steps to uninstall:\n1. Uninstall A CB.\npip uninstall adaptive_gpu_allocator  \n2. Delete A CB-related directories.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d77a5302-11d9-4d78-85e0-964589f9e738": {"__data__": {"id_": "d77a5302-11d9-4d78-85e0-964589f9e738", "embedding": null, "metadata": {"page_label": "4", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a52219e-1667-4dfe-a30b-afbf8e704a4c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "20cef1a63d2bcbb2eaf194005f3402c9b72c073006cda9fccd48512934b77409", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n4 / 50 Copyright 2025 Fujitsu Limitedrm -rf $HOME/.acb \nsudo rm -rf /tmp/aga-*  \nsudo rm -rf /var/ log/acb \nrm -rf $WORKDIR\n(Optional) Setup Procedure Using Docker\nRefer to (Supplement) Setup Procedure Using Docker .\nUsage\nStarting GPU Assigner\nGPU Assigner is automatically installed with the adaptive_gpu_allocator  package. Before running a user\nprogram that supports A CB, start gpu_assigner . Start it using the CLI with gpu_assigner start .\n$ gpu_assigner start                     # Start GPU assigner  \n \n> INFO: Successfully started GPU Assigner on 127.0.0.1:11234  \ngpu_assigner status  Confirm that it has been started\n$ gpu_assigner status              # Show current GPU assigner status  \n \n> INFO: GPU assigner is running on 127.0.0.1:11234 (pid=13412)  \nGPU Device Selection Function\nWhen multiple GPUs are installed, you can specify the GPU device that A CB will use. By default, GPU Assigner\nallocates all installed GPU devices to the user program. T o specify the GPU devices targeted by GPU Assigner,\nuse the --gpu-list GPU device number list  argument with gpu_assigner start  to specify the GPU\ndevice numbers to be used. The GPU device numbers are those displayed by the nvidia-smi  command.\nWhen specifying multiple GPU devices, separate the GPU device numbers with commas, like \"0,1,2,...\".\nExample: When using GPU devices 0, 1, and 2.\n$ gpu_assigner start --gpu-list 0,1,2  \nMulti-Node Job Execution Function\nWhen the --multinode  option is specified, multi-node jobs can be executed. When specified, the multi-node\nconfiguration file ( $HOME/.acb/multinode.toml ) is loaded.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68c9ed5e-fbac-4d9e-b806-05ba551eac6e": {"__data__": {"id_": "68c9ed5e-fbac-4d9e-b806-05ba551eac6e", "embedding": null, "metadata": {"page_label": "5", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dece9a4-6d84-444a-9884-b3d575ccdad2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "65684c56039eec4aa34020bf16869d6ad26860c8ef03e7e2f2bb1edb7778465f", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n5 / 50 Copyright 2025 Fujitsu Limitedgpu_assigner --multinode start  \nMulti-Node Configuration File ( multinode.toml )\nTo execute multi-node jobs, the multi-node configuration file ( multinode.toml ) needs to be placed on each\nnode.\nLocation: \u00a0 $HOME/.acb/  on each node\nHow to write the configuration file\n[controller0]  \ntype = \"controller\"  \nipaddr = \"xxxx\"  \nlogfile = \"/home/user/.acb/gpu-assigner.controller0.log\"  \npidfile = \"/home/user/.acb/gpu-assigner.controller0.pid\"  \nport = 11234  \n \n[executor0]  \ntype = \"executor\"  \nipaddr = \"yyyy\"  \nlogfile = \"/home/user/.acb/gpu-assigner.executor0.log\"  \npidfile = \"/home/user/.acb/gpu-assigner.executor0.pid\"  \nport = 11235  \n[controller0], [executor0]  : Node name ( hostname ) \u00a0 - type  : Node type ( controller ,\nexecutor ) \u00a0\u00a0\u00a0 - controller  : Aggregates requests from each executor  and schedules them. Only one\ncan be set. \u00a0\u00a0\u00a0 - executor  : Processes requests from the controller  for GPU information, sends its own\nGPU information, and executes jobs scheduled by the controller . \u00a0 - ipaddr  : IP address \u00a0 - logfile  :\nDirectory and filename of the log file \u00a0 - pidfile  : Directory and filename of the pid file \u00a0 - port  : Port\nnumber\nScheduler\nsimple (1 job occupies 1 GPU)\nBy default, GPU Assigner allocates GPUs using a simple First-In First-Out (FIFO) scheduling method called\n\"simple,\" which does not share GPUs among multiple jobs.\ngpu-shar ing (multiple jobs shar e 1 GPU)\nGPU Assigner also optionally supports a scheduling method called \"gpu-sharing,\" which allocates multiple\njobs to a single GPU within the permissible memory capacity. T o use the gpu-sharing scheduler, specify --\nscheduler gpu-sharing  as an argument to gpu_assigner start  to start GPU Assigner.\ngpu-affinity (GPU Shar ing for Multi-GPU Jobs)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4901744f-39a1-4f7a-9b1f-48bcd8169715": {"__data__": {"id_": "4901744f-39a1-4f7a-9b1f-48bcd8169715", "embedding": null, "metadata": {"page_label": "6", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb258cd1-a5fa-4cb5-a918-b7f7e4881546", "node_type": "4", "metadata": {"page_label": "6", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "110573d9e85093b856ed77779e90cb9119bf8acf8adf5e435d90d7b0417120ec", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n6 / 50 Copyright 2025 Fujitsu LimitedThe gpu-affinity scheduler is a scheduler designed for multi-GPU jobs. It allocates GPUs based on FIFO\nscheduling, but it also supports a Backfill feature  that executes jobs to maximize GPU utilization when GPUs\nare available, without adhering strictly to the FIFO policy. Furthermore, the gpu-affinity scheduler supports a\nGPU Affinity feature  to prevent GPU migration during job execution, which increases memory consumption\noverhead. This feature allows setting a list of GPUs that a job should use (GPU Affinity) and allocates GPUs to\njobs based on this affinity, thereby achieving high GPU utilization efficiency and efficient memory\nmanagement. T o use the gpu-affinity scheduler, start the GPU Assigner with the --scheduler gpu-affinity\nargument, like this: gpu_assigner start --scheduler gpu-affinity .\nMaximum Memory Consumption Prediction Feature\nIn schedulers like gpu-sharing that pack multiple jobs onto a single GPU, it is necessary to adjust the memory\nconsumption of jobs so that it does not exceed the GPU's onboard memory capacity. Therefore, A CB has a\nfeature to predict the memory consumption of a job based on its execution state.\nMulti-GPU Process Support\nGPU Assigner supports multi-process user programs that utilize multiple GPUs.\nBackfill Feature\nThe Backfill feature is a function to improve GPU utilization efficiency while maintaining FIFO fairness as much\nas possible. When the Backfill feature is enabled, the scheduler basically allocates GPUs to jobs using the FIFO\npolicy. However, even when GPUs are available, it uses the Execution Time Prediction Feature  to predict the\nnext GPU allocation time based on the FIFO policy. Assuming that the GPUs will be free until that time, it\nallocates GPUs to jobs that can be executed within that time. By doing so, it becomes possible to improve\nGPU utilization efficiency while maintaining the fairness of GPU allocation based on the FIFO policy.\nExecution Time Pr ediction Featur e\nEstimates the maximum execution time for each job based on the history of execution times.\nGPU Affinity Feature\nThe GPU Affinity feature enables high GPU utilization efficiency and effective memory management.\nWhen the GPU Affinity feature is enabled, multi-GPU jobs will always be assigned to the GPUs specified by\nGPU Affinity after the initial scheduling.\nThis ensures that only the GPUs specified by GPU Affinity retain the runtime context in memory, and even if\nthe scheduler switches between jobs, the same context is consistently used.\nAs a result, memory consumption can be reduced.\nFor single-GPU jobs, GPU Affinity is also considered, just like with multi-GPU jobs.\nHowever, if the Runtime Context Memory Management Feature  determines that the GPU has sufficient free\nmemory, the job can be assigned to a GPU regardless of GPU Affinity.\nRuntime Cont ext Memor y Management Featur e\nMonitors the amount of runtime context memory within GPU memory.\nIf the context memory is below a certain threshold, it is considered that there is sufficient free memory.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffd918f9-dd59-4a94-8141-a8e0ab87bbdf": {"__data__": {"id_": "ffd918f9-dd59-4a94-8141-a8e0ab87bbdf", "embedding": null, "metadata": {"page_label": "7", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "278d5fe0-bda6-4d4b-82c8-2be316c79a18", "node_type": "4", "metadata": {"page_label": "7", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "302d8c53eb6870ec296ce5435f37bc3e87628ddd5e2ac119cc4fd25a7bdaf0e6", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n7 / 50 Copyright 2025 Fujitsu Limitedgpu_assigner Arguments\nGPU Assigner Options Help: gpu_assigner -h  \u00a0 Displays a list of GPU Assigner options.\n$ gpu_assigner -h  \nusage: gpu_assigner [-h]  \n                    [--address ADDRESS]  \n                    [--port PORT]  \n                    [--pid-file-path PID_FILE_PATH]  \n                    {start,status,stop} ...  \n \npositional arguments:  \n  {start,status,stop}   GPU Assigner commands  \n    start               start GPU Assigner  \n    status              show the status of GPU Assigner  \n    stop                stop GPU Assigner  \n \noptions:  \n  -h, --help            show this help message and exit  \n \nGPU Assigner options:  \n  --address ADDRESS     GPU Assigner address (default: 127.0.0.1)  \n  --port PORT           GPU Assigner port (default: 11234)  \n \nPid File options:  \n  --pid-file-path PID_FILE_PATH  \n                        pid file path (default: $HOME/.acb/gpu-assigner.pid)  \nChanging the address used by GPU Assigner ( --address ) \u00a0 The default address used by GPU Assigner\nis 127.0.0.1 . \u00a0 To use a different address, set it with the --address  option at runtime. \u00a0 When setting,\nspecify it before the start /status /stop  commands\n$ gpu_assigner --address \u30a2\u30c9\u30ec\u30b9  [start|status|stop]  \nChanging the port number used by GPU Assigner ( --port ) \u00a0 The default port number used by GPU\nAssigner is 11234 . \u00a0 To use a different port number, set it with the --port  option at runtime. \u00a0 When\nsetting, specify it before the start /status /stop  commands.\n$ gpu_assigner --port \u30dd\u30fc\u30c8 \u756a\u53f7  [start|status|stop]  \nFurthermore, when executing a user program using A CB, set the port number to the environment variable\nAGA_GPU_ALLOC_SERVER_PORT  and run agarun . For details, refer to the following.\nSpecifying the PID file ( --pid-file-path ) A PID file is created to manage the process ID and process state of\nthe GPU Assigner. By default, $HOME/.acb/gpu-assigner.pid  is created. T o change the path of the PID file,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe7177a3-dd92-43a9-9d4d-ee03f665a94d": {"__data__": {"id_": "fe7177a3-dd92-43a9-9d4d-ee03f665a94d", "embedding": null, "metadata": {"page_label": "8", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77d45487-e5e7-45d9-98bc-8164e984c349", "node_type": "4", "metadata": {"page_label": "8", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "43b59b3138f00abf9ed1bb22fb76164cdb70580919200b2e0319d17472aa6640", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n8 / 50 Copyright 2025 Fujitsu Limitedspecify it with the runtime option --pid-file-path . If specified, it should be done before the\nstart /status /stop  commands.\n$ gpu_assigner --pid-file-path PID file path  \n [start|status|stop]  \nArguments for gpu_assigner start\nIf specified, it should be done after the start  command.\nHelp for the arguments of the gpu_assigner start  command: gpu_assigner start -h  \u00a0 A list of\narguments for gpu_assigner start  will be displayed.\n$ gpu_assigner start -h  \n  usage: gpu_assigner start [-h] [--exe-est-start-count EXE_EST_START_COUNT]  \n                            [--exe-est-latest-count EXE_EST_LATEST_COUNT] [-\n-exe-est-interval EXE_EST_INTERVAL]  \n                            [--exe-est-percentile EXE_EST_PERCENTILE]  \n                            [--exe-est-default-execution-time  \nEXE_EST_DEFAULT_EXECUTION_TIME]  \n                            [--gpu-assigner-working-directory  \nGPU_ASSIGNER_WORKING_DIRECTORY]  \n                            [--gpu-assigner-max-workers  \nGPU_ASSIGNER_MAX_WORKERS]  \n                            [--gpu-assigner-execution-time  \nGPU_ASSIGNER_EXECUTION_TIME] [--log-path LOG_PATH] [-v]  \n                            [--scheduler SCHEDULER] [--gpu-list GPU_LIST]  \n                            [--mem-est-start-count MEM_EST_START_COUNT] [--\nmem-est-interval MEM_EST_INTERVAL]  \n                            [--mem-est-data-max-len MEM_EST_DATA_MAX_LEN]  \n                            [--mem-est-percentile MEM_EST_PERCENTILE] [--\nmem-est-fixed-ratio MEM_EST_FIXED_RATIO]  \n                            [--mem-est-select-strategy  \nMEM_EST_SELECT_STRATEGY]  \n                            [--res-mem-manager-resident-memory-threshold  \nRES_MEM_MANAGER_RESIDENT_MEMORY_THRESHOLD]  \n \n  options:  \n    -h, --help            show this help message and exit  \n \n  Execution Time Estimator (exe-est) options:  \n    --exe-est-start-count EXE_EST_START_COUNT  \n                          the count at which the estimator starts (default:  \n10) \n    --exe-est-latest-count EXE_EST_LATEST_COUNT  \n                          the latest count of execution time which the  \nestimator uses. (default: 100)  \n    --exe-est-interval EXE_EST_INTERVAL  \n                          the interval at which the estimator updates  \n(default: 10)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41c9f3da-a8ad-452c-a688-994fec6994d3": {"__data__": {"id_": "41c9f3da-a8ad-452c-a688-994fec6994d3", "embedding": null, "metadata": {"page_label": "9", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68350c4f-4b00-4ce3-a5d4-c7d08358cc3e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "2dda53a0afb9d65bb7f22fd3c0b1e6aa967b0b1860aaf25f64eb4a8718094dab", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n9 / 50 Copyright 2025 Fujitsu Limited    --exe-est-percentile EXE_EST_PERCENTILE  \n                          the percentile used by the estimator. (default:  \n0.9) \n    --exe-est-default-execution-time EXE_EST_DEFAULT_EXECUTION_TIME  \n                          set the default execution time of a job with  \nunknown execution time (msec). (default: 0)  \n \n  GPU Assigner options:  \n    --gpu-assigner-working-directory GPU_ASSIGNER_WORKING_DIRECTORY  \n                          working directory of GPU Assigner (default: None)  \n    --gpu-assigner-max-workers GPU_ASSIGNER_MAX_WORKERS  \n                          max workers of GPU Assigner (default: 128)  \n    --gpu-assigner-execution-time GPU_ASSIGNER_EXECUTION_TIME  \n                          Execution time of GPU Assigner (default: None)  \n \n  Log options:  \n    --log-path LOG_PATH   log path (default: $HOME/.acb/gpu-assigner.log)  \n    -v, --verbose         verbose logging (default: False)  \n \n  Scheduler options:  \n    --scheduler SCHEDULER  \n                          scheduler type (default: simple)  \n    --gpu-list GPU_LIST   allowed GPU device indices (default: None)  \n \n  Peak Memory Estimator (mem-est) options:  \n    --mem-est-start-count MEM_EST_START_COUNT  \n                          the count at which the estimator starts (default:  \n10) \n    --mem-est-interval MEM_EST_INTERVAL  \n                          the interval at which the estimator updates  \n(default: 10)  \n    --mem-est-data-max-len MEM_EST_DATA_MAX_LEN  \n                          the latest count of peak memory usage which the  \nestimator uses. (default: 100)  \n    --mem-est-percentile MEM_EST_PERCENTILE  \n                          the percentile used by the estimator. (default:  \n0.99) \n    --mem-est-fixed-ratio MEM_EST_FIXED_RATIO  \n                          the ratio by which the estimator multiply the max  \npeak memory usage. (default: 1.1)  \n    --mem-est-select-strategy MEM_EST_SELECT_STRATEGY  \n                          whether to use max or min for calculating the  \nallocation memory from user-given and  \n                          system-detected memories (default: min)  \n \n  Resident memory manager options:  \n    --res-mem-manager-resident-memory-threshold  \nRES_MEM_MANAGER_RESIDENT_MEMORY_THRESHOLD  \n                          set the resident memory threshold of GPU memory  \n(default: 0.25)  \nSpecifying the Log File ( --log-path )", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ff541db-98d6-4bb8-a843-9671e9dbf4d9": {"__data__": {"id_": "8ff541db-98d6-4bb8-a843-9671e9dbf4d9", "embedding": null, "metadata": {"page_label": "10", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42aa9509-ded0-4cda-806a-58372745ca09", "node_type": "4", "metadata": {"page_label": "10", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "63c69228df0ec27a6ac394a780dfbf657070d97eb0233324f918d8940c88d06f", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n10 / 50 Copyright 2025 Fujitsu LimitedThe log file created by GPU Assigner is $HOME/.acb/gpu-assigner.log  by default. T o change the log file\npath, specify it with the --log-path  option at runtime.\n$ gpu_assigner start -- log-path path_to_log_file  \nSpecifying the Scheduler ( --scheduler ) \u00a0 Argument to specify the type of scheduler. \u00a0 - simple :\nDefault scheduler. Specify when all jobs are single GPU jobs. \u00a0 - gpu-sharing : Specify this scheduler\nwhen all jobs are single GPU jobs and the memory consumption of the jobs is smaller than the memory\nsize of one GPU.\n$ gpu_assigner start --scheduler gpu-sharing  \ngpu-affinity : Specify this scheduler when executing multi-GPU jobs.\n$ gpu_assigner start --scheduler gpu-affinity  \nSpecifying GPU Devices ( --gpu-list )\nAn argument to specify the GPUs to use. P ass a comma-separated list of GPU numbers. The\ndefault is the GPU numbers of all GPUs.\n$ gpu_assigner start --gpu-list 0,1,2  \nExecution Time Prediction Feature Options\n--exe-est-default-execution-time <time:int> : Maximum execution time (msec) used\nwhen the job's execution time is not registered (default: 0)\nUsed to find backfillable jobs when the execution time of a job scheduled from the FIFO\nqueue is unknown.\n--exe-est-start-count <start_count:int> : The number of data points to start estimating\nthe maximum execution time (default: 10).\n--exe-est-latest-count <latest_count:int> : The number of recent data points to use for\nmaximum execution time estimation (default: 100).\n--exe-est-interval <interval:int> : The number of data points to update the maximum\nexecution time estimate (default: 10).\n--exe-est-percentile <percentile:float> : The percentile value for maximum execution\ntime estimation (default: 0.9).\nRuntime Context Memory Management Feature Options\n--res-mem-manager-resident-memory-threshold <threshold:float> : Threshold for\nlimiting GPU migration of single-GPU jobs. The ratio of runtime context memory amount in GPU", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f5259ac-ee27-4e4d-acb3-749386b83e55": {"__data__": {"id_": "6f5259ac-ee27-4e4d-acb3-749386b83e55", "embedding": null, "metadata": {"page_label": "11", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a47b251f-b26e-4a12-aac7-73a0ce566c73", "node_type": "4", "metadata": {"page_label": "11", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b3ad754721c075ee20ca4cf9c77537a433c7a54afb9ccaa644fa4b6e2b6041b7", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n11 / 50 Copyright 2025 Fujitsu Limitedmemory (default: 0.25).\nIf the amount of context memory in GPU memory is less than the threshold, allocation\nignoring GPU Affinity is possible.\nUser Program Changes\nThis document explains how to modify user programs to run with A CB.\nACB dynamically allocates the device (CPU or GPU) to execute each process of the user program. There are\ntwo methods for switching devices: version a. without restarting the user program (a. no-restart version) and\nversion b. requiring a restart (b. restart-required version). The b. restart-required version requires processes\nsuch as saving and restoring data in memory, which results in slower processing speed compared to the a. no-\nrestart version. However, due to the framework specifications, some frameworks cannot use the a. no-restart\nversion.\nCurrently, only the PyT orch framework supports the a. no-restart\na. No-Restart Version\nIn the a. no-restart version of A CB, the user program does not need to be restarted when switching the device\n(CPU or GPU) executing each process. Therefore, you do not need to consider saving and restoring data or\nskipping initial processes, which are necessary in the b. restart-required version.\nThe specific modification methods are explained using sample programs as examples. For the training\nprogram, the explanation is based on the PyT orch framework. For preprocessing, inference, and\npostprocessing parts, the explanation uses a pseudo sample program as an example.\nPyTorch (a. No-R estar t Version)\nThe method to rewrite a training program written using PyT orch to run with the a. no-restart version of A CB is\nexplained in two types (single GPU program version and multi-GPU program version).\nRewriting PyTorch Training Program (Single GPU Program Version)\nUsing the sample program ( samples/no_restart/sample_pytorch.py ) as an example, this section explains\nhow to rewrite a training program using PyT orch. For the usage of A CB's API for PyT orch, please refer to the\nsubsequent sections.\nImport PyTorchAdaptiveGPUAllocator  (line 14) 2. Import ensure_manual_mode_without_restart  (line\n15) 3. Execute ensure_manual_mode_without_restart()  (line 18) \u00a0\u00a0\u00a0 * This function checks if the\nenvironment variables for no-restart mode are set correctly to prevent running with missing settings. 4. Make\naga a global variable (lines 67, 112) \u00a0\u00a0\u00a0 * In this sample program, the variable aga is made global to be visible\nfrom within the train_step()  function. \u00a0\u00a0\u00a0 * Modify the global variable or function arguments according to\nthe structure of the user program. 5. Specify the starting point of the GPU processing within the epoch\ntraining loop using on_device_begin()  (line 70) 6. Send the data used in the training loop to the device\nusing move_tensor_to_device()  (lines 76, 77) 7. R eturn necessary data to the CPU and delete data on the\nGPU device \u00a0\u00a0\u00a0 * R eturn data to the CPU to prevent it from being lost during GPU/CPU switching (line 91) \u00a0\u00a0\u00a0 *\nDelete data on the GPU device to avoid leaving it behind (lines 92, 93) 8. R etrieve the current device\ninformation if necessary (line 96) 9. Specify the ending point of the GPU processing using on_device_end()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5a86808-ba4e-4bb1-9f23-20ad29459d94": {"__data__": {"id_": "d5a86808-ba4e-4bb1-9f23-20ad29459d94", "embedding": null, "metadata": {"page_label": "12", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20b36c96-53fb-4bb0-a950-7d59b22852e2", "node_type": "4", "metadata": {"page_label": "12", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "3be03bae24ab463eed74e66f171655cc2974b34c7d39c9db109cc32d9417ec14", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n12 / 50 Copyright 2025 Fujitsu Limited(line 100) 10. Initialize PyTorchAdaptiveGPUAllocator  (line 113) \u00a0\u00a0\u00a0 * Initialize after defining the model,\noptimizer, scheduler, etc. 11. Finally, specify aga.finalize()  to end the A CB process (line 122)\nSample Program ( samples/no_restart/sample_pytorch.py )\n\"\"\"A sample of user's deep learning program for Adaptive GPU Allocator(AGA).  \n \nThe statements commented by [AGA] are assumed to be added to allow users  \nto utilize AGA functions.  \n\"\"\" \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport random \nimport numpy as np \n \n# [AGA] import AGA\nfrom adaptive_gpu_allocator.pytorch import PyTorchAdaptiveGPUAllocator  \nfrom adaptive_gpu_allocator.env import ensure_manual_mode_without_restart  \n \n# [AGA] Ensure running in manual AGA mode (without restart)  \nensure_manual_mode_without_restart()  \n \n \n# Define the network architecture\nclass Net(nn.Module) : \n    def __init__ (self): \n        super(Net, self).__init__()  \n        self.fc1 = nn.Linear( 20, 10)  # Input layer  \n        self.fc3 = nn.Linear( 10, 10) \n        self.fc2 = nn.Linear( 10, 1)  # Output layer  \n \n    def forward(self, x) : \n        x = torch.relu(self.fc1(x))  \n        for i in range(20): \n            x = torch.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n \ndef fix_seeds (): \n    # fix seeds for reproducibility  \n    seed = 0 \n    random.seed(seed)  \n    np.random.seed(seed)  \n    torch.manual_seed(seed)  \n    torch.backends.cudnn.benchmark = False \n    torch.backends.cudnn.deterministic = True", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01e1fb78-0777-40a9-8d7e-a3b7c5a2cb24": {"__data__": {"id_": "01e1fb78-0777-40a9-8d7e-a3b7c5a2cb24", "embedding": null, "metadata": {"page_label": "13", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41e6e3d2-698d-4f82-a07b-5c9cadbc75cb", "node_type": "4", "metadata": {"page_label": "13", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "cf536d721c75f4c2ef1148db8b39da21894468c3e443a4aae50bebec470a6c76", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n13 / 50 Copyright 2025 Fujitsu Limiteddef get_training_data (): \n    # Assume we have some data in X_train and y_train  \n    X_train = torch.randn( 10000000 , 20)  # 10000 samples, 20 features each  \n    y_train = torch.randn( 10000000 , 1)  # 10000 samples, 1 target value each  \n    return X_train, y_train  \n \n \ndef init_model_and_optimizer (): \n    # Create the network  \n    net = Net()  \n \n    # Define a Loss function and optimizer  \n    optimizer = optim.Adam(net.parameters())  \n \n    return net, optimizer  \n \n \ndef train_step (model, optimizer, X_train, y_train, epoch) : \n    # [AGA] Define aga object as global variable to make sure that  \n    # modifications to original program is *insertion only*.  \n    global aga \n \n    # [AGA] notify the beginning of device use to AGA  \n    aga.on_device_begin()  \n \n    # Zero the gradients  \n    optimizer.zero_grad()  \n \n    # [AGA] transfor training data to device  \n    X = aga.move_tensor_to_device(X_train)  \n    y = aga.move_tensor_to_device(y_train)  \n \n    # Forward pass  \n    outputs = model(X)  \n \n    # Calculate loss  \n    criterion = nn.MSELoss()  \n    loss = criterion(outputs, y)  \n \n    # Backward pass and optimization  \n    loss.backward()  \n    optimizer.step()  \n \n    # [AGA] move back (to CPU) or delete tensors on GPU device  \n    loss = loss.cpu()  \n    del outputs  \n    del X, y \n \n    # Print loss every 10 epochs  \n    gpuid = aga.get_device()  \n    print( f\"Epoch {epoch}, Loss: {loss.item()}  on {gpuid}\") \n \n    # [AGA] notify the end of device use to AGA  \n    aga.on_device_end()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d840031-7153-4418-b266-efc54b0772b6": {"__data__": {"id_": "1d840031-7153-4418-b266-efc54b0772b6", "embedding": null, "metadata": {"page_label": "14", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4116d067-14ed-4677-a599-5612229bff32", "node_type": "4", "metadata": {"page_label": "14", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b4e3183fb836cf0671bee160faf57cf3f0adb6e23a5e852656390ed032fa7110", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n14 / 50 Copyright 2025 Fujitsu Limited \n \ndef main(): \n    fix_seeds()  \n \n    # init model and aga client  \n    net, optimizer = init_model_and_optimizer()  \n \n    # [AGA] initialize AGA  \n    # Once we register model and optimizer to AGA,  \n    # they are moved to running device at `on_device_begin()`  \n    global aga \n    aga = PyTorchAdaptiveGPUAllocator(net, optimizer)  \n \n    X_train, y_train = get_training_data()  \n \n    n_epoch = 20 \n    for epoch in range(n_epoch):  \n        train_step(net, optimizer, X_train, y_train, epoch)  \n \n    # [AGA] finalize AGA  \n    aga.finalize()  \n \n \nif __name__ == \"__main__\" : \n    main()  \nModifying PyTorch Training Program (Multi-GPU Program Version)\nUsing the Distributed Data P arallel sample program provided in the PyT orch tutorial ( multigpu_torchrunpy )\nas an example, this section explains how to modify the training program. The sample program is launched\nusing torchrun. The modified sample program is located at\nsamples/no_restart/multi_gpu/multigpu_torchrun_aga.py . Refer to the subsequent section for the\nusage of the A CB API for PyT orch.\n(The following \"old\" refers to multigpu_torchrun.py , and \"new\" refers to multigpu_torchrun_aga.py )\n1. Import PyTorchDDPAdaptiveGPUAllocator  (new line 21)\n2. Remove lines specifying GPU such as set_device , to, device_ids , map_location , etc. (old line 14,\nold line 27 (new line 39), old line 37 (new line 49), old lines 40-41 (new line 52))\n3. Leave the backend argument of init_process_group()  empty (old line 15, new line 26). This prepares\nboth NC CL for GPU and GL OO for CPU, enabling necessary memory transfers between GPU and CPU in\nACB.\n4. Use local_rank  for process identification in DDP (new lines 38, 68, 97)\n5. Make aga a global variable (new line 120) \u00a0\u00a0\u00a0 * In this sample program, the variable aga is made global\nto be referenced within _run_epoch()  and train()  functions. \u00a0\u00a0\u00a0 * Adjust the global variable or\nfunction arguments as needed based on the structure of the user program.\n6. Specify the starting point of GPU processing within the epoch training loop using on_device_begin()\n(new line 71)\n7. Use move_tensor_to_device  instead of to (new lines 76-77)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7d31b2e-5a23-4da7-b5a9-2df4c6fdf35c": {"__data__": {"id_": "b7d31b2e-5a23-4da7-b5a9-2df4c6fdf35c", "embedding": null, "metadata": {"page_label": "15", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e038894b-1615-4281-9ecb-e33bbbebebfa", "node_type": "4", "metadata": {"page_label": "15", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "eceb5a3ea32733790007c2683cce4d98658b50ba7f24d764b2b942db32c294db", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n15 / 50 Copyright 2025 Fujitsu Limited8. Delete unnecessary variables before on_device_end  to release allocated memory on the GPU (new\nlines 63, 80)\n9. Specify the endpoint of GPU processing using on_device_end()  (new line 82)\n10. Load the latest model as it is moved by on_device_begin  and on_device_end  (new lines 73, 84)\n11. Initialize PyTorchDDPAdaptiveGPUAllocator  (new line 121)\n12. After using A GA, specify aga.finalize()  before destroy_process_group  to end A CB processing\n(new line 126)\nShow the differences before and after the modification.\n$ diff -u -U 1 multigpu_torchrun.py multigpu_torchrun_aga.py  \n--- multigpu_torchrun.py  \n+++ multigpu_torchrun_aga.py  \n@@ -1,18 +1,30 @@  \n+# [AGA] import AGA  \n+from adaptive_gpu_allocator.pytorch_ddp import PyTorchDDPAdaptiveGPUAllocator  \n+ \n  \n def ddp_setup (): \n-    torch.cuda.set_device(int(os.environ[ \"LOCAL_RANK\" ])) \n-    init_process_group(backend= \"nccl\") \n+    # [AGA] backend needs to be None to prepare both NCCL and GLOO backend  \n+    init_process_group()  \n+ \n  \n class Trainer: \n     def __init__ ( \n@@ -23,8 +35,8 @@ \n         save_every: int,  \n         snapshot_path: str,  \n     ) -> None: \n-        self.gpu_id = int(os.environ[ \"LOCAL_RANK\" ]) \n-        self.model = model.to(self.gpu_id)  \n+        self.local_rank = int(os.environ[ \"LOCAL_RANK\" ]) \n+        self.model = model  \n         self.train_data = train_data  \n         self.optimizer = optimizer  \n         self.save_every = save_every  \n@@ -34,11 +46,10 @@  \n             print( \"Loading snapshot\" ) \n             self._load_snapshot(snapshot_path)  \n  \n-        self.model = DDP(self.model, device_ids=[self.gpu_id])  \n+        self.model = DDP(self.model)  \n  \n     def _load_snapshot (self, snapshot_path) : \n-        loc = f\"cuda:{self.gpu_id} \" \n-        snapshot = torch.load(snapshot_path, map_location=loc)  \n+        snapshot = torch.load(snapshot_path)  \n         self.model.load_state_dict(snapshot[ \"MODEL_STATE\" ]) \n         self.epochs_run = snapshot[ \"EPOCHS_RUN\" ] \n         print( f\"Resuming training from snapshot at Epoch {self.epochs_run} \")", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4facc20-e577-4fff-b7f2-4bb5bb923822": {"__data__": {"id_": "f4facc20-e577-4fff-b7f2-4bb5bb923822", "embedding": null, "metadata": {"page_label": "16", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fdaa40cf-e999-489f-8aa2-67f42a846d70", "node_type": "4", "metadata": {"page_label": "16", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "630a0098a9d2488382695f7a2e257650dd32bd493aa5aed8b876d2d1c6f48ce3", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n16 / 50 Copyright 2025 Fujitsu Limited@@ -48,16 +59,30 @@  \n         output = self.model(source)  \n         loss = F.cross_entropy(output, targets)  \n         loss.backward()  \n+        # [AGA] delete tensors on GPU device  \n+        del output, loss  \n         self.optimizer.step()  \n  \n     def _run_epoch (self, epoch) : \n         b_sz = len(next(iter(self.train_data))[ 0]) \n-        print( f\"[GPU{self.gpu_id} ] Epoch {epoch} | Batchsize: {b_sz} | Steps:  \n{len(self.train_data)} \") \n+        print( f\"[RANK{self.local_rank} ] Epoch {epoch} | Batchsize: {b_sz} | \nSteps: {len(self.train_data)} \") \n         self.train_data.sampler.set_epoch(epoch)  \n+        # [AGA] notify the beginning of device use to AGA  \n+        aga.on_device_begin()  \n+        # [AGA] get the latest model  \n+        self.model = aga.models[ 0] \n         for source, targets in self.train_data:  \n-            source = source.to(self.gpu_id)  \n-            targets = targets.to(self.gpu_id)  \n+            # [AGA] transfer training data to device  \n+            source = aga.move_tensor_to_device(source)  \n+            targets = aga.move_tensor_to_device(targets)  \n             self._run_batch(source, targets)  \n+        # [AGA] delete tensors on GPU device  \n+        del self.model, source, targets  \n+        # [AGA] notify the end of device use to AGA  \n+        aga.on_device_end()  \n+        # [AGA] get the latest model  \n+        self.model = aga.models[ 0] \n  \n     def _save_snapshot (self, epoch) : \n         snapshot = {  \n@@ -68,9 +93,10 @@  \n         print( f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path} \") \n  \n     def train(self, max_epochs: int) : \n         for epoch in range(self.epochs_run, max_epochs):  \n             self._run_epoch(epoch)  \n-            if self.gpu_id == 0 and epoch % self.save_every == 0: \n+            if self.local_rank == 0 and epoch % self.save_every == 0: \n                 self._save_snapshot(epoch)  \n@@ -96,16 +118,24 @@  \n     dataset, model, optimizer = load_train_objs()  \n     train_data = prepare_dataloader(dataset, batch_size)  \n     trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)  \n+    # [AGA] initialize AGA  \n+    global aga \n+    aga = PyTorchDDPAdaptiveGPUAllocator(  \n+        trainer.model, optimizer, world_size=int(os.environ[ \"WORLD_SIZE\" ]), \nrank=trainer.local_rank, device= \"cuda\" \n+    )", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ea0cc3c-2d27-45a0-962a-9273aa57817d": {"__data__": {"id_": "4ea0cc3c-2d27-45a0-962a-9273aa57817d", "embedding": null, "metadata": {"page_label": "17", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aefc5c38-7eb2-48f2-86c4-9f7d28e01f88", "node_type": "4", "metadata": {"page_label": "17", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0160dc5d55d50ec703b62a6e58ab4e9a87cebc2512065c4904101015c2d97762", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n17 / 50 Copyright 2025 Fujitsu Limited     trainer.train(total_epochs)  \n+    # [AGA] finalize AGA  \n+    aga.finalize()  \n     destroy_process_group()  \n  \nShow the differences before and after the modification.\nShow the modified sample program ( multigpu_torchrun_aga.py ).\n\"\"\" \nUsage: \nagarun -- torchrun --standalone --nproc_per_node=<nproc_per_node>  \nmultigpu_torchrun_aga.py <total_epochs> <save_every>  \n \nThis program is an AGA-enabled version of DDP's example program demonstrated in  \nPyTorch's video tutorial.  \nYou can find the original code at the link below.  \nhttps://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-\nseries/multigpu_torchrun.py  \n\"\"\" \n \nimport torch \nimport torch.nn.functional as F \nfrom torch.utils.data import Dataset, DataLoader  \nfrom datautils import MyTrainDataset  \n \nfrom torch.utils.data.distributed import DistributedSampler  \nfrom torch.nn.parallel import DistributedDataParallel as DDP \nfrom torch.distributed import init_process_group, destroy_process_group  \nimport os \n \n# [AGA] import AGA\nfrom adaptive_gpu_allocator.pytorch_ddp import PyTorchDDPAdaptiveGPUAllocator  \n \n \ndef ddp_setup (): \n    # [AGA] backend needs to be None to prepare both NCCL and GLOO backend  \n    init_process_group()  \n \n \nclass Trainer: \n    def __init__ ( \n        self,  \n        model: torch.nn.Module,  \n        train_data: DataLoader,  \n        optimizer: torch.optim.Optimizer,  \n        save_every: int,  \n        snapshot_path: str,  \n    ) -> None: \n        self.local_rank = int(os.environ[ \"LOCAL_RANK\" ])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db430269-d8d0-46ce-8afa-fe1d14045444": {"__data__": {"id_": "db430269-d8d0-46ce-8afa-fe1d14045444", "embedding": null, "metadata": {"page_label": "18", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5927151-5713-45ad-83e4-432b2fba4efc", "node_type": "4", "metadata": {"page_label": "18", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0c29d1674845f8ed405a7fe091afe27d256bb1c29e33b4ed14526686c586abc1", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n18 / 50 Copyright 2025 Fujitsu Limited        self.model = model  \n        self.train_data = train_data  \n        self.optimizer = optimizer  \n        self.save_every = save_every  \n        self.epochs_run = 0 \n        self.snapshot_path = snapshot_path  \n        if os.path.exists(snapshot_path):  \n            print( \"Loading snapshot\" ) \n            self._load_snapshot(snapshot_path)  \n \n        self.model = DDP(self.model)  \n \n    def _load_snapshot (self, snapshot_path) : \n        snapshot = torch.load(snapshot_path)  \n        self.model.load_state_dict(snapshot[ \"MODEL_STATE\" ]) \n        self.epochs_run = snapshot[ \"EPOCHS_RUN\" ] \n        print( f\"Resuming training from snapshot at Epoch {self.epochs_run} \") \n \n    def _run_batch (self, source, targets) : \n        self.optimizer.zero_grad()  \n        output = self.model(source)  \n        loss = F.cross_entropy(output, targets)  \n        loss.backward()  \n        # [AGA] delete tensors on GPU device  \n        del output, loss  \n        self.optimizer.step()  \n \n    def _run_epoch (self, epoch) : \n        b_sz = len(next(iter(self.train_data))[ 0]) \n        print( f\"[RANK{self.local_rank} ] Epoch {epoch} | Batchsize: {b_sz} | Steps:  \n{len(self.train_data)} \") \n        self.train_data.sampler.set_epoch(epoch)  \n        # [AGA] notify the beginning of device use to AGA  \n        aga.on_device_begin()  \n        # [AGA] get the latest model  \n        self.model = aga.models[ 0] \n        for source, targets in self.train_data:  \n            # [AGA] transfer training data to device  \n            source = aga.move_tensor_to_device(source)  \n            targets = aga.move_tensor_to_device(targets)  \n            self._run_batch(source, targets)  \n        # [AGA] delete tensors on GPU device  \n        del self.model, source, targets  \n        # [AGA] notify the end of device use to AGA  \n        aga.on_device_end()  \n        # [AGA] get the latest model  \n        self.model = aga.models[ 0] \n \n    def _save_snapshot (self, epoch) : \n        snapshot = {  \n            \"MODEL_STATE\" : self.model.module.state_dict(),  \n            \"EPOCHS_RUN\" : epoch,  \n        }  \n        torch.save(snapshot, self.snapshot_path)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1636df94-ca6d-4f0f-9a73-fa2bbdc88c51": {"__data__": {"id_": "1636df94-ca6d-4f0f-9a73-fa2bbdc88c51", "embedding": null, "metadata": {"page_label": "19", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c05a48b2-a873-4943-9d72-b28dab3ee63a", "node_type": "4", "metadata": {"page_label": "19", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b0be2482c37a8f5c27d8d3876a1cb9ba0ddcfb72e7be588959949994b9fa7755", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n19 / 50 Copyright 2025 Fujitsu Limited        print( f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path} \") \n \n    def train(self, max_epochs: int) : \n        for epoch in range(self.epochs_run, max_epochs):  \n            self._run_epoch(epoch)  \n            if self.local_rank == 0 and epoch % self.save_every == 0: \n                self._save_snapshot(epoch)  \n \n \ndef load_train_objs (): \n    train_set = MyTrainDataset( 2048)  # load your dataset  \n    model = torch.nn.Linear( 20, 1)  # load your model  \n    optimizer = torch.optim.SGD(model.parameters(), lr= 1e-3) \n    return train_set, model, optimizer  \n \n \ndef prepare_dataloader (dataset: Dataset, batch_size: int) : \n    return DataLoader(  \n        dataset, batch_size=batch_size, pin_memory= True, shuffle= False, \nsampler=DistributedSampler(dataset)  \n    ) \n \n \ndef main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str =  \n\"snapshot.pt\" ): \n    ddp_setup()  \n    dataset, model, optimizer = load_train_objs()  \n    train_data = prepare_dataloader(dataset, batch_size)  \n    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)  \n    # [AGA] initialize AGA  \n    global aga \n    aga = PyTorchDDPAdaptiveGPUAllocator(  \n        trainer.model, optimizer, world_size=int(os.environ[ \"WORLD_SIZE\" ]), \nrank=trainer.local_rank, device= \"cuda\" \n    ) \n    trainer.train(total_epochs)  \n    # [AGA] finalize AGA  \n    aga.finalize()  \n    destroy_process_group()  \n \n \nif __name__ == \"__main__\" : \n    import argparse  \n \n    parser = argparse.ArgumentParser(description= \"simple distributed training  \njob\") \n    parser.add_argument( \"total_epochs\" , type=int, help= \"Total epochs to train the  \nmodel\") \n    parser.add_argument( \"save_every\" , type=int, help= \"How often to save a  \nsnapshot\" ) \n    parser.add_argument( \"--batch_size\" , default= 32, type=int, help= \"Input batch  \nsize on each device (default: 32)\" ) \n    args = parser.parse_args()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d77387e8-a24e-4dac-b0f9-165961280b79": {"__data__": {"id_": "d77387e8-a24e-4dac-b0f9-165961280b79", "embedding": null, "metadata": {"page_label": "20", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d16ffbae-d972-4d57-9044-77eae7472cfc", "node_type": "4", "metadata": {"page_label": "20", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "fe36f1fa03114985cc4b6101b74185438b16f9c40fe23509f737221764e89038", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n20 / 50 Copyright 2025 Fujitsu Limited    main(args.save_every, args.total_epochs, args.batch_size)  \n \nRewriting PyTorch Training Program (Multi-Node Version)\nUsing the Distributed Data P arallel sample program provided in PyT orch's tutorial ( multigpu_torchrun.py )\nas an example, this section explains how to rewrite a training program. The sample program is launched using\ntorchrun. The rewritten sample program can be found at\nsamples/no_restart/multi_node/multinode_torchrun_aga.py . For the usage of A CB's API for PyT orch,\nplease refer to the subsequent [PyT orch API] section.\n(Hereafter, \"old\" refers to multigpu_torchrun.py , and \"new\" refers to multinode_torchrun_aga.py )\n1. Import PyTorchDDPAdaptiveGPUAllocator  (new line 23)\n2. Delete lines specifying GPU such as set_device , to, device_ids , map_location , etc. (old line 14, old\nline 27 (new line 41), old line 37 (new line 51), old lines 40-41 (new line 54))\n3. Leave the backend argument of init_process_group()  empty (old line 15, new line 28). This prepares\nboth NC CL for GPU and GL OO for CPU, enabling the movement of necessary memory between GPU\nand CPU in A CB.\n4. Use local_rank  for process identification in DDP (new lines 40, 70, 99)\n5. Make aga a global variable (new line 122) \u00a0\u00a0\u00a0 * In this sample program, the variable aga is made global\nto be referenced from within the _run_epoch()  and train()  functions. \u00a0\u00a0\u00a0 * Modify the global variable\nor function arguments according to the structure of the user program.\n6. Specify the starting point of the GPU processing within the epoch training loop using\non_device_begin()  (new line 73)\n7. Use move_tensor_to_device  instead of to (new lines 78-79)\n8. Delete unnecessary variables before on_device_end  to release allocated memory on the GPU (new\nlines 65, 82)\n9. Specify the ending point of the GPU processing using on_device_end()  (new line 84)\n10. Load the latest model as it is moved by on_device_begin  and on_device_end  (new lines 75, 86)\n11. Initialize PyTorchDDPAdaptiveGPUAllocator  (new line 123)\n12. Specify aga.finalize()  before destroy_process_group  to end the A CB process after using A GA\n(new line 128)\nShow the differences before and after rewriting.\n$ diff -u -U 1 multigpu_torchrun.py multigpu_torchrun_aga.py  \n--- multigpu_torchrun.py  \n+++ multinode_torchrun_aga.py  \n@@ -1,18 +1,32 @@  \n+# [AGA] import AGA  \n+from adaptive_gpu_allocator.pytorch_ddp import PyTorchDDPAdaptiveGPUAllocator  \n+ \n  \n def ddp_setup (): \n-    torch.cuda.set_device(int(os.environ[ \"LOCAL_RANK\" ])) \n-    init_process_group(backend= \"nccl\") \n+    # [AGA] backend needs to be None to prepare both NCCL and GLOO backend", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82a21c62-ea2d-4e29-8e9c-bd054dd6a5b5": {"__data__": {"id_": "82a21c62-ea2d-4e29-8e9c-bd054dd6a5b5", "embedding": null, "metadata": {"page_label": "21", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52813620-09c3-47ef-913b-3d57785beabb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7b82db80fa96baad9f79bb83707ea8b464c9e989cbcb55b532f4cbba5bf96722", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n21 / 50 Copyright 2025 Fujitsu Limited+    init_process_group()  \n+ \n  \n class Trainer: \n     def __init__ ( \n@@ -23,8 +37,8 @@ \n         save_every: int,  \n         snapshot_path: str,  \n     ) -> None: \n-        self.gpu_id = int(os.environ[ \"LOCAL_RANK\" ]) \n-        self.model = model.to(self.gpu_id)  \n+        self.local_rank = int(os.environ[ \"LOCAL_RANK\" ]) \n+        self.model = model  \n         self.train_data = train_data  \n         self.optimizer = optimizer  \n         self.save_every = save_every  \n@@ -34,11 +48,10 @@  \n             print( \"Loading snapshot\" ) \n             self._load_snapshot(snapshot_path)  \n  \n-        self.model = DDP(self.model, device_ids=[self.gpu_id])  \n+        self.model = DDP(self.model)  \n  \n     def _load_snapshot (self, snapshot_path) : \n-        loc = f\"cuda:{self.gpu_id} \" \n-        snapshot = torch.load(snapshot_path, map_location=loc)  \n+        snapshot = torch.load(snapshot_path)  \n         self.model.load_state_dict(snapshot[ \"MODEL_STATE\" ]) \n         self.epochs_run = snapshot[ \"EPOCHS_RUN\" ] \n         print( f\"Resuming training from snapshot at Epoch {self.epochs_run} \") \n@@ -48,16 +61,29 @@  \n         output = self.model(source)  \n         loss = F.cross_entropy(output, targets)  \n         loss.backward()  \n+        # [AGA] delete tensors on GPU device  \n+        del output, loss  \n         self.optimizer.step()  \n  \n     def _run_epoch (self, epoch) : \n         b_sz = len(next(iter(self.train_data))[ 0]) \n-        print( f\"[GPU{self.gpu_id} ] Epoch {epoch} | Batchsize: {b_sz} | Steps:  \n{len(self.train_data)} \") \n+        print( f\"[RANK{self.local_rank} ] Epoch {epoch} | Batchsize: {b_sz} | \nSteps: {len(self.train_data)} \") \n         self.train_data.sampler.set_epoch(epoch)  \n+        # [AGA] notify the beginning of device use to AGA  \n+        aga.on_device_begin()  \n+        # [AGA] get the latest model  \n+        self.model = aga.models[ 0] \n         for source, targets in self.train_data:  \n-            source = source.to(self.gpu_id)  \n-            targets = targets.to(self.gpu_id)  \n+            # [AGA] transfer training data to device  \n+            source = aga.move_tensor_to_device(source)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ae4c6eb-0265-4e6e-82a3-0eed2e606052": {"__data__": {"id_": "1ae4c6eb-0265-4e6e-82a3-0eed2e606052", "embedding": null, "metadata": {"page_label": "22", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b104bc7-67b4-4738-9cda-049342fd3c5d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "ccd0542d8756b2d0580c7f1df058a1571f5380cdf3c9524c9fe73f7567bdcba1", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n22 / 50 Copyright 2025 Fujitsu Limited+            targets = aga.move_tensor_to_device(targets)  \n             self._run_batch(source, targets)  \n+        # [AGA] delete tensors on GPU device  \n+        del self.model, source, targets  \n+        # [AGA] notify the end of device use to AGA  \n+        aga.on_device_end()  \n+        # [AGA] get the latest model  \n+        self.model = aga.models[ 0] \n  \n     def _save_snapshot (self, epoch) : \n         snapshot = {  \n@@ -70,7 +96,7 @@  \n     def train(self, max_epochs: int) : \n         for epoch in range(self.epochs_run, max_epochs):  \n             self._run_epoch(epoch)  \n-            if self.gpu_id == 0 and epoch % self.save_every == 0: \n+            if self.local_rank == 0 and epoch % self.save_every == 0: \n                 self._save_snapshot(epoch)  \n@@ -96,16 +118,24 @@  \n     dataset, model, optimizer = load_train_objs()  \n     train_data = prepare_dataloader(dataset, batch_size)  \n     trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)  \n+    # [AGA] initialize AGA  \n+    global aga \n+    aga = PyTorchDDPAdaptiveGPUAllocator(  \n+        trainer.model, optimizer, world_size=int(os.environ[ \"LOCAL_WORLD_SIZE\" ]), \nrank=trainer.local_rank, device= \"cuda\" \n+    ) \n     trainer.train(total_epochs)  \n+    # [AGA] finalize AGA  \n+    aga.finalize()  \n     destroy_process_group()  \n  \nShow the modified sample program ( multigpu_torchrun_aga.py ).\n\"\"\" \nUsage: \nagarun -- torchrun --nproc_per_node=<nproc_per_node> --nnodes=<nnodes> --rdzv_id=\n<rdzv_id> \\  \n--rdzv_endpoint=<address:port> --node_rank=<node_rank> \\  \nmultinode_torchrun_aga.py <total_epochs> <save_every>  \n \nThis program is an AGA-enabled version of DDP's example program demonstrated in  \nPyTorch's video tutorial.  \nYou can find the original code at the link below.  \nhttps://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-\nseries/multigpu_torchrun.py  \n\"\"\" \n \nimport torch \nimport torch.nn.functional as F", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24e80683-9d05-4b37-9f1f-55eac55b23f0": {"__data__": {"id_": "24e80683-9d05-4b37-9f1f-55eac55b23f0", "embedding": null, "metadata": {"page_label": "23", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed83d7ca-0484-4a88-8080-20adca13d219", "node_type": "4", "metadata": {"page_label": "23", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0d1f7c2215fedd788079cd7d803bff64075e24a7d4441bb14fe8d8981017544c", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n23 / 50 Copyright 2025 Fujitsu Limitedfrom torch.utils.data import Dataset, DataLoader  \nfrom datautils import MyTrainDataset  \n \nfrom torch.utils.data.distributed import DistributedSampler  \nfrom torch.nn.parallel import DistributedDataParallel as DDP \nfrom torch.distributed import init_process_group, destroy_process_group  \nimport os \n \n# [AGA] import AGA\nfrom adaptive_gpu_allocator.pytorch_ddp import PyTorchDDPAdaptiveGPUAllocator  \n \n \ndef ddp_setup (): \n    # [AGA] backend needs to be None to prepare both NCCL and GLOO backend  \n    init_process_group()  \n \n \nclass Trainer: \n    def __init__ ( \n        self,  \n        model: torch.nn.Module,  \n        train_data: DataLoader,  \n        optimizer: torch.optim.Optimizer,  \n        save_every: int,  \n        snapshot_path: str,  \n    ) -> None: \n        self.local_rank = int(os.environ[ \"LOCAL_RANK\" ]) \n        self.model = model  \n        self.train_data = train_data  \n        self.optimizer = optimizer  \n        self.save_every = save_every  \n        self.epochs_run = 0 \n        self.snapshot_path = snapshot_path  \n        if os.path.exists(snapshot_path):  \n            print( \"Loading snapshot\" ) \n            self._load_snapshot(snapshot_path)  \n \n        self.model = DDP(self.model)  \n \n    def _load_snapshot (self, snapshot_path) : \n        snapshot = torch.load(snapshot_path)  \n        self.model.load_state_dict(snapshot[ \"MODEL_STATE\" ]) \n        self.epochs_run = snapshot[ \"EPOCHS_RUN\" ] \n        print( f\"Resuming training from snapshot at Epoch {self.epochs_run} \") \n \n    def _run_batch (self, source, targets) : \n        self.optimizer.zero_grad()  \n        output = self.model(source)  \n        loss = F.cross_entropy(output, targets)  \n        loss.backward()  \n        # [AGA] delete tensors on GPU device  \n        del output, loss  \n        self.optimizer.step()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e52f508-0760-4222-bbd5-6210a0a1d47e": {"__data__": {"id_": "3e52f508-0760-4222-bbd5-6210a0a1d47e", "embedding": null, "metadata": {"page_label": "24", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b144f4c2-946c-460f-877b-48f1eefb650b", "node_type": "4", "metadata": {"page_label": "24", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "f854ecea181d46c630c5e0922fab11fd9a1f79530d00a080fbe3653cd17d5374", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n24 / 50 Copyright 2025 Fujitsu Limited    def _run_epoch (self, epoch) : \n        b_sz = len(next(iter(self.train_data))[ 0]) \n        print( f\"[RANK{self.local_rank} ] Epoch {epoch} | Batchsize: {b_sz} | Steps:  \n{len(self.train_data)} \") \n        self.train_data.sampler.set_epoch(epoch)  \n        # [AGA] notify the beginning of device use to AGA  \n        aga.on_device_begin()  \n        # [AGA] get the latest model  \n        self.model = aga.models[ 0] \n        for source, targets in self.train_data:  \n            # [AGA] transfer training data to device  \n            source = aga.move_tensor_to_device(source)  \n            targets = aga.move_tensor_to_device(targets)  \n            self._run_batch(source, targets)  \n        # [AGA] delete tensors on GPU device  \n        del self.model, source, targets  \n        # [AGA] notify the end of device use to AGA  \n        aga.on_device_end()  \n        # [AGA] get the latest model  \n        self.model = aga.models[ 0] \n \n    def _save_snapshot (self, epoch) : \n        snapshot = {  \n            \"MODEL_STATE\" : self.model.module.state_dict(),  \n            \"EPOCHS_RUN\" : epoch,  \n        }  \n        torch.save(snapshot, self.snapshot_path)  \n        print( f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path} \") \n \n    def train(self, max_epochs: int) : \n        for epoch in range(self.epochs_run, max_epochs):  \n            self._run_epoch(epoch)  \n            if self.local_rank == 0 and epoch % self.save_every == 0: \n                self._save_snapshot(epoch)  \n \n \ndef load_train_objs (): \n    train_set = MyTrainDataset( 2048)  # load your dataset  \n    model = torch.nn.Linear( 20, 1)  # load your model  \n    optimizer = torch.optim.SGD(model.parameters(), lr= 1e-3) \n    return train_set, model, optimizer  \n \n \ndef prepare_dataloader (dataset: Dataset, batch_size: int) : \n    return DataLoader(  \n        dataset, batch_size=batch_size, pin_memory= True, shuffle= False, \nsampler=DistributedSampler(dataset)  \n    ) \n \n \ndef main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str =  \n\"snapshot.pt\" ): \n    ddp_setup()  \n    dataset, model, optimizer = load_train_objs()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7194299-65eb-4f09-aa09-91be2162a33c": {"__data__": {"id_": "d7194299-65eb-4f09-aa09-91be2162a33c", "embedding": null, "metadata": {"page_label": "25", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98fd8831-f1fe-4eca-82e5-1ee272dc1cf9", "node_type": "4", "metadata": {"page_label": "25", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e9ef5f81158e373c618c41caa7fd4e73d262b36b5ecea394fd70b27f8a12018f", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n25 / 50 Copyright 2025 Fujitsu Limited    train_data = prepare_dataloader(dataset, batch_size)  \n    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)  \n    # [AGA] initialize AGA  \n    global aga \n    aga = PyTorchDDPAdaptiveGPUAllocator(  \n        trainer.model, optimizer, world_size=int(os.environ[ \"LOCAL_WORLD_SIZE\" ]), \nrank=trainer.local_rank, device= \"cuda\" \n    ) \n    trainer.train(total_epochs)  \n    # [AGA] finalize AGA  \n    aga.finalize()  \n    destroy_process_group()  \n \n \nif __name__ == \"__main__\" : \n    import argparse  \n \n    parser = argparse.ArgumentParser(description= \"simple distributed training  \njob\") \n    parser.add_argument( \"total_epochs\" , type=int, help= \"Total epochs to train the  \nmodel\") \n    parser.add_argument( \"save_every\" , type=int, help= \"How often to save a  \nsnapshot\" ) \n    parser.add_argument( \"--batch_size\" , default= 32, type=int, help= \"Input batch  \nsize on each device (default: 32)\" ) \n    args = parser.parse_args()  \n \n    main(args.save_every, args.total_epochs, args.batch_size)  \n \nPyTorch API (Single GPU Program Version)\nThis section explains the APIs available for user programs using PyT orch in the single GPU program version. All\nof these are methods of the PyTorchAdaptiveGPUAllocator  class in the\nadaptive_gpu_allocator.pytorch  module.\nPyTorchAdaptiveGPUAllocator.__init__()  \u00a0\u00a0\u00a0 - Constructor for PyTorchAdaptiveGPUAllocator\n\u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - model : Model (an object of a class inheriting from torch.nn.Module ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -\noptimizer : Optimizer (e.g., torch.optim.Adam ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - scheduler : Scheduler (e.g.,\ntorch.optim.lr_scheduler.ExponentialLR ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - device : Device to request allocation ( any, cuda ,\ngpu, cpu; gpu is an alias for cuda ) \u00a0\u00a0\u00a0 - R eturns \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - PyTorchAdaptiveGPUAllocator  object\nPytorchAdaptiveGPUAllocator.move_tensor_to_device()  \u00a0\u00a0\u00a0 - Moves the tensor of training data\nto the device (GPU or CPU) \u00a0\u00a0\u00a0 - Should be written before the actual training calculations (Forward, loss\ncalculation, Backward, Optimization) within the training loop \u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - tensor_data :\nTensor of training data \u00a0\u00a0\u00a0 - R eturns \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - T ensor of training data moved to the device\nPytorchAdaptiveGPUAllocator.on_device_begin()  \u00a0\u00a0\u00a0 - Specifies the starting point of the process\nwhere the device is to be used and requests device allocation \u00a0\u00a0\u00a0 - Generally written at the beginning of\nthe training loop \u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - device : Device to request allocation ( any, cuda , gpu, cpu; gpu\nis an alias for cuda ) \u00a0\u00a0\u00a0 - No return value", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f93b0a6d-bb07-48a1-bf54-60b79f725505": {"__data__": {"id_": "f93b0a6d-bb07-48a1-bf54-60b79f725505", "embedding": null, "metadata": {"page_label": "26", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83f890e2-6cdb-4bc1-ac16-7978f0810ae6", "node_type": "4", "metadata": {"page_label": "26", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a2b46013b954b7d3b934c53bff2262e4f2417ce5aeed1c1b055d1af3357a6501", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n26 / 50 Copyright 2025 Fujitsu LimitedPytorchAdaptiveGPUAllocator.on_device_end()  \u00a0\u00a0\u00a0 - Specifies the ending point of the process\nwhere the device is to be used and releases the allocated device \u00a0\u00a0\u00a0 - Generally written after the training\nloop \u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - no_release : If True, the allocated device is not released here and is\nreleased in finalize \u00a0\u00a0\u00a0 - No return value\nPytorchAdaptiveGPUAllocator.finalize()  \u00a0\u00a0\u00a0 - Ends the collaboration with the device allocation\nfunction and terminates the A CB process \u00a0\u00a0\u00a0 - Generally written at the end of the training \u00a0\u00a0\u00a0 - No\narguments or return value\nPyTorch API (Multi-GPU Program Version)\nThis section explains the APIs available for user programs using PyT orch in the multi-GPU program version. All\nof these are methods of the PyTorchDDPAdaptiveGPUAllocator  class in the\nadaptive_gpu_allocator.pytorch_ddp  module. Except for the constructor, the methods inherit from the\nparent class PytorchAdaptiveGPUAllocator .\nPyTorchDDPAdaptiveGPUAllocator.__init__()  \u00a0\u00a0\u00a0 - Constructor for\nPyTorchDDPAdaptiveGPUAllocator  \u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - model : Model (an object of a class\ninheriting from torch.nn.Module ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - optimizer : Optimizer (e.g., torch.optim.Adam ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -\nscheduler : Scheduler (e.g., torch.optim.lr_scheduler.ExponentialLR ) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - device : Device to\nrequest allocation ( any, cuda , gpu, cpu; gpu is an alias for cuda . If the environment variable\nAGA_REQUEST_DEVICE  is specified, it takes precedence) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - world_size : Number of processes when\nusing multi-process \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - rank : ID assigned to each process (range [ 0:world_size-1 ]) \u00a0\u00a0\u00a0 - R eturns\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - PyTorchDDPAdaptiveGPUAllocator  object\nPreprocessing, Infer ence, P ostpr ocessing (a. No-R estar t Version)\nUsing the sample program ( samples/no_restart/inference_skeleton.py ) as an example, this section\nexplains how to rewrite a user program with preprocessing, inference, and postprocessing for the a. no-restart\nversion of A CB.\nThe processing flow of the sample pseudo program is as follows: \u00a0 Preprocessing 1 > Inference 1 >\nPreprocessing 2 > Inference 2 > P ostprocessing\nModification points \u00a0 1. (If using PyT orch) Import PyTorchAdaptiveGPUAllocator  (line 3) \u00a0 2. Initialize\nPyTorchAdaptiveGPUAllocator  (line 28) \u00a0 3. Similar to training processing, specify the starting point\nof GPU processing for inference using on_device_begin()  (lines 43, 64) \u00a0 4. Similar to training\nprocessing, specify the ending point of GPU processing for inference using on_device_end()  (lines 48,\n69) \u00a0 5. Specify aga.finalize()  to end the A CB process (line 71)\nSample Program ( samples/no_restart/inference_skeleton.py )\nimport torch \nimport torch.nn as nn \nfrom adaptive_gpu_allocator.pytorch import PyTorchAdaptiveGPUAllocator  \n \n \n# Define the network architecture\nclass Net(nn.Module) : \n    def __init__ (self):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dd06500-42c7-4a11-a4d4-9532edd2999f": {"__data__": {"id_": "3dd06500-42c7-4a11-a4d4-9532edd2999f", "embedding": null, "metadata": {"page_label": "27", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62c6c232-1fc0-4495-861c-f86ea7b02995", "node_type": "4", "metadata": {"page_label": "27", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "1b099a06055a86958840d6cbf8948aae5454700a8c2d55beb910ff0f34397eb1", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n27 / 50 Copyright 2025 Fujitsu Limited        super(Net, self).__init__()  \n        self.fc1 = nn.Linear( 20, 10)  # Input layer  \n        self.fc3 = nn.Linear( 10, 10) \n        self.fc2 = nn.Linear( 10, 1)  # Output layer  \n \n    def forward(self, x) : \n        x = torch.relu(self.fc1(x))  \n        for i in range(20): \n            x = torch.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n \ndef preprocess (): \n    data = []  \n# Create Tensor Data (data) in Preprocessing  \n    return data \n \n \naga = PyTorchAdaptiveGPUAllocator(Net())  \n \n \n#\n# Preprocessing 1\n# \n \n# Execute Preprocessing  \nprint(\"pre-processing something (pre-processing 1)\" ) \ndata = preprocess()  \n \n \n \n#\n# Inference 1\n# \n \n \naga.on_device_begin()  \n \n# Inference Processing  \nprint(\"infer something (infer 1)\" ) \n \naga.on_device_end()  \n \n \n \n#\n# Preprocessing 2\n# \n \n# Execute Preprocessing  \nprint(\"pre-processing something (pre-processing 2)\" ) \ndata = preprocess()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9788c3a-5102-4c6e-baae-68bbd7842bff": {"__data__": {"id_": "f9788c3a-5102-4c6e-baae-68bbd7842bff", "embedding": null, "metadata": {"page_label": "28", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "572be251-e7be-4921-955d-9d7d5f43c7b9", "node_type": "4", "metadata": {"page_label": "28", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d16bf3649fadaf774f4b75d3f3fa58255f3241c87934212e83d16ddedb4a6059", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n28 / 50 Copyright 2025 Fujitsu Limited \n \n#\n# Inference 2\n# \n \n \naga.on_device_begin()  \n \nprint(\"infer something (infer 2)\" ) \n \naga.on_device_end()  \n \naga.finalize()  \n \n#\n# Postprocessing\n# \nprint(\"post-processing something\" ) \n \nMNIST Sample Pr ogram (a. No-R estar t Version)\nA sample program for training and inference using the a. no-restart version of the PyT orch framework,\ntargeting the MNIST test dataset for handwritten digit recognition, is also available in\nsamples/no_restart/mnist_pytorch/ . Please refer to it as well.\nb. Restart-Required Version\nIn the b. restart-required version of A CB, the user program is restarted to switch the device (CPU or GPU)\nexecuting each process.\nTherefore, as shown in the following diagram, processes executed before the restart will be executed again\nafter the restart.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47332d78-290d-449f-b119-d012c50dba89": {"__data__": {"id_": "47332d78-290d-449f-b119-d012c50dba89", "embedding": null, "metadata": {"page_label": "29", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5ae36ab-74bd-446b-bb39-039678271148", "node_type": "4", "metadata": {"page_label": "29", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "5a054972b69bca6d76a39d08d3b74cc0ce16ab6f5962612affb3862e2e160d4c", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n29 / 50 Copyright 2025 Fujitsu Limited\nTo prevent this, the following modifications need to be added to each process:\nSkip the process during restart if it has already been executed.\nSave the memory data generated by each process, which is required for the next process, to a file. The\nnext process will read this data.\nThis ensures that each process is executed once in order, as shown in the following diagram.\nThe specific modification methods are explained using sample programs as examples. For the training part,\nthe explanation is provided for each framework: PyT orch Lightning and T ensorFlow. For preprocessing,\ninference, and postprocessing parts, the explanation uses a pseudo sample program as an example. When\nrunning the b. restart-required version, it is necessary to set the environment variable AGA_USE_RESTART=1\nwith agarun. Please refer to the details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a871bd97-be48-4423-869e-7cf90a54ad37": {"__data__": {"id_": "a871bd97-be48-4423-869e-7cf90a54ad37", "embedding": null, "metadata": {"page_label": "30", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5c95b2a-21cb-4229-808a-f2e5537b8ec5", "node_type": "4", "metadata": {"page_label": "30", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0b594b14cd94ea80292b6f9d6507fc0caaa4f5d03645e8a032561b0e41760d92", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n30 / 50 Copyright 2025 Fujitsu LimitedPyTorch Lightning (b. R estar t-Requir ed Version)\nThis section explains how to rewrite a training program written using PyT orch Lightning to run with the b.\nrestart-required version of A CB.\nRewriting PyTorch Lightning Training Program\nUsing the sample program ( samples/restart/sample_pl.py ) as an example, this section explains how to\nrewrite a training program using PyT orch Lightning. For the usage of A CB's API for PyT orch Lightning, please\nrefer to the subsequent sections.\n1. Import Trainer  from adaptive_gpu_allocator.lightning  (line 11) \u00a0\u00a0\u00a0 * Simply importing the\nTrainer  class wrapped for A CB allows you to use A CB with PyT orch Lightning.\n2. Import ensure_manual_mode_with_restart  from adaptive_gpu_allocator.env  (line 13)\n3. Execute ensure_manual_mode_with_restart()  (line 16) \u00a0\u00a0\u00a0 * This function checks if the environment\nvariables for restart-required mode are set correctly to prevent running with missing settings.\nThe sample program ( samples/restart/sample_pl.py ) is as follows:\n# -*- coding: utf-8 -*-\n\"\"\"Sample script using AGA (pytorch lightning version).\"\"\"  \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport random \nimport numpy as np \n \nimport lightning as L \nfrom adaptive_gpu_allocator.lightning import Trainer  \n \nfrom adaptive_gpu_allocator.env import ensure_manual_mode_with_restart  \n \n# [AGA] Ensure running in manual AGA mode with restart  \nensure_manual_mode_with_restart()  \n \n \n# Define the network architecture\nclass Net(L.LightningModule) : \n    \"\"\"Sample model.\"\"\"  \n \n    def __init__ (self): \n        \"\"\"Define network components.\"\"\"  \n        super(Net, self).__init__()  \n        self.fc1 = nn.Linear( 20, 10)  # Input layer  \n        self.fc3 = nn.Linear( 10, 10) \n        self.fc2 = nn.Linear( 10, 1)  # Output layer  \n        self.criterion = nn.MSELoss()  \n \n    def forward(self, x) : \n        \"\"\"Define forward propagation process.\"\"\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3849e30-c6c3-4327-8300-f5904ed9e6b5": {"__data__": {"id_": "e3849e30-c6c3-4327-8300-f5904ed9e6b5", "embedding": null, "metadata": {"page_label": "31", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe135c7a-9559-4e21-be3c-6a2e3d0521df", "node_type": "4", "metadata": {"page_label": "31", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "bad2d70bba2b63a2c964f8aec92e12ec6321b9442c986ba23e1ded814e10cd18", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n31 / 50 Copyright 2025 Fujitsu Limited        x = torch.relu(self.fc1(x))  \n        for i in range(20): \n            x = torch.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n    def training_step (self, batch, batch_idx) : \n        \"\"\"Process training data.\"\"\"  \n        data, target = batch  \n        output = self.forward(data)  \n        loss = self.criterion(output, target)  \n        return loss \n \n    def configure_optimizers (self): \n        \"\"\"Define optimization method.\"\"\"  \n        optimizer = optim.Adam(self.parameters())  \n        return optimizer  \n \n \n# fix seeds  \nseed = 0 \nrandom.seed(seed)  \nnp.random.seed(seed)  \ntorch.manual_seed(seed)  \ntorch.backends.cudnn.benchmark = False \ntorch.backends.cudnn.deterministic = True \n \n# Assume we have some data in X_train and y_train  \nX_train = torch.randn( 1000, 20)  # 10000 samples, 20 features each  \ny_train = torch.randn( 1000, 1)  # 10000 samples, 1 target value each  \ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)  \ntrain_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,  \nbatch_size= 1) \n \n# Create the network  \nnet = Net()  \n \nmax_epoch = 5 \ntrainer = Trainer(max_epochs=max_epoch)  \ntrainer.fit(net, train_dataloaders=train_dataloader)  \nPyTorch Lightning API\nThis section explains the APIs available for user programs using PyT orch Lightning. All of these are methods of\nthe Trainer  class in the adaptive_gpu_allocator.lightning  module.\nTrainer.__init__()  \u00a0\u00a0\u00a0 - Constructor for Trainer  \u00a0\u00a0\u00a0 - Used in the same way as the original\npytorch_lightning.Trainer.__init__()\nTrainer.fit()  \u00a0\u00a0\u00a0 - Executes training using A CB \u00a0\u00a0\u00a0 - Used in the same way as the original\npytorch_lightning.Trainer.fit()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "773a9127-14be-4b85-8280-ed1b24ccacea": {"__data__": {"id_": "773a9127-14be-4b85-8280-ed1b24ccacea", "embedding": null, "metadata": {"page_label": "32", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c63b837-af89-4512-a1b2-c7e5240eea66", "node_type": "4", "metadata": {"page_label": "32", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "414b3d555577ae88a2f8460450c352f2f01bbbfe409344bdcf6057243889e352", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n32 / 50 Copyright 2025 Fujitsu Limitedimport tensorflow as tf \nfrom tensorflow.keras import layers \nimport numpy as np \n \n# [AGA] import AGA\nfrom adaptive_gpu_allocator.tensorflow import TFModel  \n \nfrom adaptive_gpu_allocator.env import ensure_manual_mode_with_restart  \n \n# [AGA] Ensure running in manual AGA mode with restart  \nensure_manual_mode_with_restart()  \n \n \n# [AGA] \nclass Net(TFModel) : \n    def __init__ (self): \n        super(Net, self).__init__()  \n        self.fc1 = layers.Dense(units= 10, input_shape=( 20,))  # Input layer  \n        self.fc3 = layers.Dense(units= 10) \n        self.fc2 = layers.Dense(units= 1)  # Output layer  \n \n    def call(self, x) : \n        x = tf.nn.relu(self.fc1(x))  \n        for _ in range(20): \n            x = tf.nn.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n \n# fix seeds  \nseed = 0 \ntf.keras.utils.set_random_seed(seed)  \ntf.config.experimental.enable_op_determinism()  \n \n# 1000 samples, 20 features each  \nX_train = np.random.rand( 1000, 20).astype(np.float64)  \n# 1000 samples, 1 features each  \ny_train = np.random.rand( 1000, 1).astype(np.float64)  \n \n# Model Compilation  \nmodel = Net()  \nmodel.compile(  \n    loss=tf.keras.losses.MSE,  \n    optimizer=tf.keras.optimizers.Adam( 0.1), \n) \n \nhistory = model.fit(X_train, y_train, batch_size= 1, epochs= 10) \nRewriting Training Programs Using Low-Level APIs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12df4507-f05a-414e-890e-be4767cb20da": {"__data__": {"id_": "12df4507-f05a-414e-890e-be4767cb20da", "embedding": null, "metadata": {"page_label": "33", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "920225d9-772f-4231-88c3-606c50af4422", "node_type": "4", "metadata": {"page_label": "33", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "828ce0f1e82ff9428a3141eb37dff54833518eb2acaf1a5dddc17ca032b94414", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n33 / 50 Copyright 2025 Fujitsu LimitedUsing the sample program ( samples/restart/sample_tf.py ) as an example, this guide explains how to\nrewrite training programs using T ensorFlow's low-level APIs for detailed settings such as Checkpoint and A GA.\nRefer to the subsequent sections for the usage of A CB APIs for T ensorFlow.\n1. Import TensorflowAdaptiveGPUAllocator  from adaptive_gpu_allocator.tensorflow  (line 7)\n2. Import ensure_manual_mode_with_restart  from adaptive_gpu_allocator.env  (line 9)\n3. Execute ensure_manual_mode_with_restart()  (line 12) \u00a0\u00a0\u00a0 * This function checks if the environment\nvariable settings for restart are working, preventing operation with missing settings.\n4. Define the model using the tf.keras.Model  class (line 24)\n5. Initialize TensorflowAdaptiveGPUAllocator  (line 55)\u00a0 \u00a0\u00a0\u00a0 * Initialize after defining the model,\noptimizer, scheduler, etc.\n6. Set the starting point of the training loop epoch with start_epoch  (line 58)\n7. Specify the starting point of the GPU processing section within the epoch training loop with\non_device_begin()  (line 63)\n8. Update the value of epoch_count to be saved with update_state_dict(epoch_count=epoch)  (line\n82)\u00a0 \u00a0\u00a0\u00a0 * Since checkpoints are saved/loaded during GPU/CPU switching, always include this in loops\nexecuted multiple times\u00a0 \u00a0\u00a0\u00a0 * The value registered with update_state_dict  (epoch in this case) is\nincluded in the checkpoint and loaded into start_epoch  after restart\n9. Specify the endpoint of the GPU processing section with on_device_end(no_release=True)  (line 85)\u00a0\n\u00a0\u00a0\u00a0 * Specify the no_release=True  option to prevent GPU release and suppress restart\n10. Finally, specify aga.finalize()  to end the A CB processing (line 88)\nSample program ( samples/restart/sample_tf.py )\nimport tensorflow as tf \nfrom tensorflow.keras import layers \nimport numpy as np \nimport os \n \n# [AGA] import AGA\nfrom adaptive_gpu_allocator.tensorflow import TensorflowAdaptiveGPUAllocator  \n \nfrom adaptive_gpu_allocator.env import ensure_manual_mode_with_restart  \n \n# [AGA] Ensure running in manual AGA mode with restart  \nensure_manual_mode_with_restart()  \n \n \n# loss function\ndef loss_fn(y_predict, y) : \n    return tf.keras.losses.MSE(y_predict, y)  \n \n \ntrain_loss = tf.keras.metrics.Mean(name= \"train_loss\" ) \n \n \n# Model Definition class Net(tf.keras.Model):  \n    def __init__ (self): \n        super(Net, self).__init__()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0e01798-8da1-49bb-be5a-131c9be9a216": {"__data__": {"id_": "e0e01798-8da1-49bb-be5a-131c9be9a216", "embedding": null, "metadata": {"page_label": "34", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25822ed2-3fd4-4f7a-a44c-364dc4b57f81", "node_type": "4", "metadata": {"page_label": "34", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "22d85657bf53b8aaf296a2e90c4386aa2d0be4dce68d529841b302b489d86c28", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n34 / 50 Copyright 2025 Fujitsu Limited        self.fc1 = layers.Dense(units= 10, input_shape=( 20,))  # Input layer  \n        self.fc3 = layers.Dense(units= 10) \n        self.fc2 = layers.Dense(units= 1)  # Output layer  \n \n    def call(self, x) : \n        x = tf.nn.relu(self.fc1(x))  \n        for i in range(20): \n            x = tf.nn.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n \n# fix seeds  \nseed = 0 \ntf.keras.utils.set_random_seed(seed)  \ntf.config.experimental.enable_op_determinism()  \n \n# 10000 samples, 20 features each  \nX_train = np.random.rand( 1000000, 20).astype(np.float32)  \n# 10000 samples, 1 features each  \ny_train = np.random.rand( 1000000, 1).astype(np.float32)  \n \n# create model  \nmodel = Net()  \noptimizer = tf.keras.optimizers.Adam( 0.1) \n \n# [AGA] initialize AGA  \naga = TensorflowAdaptiveGPUAllocator(model=model, optimizer=optimizer)  \n \n# Setting the Starting Point of the Epoch  \nstart_epoch = aga.start_epoch  \n \n# Training Loop\nfor epoch in range(start_epoch, 20): \n    # [AGA] notify the begining of iterations to AGA  \n    aga.on_device_begin()  \n \n    with tf.GradientTape() as tape: \n        y_predict = model(X_train)  \n        loss = loss_fn(y_predict, y_train)  \n        train_loss(loss)  \n    grads = tape.gradient(loss, model.variables)  \n \n    # update parameters using grads  \n    optimizer.apply_gradients(zip(grads, model.variables))  \n \n    # Print loss every 10 epochs  \n    gpuid = os.getenv( \"CUDA_VISIBLE_DEVICES\" ) \n    if gpuid is None or gpuid == \"NoGPU\" or gpuid == \"\": \n        print( f\"Epoch {epoch}, Loss: {train_loss.result().numpy()}  on CPU\" ) \n    else: \n        print( f\"Epoch {epoch}, Loss: {train_loss.result().numpy()}  on {gpuid}\") \n \n    # [AGA] register arbitrary key-value for checkpointing per epoch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "264b3773-fcab-4fc3-9654-9a5f9d624ec0": {"__data__": {"id_": "264b3773-fcab-4fc3-9654-9a5f9d624ec0", "embedding": null, "metadata": {"page_label": "35", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1771bfa-ffc5-41ff-958b-ad96a2245318", "node_type": "4", "metadata": {"page_label": "35", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "11db459d2a1d3e84e2b85834cde34538c0d014c536324703c14ecc8da3400f3c", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n35 / 50 Copyright 2025 Fujitsu Limited    aga.update_state_dict(epoch_count=epoch)  \n \n    # [AGA] notify the end of iterations to AGA  \n    aga.on_device_end(no_release= True) \n \n# [AGA] notifiy the end of iterations to AGA  \naga.finalize()  \nAPI for TensorFlow\nThis section explains the APIs available for user programs in T ensorFlow.\u00a0 The APIs include methods from the\nTensorflowAdaptiveGPUAllocator  class and the TFModel  class in the\nadaptive_gpu_allocator.tensorflow  module.\nBelow are the methods of the TensorflowAdaptiveGPUAllocator  class:\nTensorflowAdaptiveGPUAllocator.__init__() \u00a0 \u00a0\u00a0\u00a0 - Constructor for\nTensorflowAdaptiveGPUAllocator  \u00a0\u00a0\u00a0 - Also loads checkpoints during restart\u00a0 \u00a0\u00a0\u00a0 - Arguments\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -\nmodel : An instance of a class inheriting from tf.keras.Model \u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - optimizer : Optimizer (e.g.,\ntf.keras.optimizers.Adam )\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - scheduler : Scheduler (e.g.,\ntf.keras.optimizers.schedules.LearningRateSchedule )\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - device : Device requested for\nallocation ( any, cuda , gpu, cpu; gpu is an alias for cuda ) \u00a0\u00a0\u00a0 - R eturns\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -\nTensorflowAdaptiveGPUAllocator  object\nTensorflowAdaptiveGPUAllocator.update_state_dict() \u00a0 \u00a0\u00a0\u00a0 - R egisters information necessary for\nresuming processing after a restart\u00a0 \u00a0\u00a0\u00a0 - Should be written before points where device switching occurs\n(e.g., on_device_end() , end of training loop)\u00a0 \u00a0\u00a0\u00a0 - Arguments\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - epoch_count : Number of epochs\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - batch_count : Number of batches \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - completed : Flag for skipping inference or training\nprocessing \u00a0\u00a0\u00a0 - No return value\nTensorflowAdaptiveGPUAllocator.on_device_begin() \u00a0 \u00a0\u00a0\u00a0 - Specifies the starting point of the\nprocessing where the device is to be used and requests device allocation\u00a0 \u00a0\u00a0\u00a0 - Generally written at the\nbeginning of the training loop\u00a0 \u00a0\u00a0\u00a0 - Arguments\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - device : Device requested for allocation ( any,\ncuda , gpu, cpu; gpu is an alias for cuda . If specified in the constructor of\nTensorflowAdaptiveGPUAllocator , those take precedence)\u00a0 \u00a0\u00a0\u00a0 - No return value\nTensorflowAdaptiveGPUAllocator.on_device_end() \u00a0 \u00a0\u00a0\u00a0 - Specifies the endpoint of the processing\nwhere the device is to be used and releases the allocated device\u00a0 \u00a0\u00a0\u00a0 - Generally written after the training\nloop\u00a0 \u00a0\u00a0\u00a0 - Arguments \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - no_release : If True , the allocated device is not released and remains\nallocated \u00a0\u00a0\u00a0 - No return value\nTensorflowAdaptiveGPUAllocator.finalize() \u00a0 \u00a0\u00a0\u00a0 - Ends the interaction with the device allocation\nfunction and terminates A CB processing\u00a0 \u00a0\u00a0\u00a0 - Generally written at the end of training\u00a0 \u00a0\u00a0\u00a0 - No arguments\nor return value\nBelow are the methods of the TFModel  class:\nTFModel.__init__() \u00a0 \u00a0\u00a0\u00a0 - Constructor for TFModel \u00a0 \u00a0\u00a0\u00a0 - Used similarly to the original\ntensorflow.keras.Model.__init__()\nTFModel.compile() \u00a0 \u00a0\u00a0\u00a0 - Compiles the TFModel  \u00a0\u00a0\u00a0 - Used similarly to the original\ntensorflow.keras.Model.compile()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d534646f-2c79-459d-ba12-6b807ec42780": {"__data__": {"id_": "d534646f-2c79-459d-ba12-6b807ec42780", "embedding": null, "metadata": {"page_label": "36", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3388b4ad-8628-4a4b-973f-f2d7ccfdc36c", "node_type": "4", "metadata": {"page_label": "36", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b8f1bf2239e01c615872f9744a8459cd2f3f1c9380b22bd35300b90abeacd8e4", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n36 / 50 Copyright 2025 Fujitsu LimitedTFModel.fit() \u00a0 \u00a0\u00a0\u00a0 - Executes training using A CB \u00a0\u00a0\u00a0 - Used similarly to the original\ntensorflow.keras.Model.fit()\nPreprocessing, Infer ence, P ostpr ocessing (b. With R estar t)\nACB performs process restarts to switch devices (CPU/GPU). Therefore, preprocessing and postprocessing that\noccur before training or inference must be prevented from executing again upon restart after the program\nhas completed once.\nUsing the sample program ( samples/restart/inference_skeleton.py ) as an example, this guide explains\nhow to rewrite user programs with preprocessing, inference, and postprocessing for A CB's b. with restart\nversion using the PyT orch framework. If using other frameworks, replace the imported modules and APIs with\nthose specific to the framework.\nThe processing flow of the sample pseudo-program is as follows:\u00a0 \u00a0 Preprocessing 1 > Inference 1 >\nPreprocessing 2 > Inference 2 > P ostprocessing\nModifications\u00a0 \u00a0 1. Import PyTorchAdaptiveGPUAllocator  (line 5) if using PyT orch\u00a0 \u00a0 2. Initialize\nPyTorchAdaptiveGPUAllocator  (line 38)\u00a0 \u00a0 3. T o prevent multiple executions of preprocessing,\ninference, and postprocessing upon restart, create checkpoint files and preprocessing data save files for\neach process.\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0 1. Specify checkpoint file names and preprocessing data save file names (lines 8-11,\n13)\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0 2. Check for the existence of these files at the beginning of each process (lines 41, 54, 71, 84)\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0 3. If the files do not exist, the user program executes as the process is not complete. No\nmodification needed here (lines 44, 62, 74, 92, 102)\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0 4. Create the files once the process is complete\n(lines 51, 66, 81, 96)\u00a0 \u00a0 4. Save files generated during each process that are needed after restart using\nfunctions like pickle (lines 47-48, 77-78)\u00a0 \u00a0 5. Load the saved files after restart (lines 57-58, 87-88)\u00a0 \u00a0 6.\nSimilar to training, specify the starting point of GPU processing in inference with on_device_begin()\n(lines 60, 90)\u00a0 \u00a0 7. Similar to training, specify the endpoint of GPU processing in inference with\non_device_end()  (lines 68, 98)\u00a0 \u00a0 8. Specify aga.finalize()  to end A CB processing (line 100)\u00a0 \u00a0 9.\nFinally, delete unnecessary data (lines 106-110)\nSample program ( samples/restart/inference_skeleton.py )\nimport os \nimport pickle \nimport torch \nimport torch.nn as nn \nfrom adaptive_gpu_allocator.pytorch import PyTorchAdaptiveGPUAllocator  \n \n \n# Checkpoint Files  \nckpt1 = 'checkpoint1'  \nckpt2 = 'checkpoint2'  \nckpt3 = 'checkpoint3'  \nckpt4 = 'checkpoint4'  \n \ndata_file = 'data.pickle'  \n \n \n# Define the network architecture", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a7ebce8-d84f-490f-aa6f-690e84ffda2b": {"__data__": {"id_": "3a7ebce8-d84f-490f-aa6f-690e84ffda2b", "embedding": null, "metadata": {"page_label": "37", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba1363df-32c9-497b-9a5d-5f64be45a202", "node_type": "4", "metadata": {"page_label": "37", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "1ae8762a555ca1bb2e203c67006c3246fa663f92f7b2053b762a414a433efd47", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n37 / 50 Copyright 2025 Fujitsu Limitedclass Net(nn.Module) : \n    def __init__ (self): \n        super(Net, self).__init__()  \n        self.fc1 = nn.Linear( 20, 10)  # Input layer  \n        self.fc3 = nn.Linear( 10, 10) \n        self.fc2 = nn.Linear( 10, 1)   # Output layer  \n \n    def forward(self, x) : \n        x = torch.relu(self.fc1(x))  \n        for i in range(20): \n            x = torch.relu(self.fc3(x))  \n        x = self.fc2(x)  \n        return x \n \n \ndef preprocess (): \n    data = []  \n    return data \n \n \naga = PyTorchAdaptiveGPUAllocator(Net())  \n \nif not os.path.exists(ckpt1):  \n    print( \"pre-processing something (pre-processing 1)\" ) \n    data = preprocess()  \n \n    with open(data_file, \"wb\") as f: \n        pickle.dump(data, f)  \n \n    open(ckpt1, \"a\").close()  \n \nif not os.path.exists(ckpt2):  \n \n    with open(data_file, \"rb\") as f: \n        data = pickle.load(f)  \n \n    aga.on_device_begin()  \n \n    print( \"infer something (infer 1)\" ) \n \n    open(ckpt2, \"a\").close()  \n \n    aga.on_device_end()  \n \nif not os.path.exists(ckpt3):  \n    print( \"pre-processing something (pre-processing 2)\" ) \n    data = preprocess()  \n \n    with open(data_file, \"wb\") as f: \n        pickle.dump(data, f)  \n \n    open(ckpt3, \"a\").close()  \n \nif not os.path.exists(ckpt4):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bef9d067-a125-43fa-8093-01c509efd727": {"__data__": {"id_": "bef9d067-a125-43fa-8093-01c509efd727", "embedding": null, "metadata": {"page_label": "38", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f7ce314-e135-4612-b2bd-56ac4dbbf240", "node_type": "4", "metadata": {"page_label": "38", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "1e4a134256a47135e72e86a5248192742a9105d89cb5f5150ff2f4136a2a6bbc", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n38 / 50 Copyright 2025 Fujitsu Limited \n    with open(data_file, \"rb\") as f: \n        data = pickle.load(f)  \n \n    aga.on_device_begin()  \n \n    print( \"infer something (infer 2)\" ) \n \n    open(ckpt4, \"a\").close()  \n \n    aga.on_device_end()  \n \naga.finalize()  \n \nprint(\"post-processing something\" ) \n \nos.remove(data_file)  \nos.remove(ckpt1)  \nos.remove(ckpt2)  \nos.remove(ckpt3)  \nos.remove(ckpt4)  \nMNIST Sample Pr ogram (b. With R estar t)\nSample programs for training and inference using the b. with restart version of the PyT orch framework for the\nMNIST handwritten digit recognition test dataset are available in samples/restart/mnist_pytorch/ , and\nsample programs using the T ensorFlow framework are available in samples/restart/mnist_tf/ . Please\nrefer to these as well.\nExecuting User Programs with A CB\nExecute user programs compatible with A CB by passing them as arguments to the driver command (agarun).\nBy changing environment variables, you can specify the port number, the initial device to use (GPU or CPU),\nwhether process restart is required when switching devices (CPU/GPU), and the controller  and executor\nfor executing multi-node jobs.\nagarun python sample.py   \nIf the user program starts with command line options like python sample.py --foo , adding -- after\n'agarun' will allow the user program to recognize the command line options.\nagarun -- python sample.py --foo   \nAddress of controller  when using multi-node \u00a0 When executing multi-node jobs with GPU Assigner,\nspecifying the address of the controller  is necessary.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7098a2cf-fd19-46b8-b8b5-034e65ea3b47": {"__data__": {"id_": "7098a2cf-fd19-46b8-b8b5-034e65ea3b47", "embedding": null, "metadata": {"page_label": "39", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "470d1af1-7d71-4406-abed-d364be33f663", "node_type": "4", "metadata": {"page_label": "39", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b19484ea07a49c639c40183b821587673fb6c3b0d2b8c60e944514881ebb151a", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n39 / 50 Copyright 2025 Fujitsu LimitedAGA_GPU_ALLOC_SERVER_ADDRESS=controller \u306e\u30a2\u30c9\u30ec\u30b9  agarun torchrun sample.py  \n\u30dd\u30fc\u30c8 \u756a\u53f7 \u306e \u5909\u66f4\nGPU Assigner \u3092\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30dd\u30fc\u30c8 \u756a\u53f7 (11234) \u4ee5\u5916 \u3067 \u4f7f\u2f64 \u3059\u308b \u5834\u5408\u3001agarun \u306e \u5b9f\u2f8f \u3082\u30dd\u30fc\u30c8 \u756a\u53f7 \u306e \u6307\u5b9a \u304c \u5fc5\n\u8981\u3002\nAGA_GPU_ALLOC_SERVER_PORT=controler 's addaddressress agarun python sample.py  \nSpecifying the initial device to use\u00a0 \u00a0 Y ou can specify whether the initial processing of the user program\nshould be done on the GPU or if CPU processing (such as preprocessing) is sufficient. \u00a0 - gpu: Specify\nGPU (default value) \u00a0 - cpu: Specify CPU \u00a0 \u00a0 Example of specifying CPU\nAGA_INITIAL_DEVICE=cpu agarun python sample.py  \nSpecifying the b. with restart version when switching devices (CPU/GPU)\u00a0 \u00a0 If using the b. with restart\nversion, specifying AGA_USE_RESTART  is necessary. \u00a0 When using deep learning frameworks like PyT orch\nLightning or T ensorFlow, only the b. with restart version can be used, so specifying AGA_USE_RESTART  is\nmandatory. When using PyT orch, the a. without restart version can be used, so specifying is not\nnecessary. \u00a0 - 0: a. without restart (default) \u00a0 - 1: b. with restart \u00a0 - Other values: Runtime exception\n(ValueError )\nExample of specifying to use the b. with restart version\nAGA_USE_RESTART=1 agarun python sample_tf.py  \nArguments for agarun\nHelp for agarun options: agarun -h  \u00a0 Displays a list of options for agarun\nusage: agarun [-h] [--without-aga] [--history-path HISTORY_PATH]  \n              [--checkpoints-topdir CHECKPOINTS_TOPDIR]  \n              args [args ...]  \n \nCLI to run your program with AGA enabled  \n \npositional arguments:  \n  args                  Command to run with AGA enabled  \n \noptions:  \n  -h, --help            show this help message and exit  \n  --without-aga         Disable GPU scheduling with AGA (for assessment  \npurpose)  \n  --history-path HISTORY_PATH", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6031395d-1495-4d34-9cff-ae6430745cc7": {"__data__": {"id_": "6031395d-1495-4d34-9cff-ae6430745cc7", "embedding": null, "metadata": {"page_label": "40", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6b31287-bcb9-4839-a2cc-f550f81c94e5", "node_type": "4", "metadata": {"page_label": "40", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "161b0009f75ec3144850b1021ac00cfab0ef351b9810ee33968ac822f84738c0", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n40 / 50 Copyright 2025 Fujitsu Limited                        Path of the job history file  \n  --checkpoints-topdir CHECKPOINTS_TOPDIR  \n                        Directory where checkpoint directories for each job  \nare stored  \n\u6709\u52b9 \u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u306f --history-path \u3068--checkpoints-topdir \u3067\u3042\u308b\u3002\nThe valid options are --history-path  and --checkpoints-topdir .\nSpecifying the location for storing job history ( --history-path ) \u00a0 The --history-path  argument\nallows you to specify the location for storing job history. \u00a0 Job history refers to the record of information\nsuch as timestamps and call arguments at the time of executing the agarun  command or library API\nduring A CB job execution.\nBy understanding how jobs were executed from the job history, it is possible to analyze the job execution\nstatus and the effects of A CB implementation after operation.\nThe default location for storing job history is $HOME/.acb/job-history.log .\nSpecifying the location for storing checkpoints ( --checkpoints-topdir ) \u00a0 The --checkpoints-\ntopdir  argument allows you to specify the location for storing and loading checkpoints during\nGPU/CPU switching. \u00a0 The default location for storing checkpoints is /tmp/.acb-checkpoint-\n<job_uuid> .\nExample of Executing Single-Node Jobs\nThis section shows an example of executing a sample program for single-node jobs using the driver\ncommand.\nExecuting a S tandalone Pr ogram\n1. Move to the working directory\ncd $WORKDIR/adaptive-gpu-allocator/samples  \n2. agarun  Execute the user program (sample.py) using\nagarun python sample.py  \nThe output will be as follows, indicating that the processing is being done on the GPU:\nEpoch 0, Loss: 1.0316227674484253 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 1, Loss: 1.0306103229522705 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 2, Loss: 1.0296107530593872 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 3, Loss: 1.0286246538162231 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 4, Loss: 1.0276520252227783 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf9cb4b1-e1fa-4fba-9c7c-dafa5f63e101": {"__data__": {"id_": "bf9cb4b1-e1fa-4fba-9c7c-dafa5f63e101", "embedding": null, "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48c1980-9844-4ce9-8c6e-8df97b4883e4", "node_type": "4", "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0df21714f306e6313d3eaa0553f015c21b8ee31d139f5add0889d33e86632c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcf5be8e-a206-40c8-87f6-bf04762786e0", "node_type": "1", "metadata": {}, "hash": "11a8b17c24c01feaa991acd98ac89133563aaa9bd973119bb0f9ef5da8c7cef6", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n41 / 50 Copyright 2025 Fujitsu LimitedEpoch 5, Loss: 1.02669358253479 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 6, Loss: 1.0257498025894165 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 7, Loss: 1.0248205661773682 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 8, Loss: 1.0239065885543823 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 9, Loss: 1.023008108139038 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 10, Loss: 1.022125244140625 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 11, Loss: 1.0212583541870117 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 12, Loss: 1.020407795906067 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 13, Loss: 1.01957368850708 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 14, Loss: 1.0187562704086304 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 15, Loss: 1.0179557800292969 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 17, Loss: 1.0164062976837158 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 18, Loss: 1.015657663345337 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 19, Loss: 1.014926552772522 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nExecuting Multiple Pr ograms\nThis section shows an example of executing more programs than the number of GPUs available.\n1. Execute 3 processes in a 2-GPU environment\nagarun python sample.py >log1 &  \nagarun python sample.py >log2 &  \nagarun python sample.py >log3 &  \nThe 1st and 2nd processes will be executed on the GPU, while the remaining 3rd process will be executed on\nthe CPU. When the 1st and 2nd processes, which were running on the GPU, finish, the 3rd process, which was\nrunning on the CPU, will switch to execution on the GPU.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcf5be8e-a206-40c8-87f6-bf04762786e0": {"__data__": {"id_": "bcf5be8e-a206-40c8-87f6-bf04762786e0", "embedding": null, "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48c1980-9844-4ce9-8c6e-8df97b4883e4", "node_type": "4", "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0df21714f306e6313d3eaa0553f015c21b8ee31d139f5add0889d33e86632c67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf9cb4b1-e1fa-4fba-9c7c-dafa5f63e101", "node_type": "1", "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d397f732603841d94f73bda072b8bbccbd5a03c96721bd075935f036c30a2d52", "class_name": "RelatedNodeInfo"}}, "text": "1. Execute 3 processes in a 2-GPU environment\nagarun python sample.py >log1 &  \nagarun python sample.py >log2 &  \nagarun python sample.py >log3 &  \nThe 1st and 2nd processes will be executed on the GPU, while the remaining 3rd process will be executed on\nthe CPU. When the 1st and 2nd processes, which were running on the GPU, finish, the 3rd process, which was\nrunning on the CPU, will switch to execution on the GPU.\n1st process (log1)\nEpoch 0, Loss: 1.0316227674484253 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 1, Loss: 1.0306103229522705 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 2, Loss: 1.0296107530593872 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 3, Loss: 1.0286246538162231 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 4, Loss: 1.0276520252227783 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 5, Loss: 1.02669358253479 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 6, Loss: 1.0257498025894165 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 7, Loss: 1.0248205661773682 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 8, Loss: 1.0239065885543823 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 9, Loss: 1.023008108139038 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 10, Loss: 1.022125244140625 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 11, Loss: 1.0212583541870117 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 12, Loss: 1.020407795906067 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 13, Loss: 1.01957368850708 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 14, Loss: 1.0187562704086304 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 15, Loss: 1.0179557800292969 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f", "mimetype": "text/plain", "start_char_idx": 1381, "end_char_idx": 3176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82a580ed-4242-47c9-858e-5744f124bbc7": {"__data__": {"id_": "82a580ed-4242-47c9-858e-5744f124bbc7", "embedding": null, "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1", "node_type": "4", "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c402dfa7da29d0e01bfe03bdfaab2ff844be3dcc43a01a024cd8346899d7927d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a72e86e-e4ed-458a-b674-670a4f78cf91", "node_type": "1", "metadata": {}, "hash": "55232d07d0e530481ecebb2352a5a969fc137f05f52fe8e9b92d73541fc5491b", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n42 / 50 Copyright 2025 Fujitsu LimitedEpoch 17, Loss: 1.0164062976837158 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 18, Loss: 1.015657663345337 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 19, Loss: 1.014926552772522 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \n2nd process (log2)\nEpoch 0, Loss: 1.0316227674484253 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 1, Loss: 1.0306103229522705 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 2, Loss: 1.0296107530593872 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 3, Loss: 1.0286246538162231 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 4, Loss: 1.0276520252227783 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 5, Loss: 1.02669358253479 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 6, Loss: 1.0257498025894165 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 7, Loss: 1.0248205661773682 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 8, Loss: 1.0239065885543823 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 9, Loss: 1.023008108139038 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 10, Loss: 1.022125244140625 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 11, Loss: 1.0212583541870117 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 12, Loss: 1.020407795906067 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 13, Loss: 1.01957368850708 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 14, Loss: 1.0187562704086304 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 15, Loss: 1.0179557800292969 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 17, Loss: 1.0164062976837158 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 18, Loss: 1.015657663345337 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 19, Loss: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a72e86e-e4ed-458a-b674-670a4f78cf91": {"__data__": {"id_": "0a72e86e-e4ed-458a-b674-670a4f78cf91", "embedding": null, "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1", "node_type": "4", "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c402dfa7da29d0e01bfe03bdfaab2ff844be3dcc43a01a024cd8346899d7927d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82a580ed-4242-47c9-858e-5744f124bbc7", "node_type": "1", "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "66e6e8fab9d8f9dc63af173bbc83b77898cdd75f38d34e57530a14f0a9976257", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9115c532-d149-49c3-b641-745a5efe420c", "node_type": "1", "metadata": {}, "hash": "2a9622db0987c26a1461b46ed8adb6ca8d4466f1acabe803c8e9c6e43d6ed316", "class_name": "RelatedNodeInfo"}}, "text": "Loss: 1.0179557800292969 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 17, Loss: 1.0164062976837158 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 18, Loss: 1.015657663345337 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \nEpoch 19, Loss: 1.014926552772522 on GPU-173d3486-b338-629d-3b9d-8a3a4c31ea3f  \n3rd process (log3)\nEpoch 0, Loss: 1.0316228866577148 on CPU  \nEpoch 1, Loss: 1.0306103229522705 on CPU  \nEpoch 2, Loss: 1.0296107530593872 on CPU  \nEpoch 3, Loss: 1.0286246538162231 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 4, Loss: 1.0276520252227783 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 5, Loss: 1.02669358253479 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 6, Loss: 1.0257498025894165 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 7, Loss: 1.0248205661773682 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 8, Loss: 1.0239065885543823 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 9, Loss: 1.023008108139038 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 10, Loss: 1.022125244140625 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 11, Loss: 1.0212583541870117 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 12, Loss: 1.020407795906067 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 13, Loss: 1.01957368850708 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 14, Loss: 1.0187562704086304 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 15, Loss: 1.0179557800292969 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 17, Loss: 1.0164062976837158 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 18, Loss: 1.015657663345337 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 19, Loss: 1.", "mimetype": "text/plain", "start_char_idx": 1527, "end_char_idx": 3367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9115c532-d149-49c3-b641-745a5efe420c": {"__data__": {"id_": "9115c532-d149-49c3-b641-745a5efe420c", "embedding": null, "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1", "node_type": "4", "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c402dfa7da29d0e01bfe03bdfaab2ff844be3dcc43a01a024cd8346899d7927d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a72e86e-e4ed-458a-b674-670a4f78cf91", "node_type": "1", "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "395190fd5d309f44efd645a00b4292ba6ba55d70650a167de2d29f9f52cf6aec", "class_name": "RelatedNodeInfo"}}, "text": "Loss: 1.0179557800292969 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 16, Loss: 1.0171724557876587 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 17, Loss: 1.0164062976837158 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 18, Loss: 1.015657663345337 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f  \nEpoch 19, Loss: 1.014926552772522 on GPU-b9d18b6a-6648-3e3a-90af-d3d717ee8d8f", "mimetype": "text/plain", "start_char_idx": 3036, "end_char_idx": 3426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a177d4fd-bdd6-4d7d-9b52-7c7656caeb5c": {"__data__": {"id_": "a177d4fd-bdd6-4d7d-9b52-7c7656caeb5c", "embedding": null, "metadata": {"page_label": "43", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da123fb6-ee9c-45a6-8e7f-e8f9290d65d7", "node_type": "4", "metadata": {"page_label": "43", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "947b3908e39c9f35fd129f1e989d0a6bc94f12687f42514cb5f6ed28fcf6ca70", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n43 / 50 Copyright 2025 Fujitsu LimitedExample of Executing Multi-Node Jobs\nThis section shows an example of executing a sample program for multi-node jobs\n(samples/no_restart/multi_node/multinode_torchrun_aga.py ) using the driver command.\nExecution S teps\n1. Create the multi-node A CB environment file multinode.toml . \u00a0\u00a0 - Create\n$HOME/.acb/multinode.toml  on all nodes. \u00a0\u00a0 - R efer to the contents.\n2. Set environment variables. \u00a0\u00a0 Set the environment variables that will be commonly used in the\ncommand lines explained below. \u00a0\u00a0 T o do this, execute the following on the command line of each node.\n $ export CONTROLLER_ADDR=controller \u306e IP \u30a2\u30c9\u30ec\u30b9 \n $ export CONTROLLER_PORT=controller \u3067 \u4f7f\u2f64 \u3059\u308b\u30dd\u30fc\u30c8 \u756a\u53f7 \n $ export EXECUTOR_ADDR=executor \u306e IP \u30a2\u30c9\u30ec\u30b9 \n $ export EXECUTOR_PORT=executor \u3067 \u4f7f\u2f64 \u3059\u308b\u30dd\u30fc\u30c8 \u756a\u53f7 \n $ export IFNAME=`ip route | grep default | cut -d ' ' -f 5` \n3. Start gpu_assigner  on the executor  node. \u00a0\u00a0\u00a0 Execute the following on the executor  node. Set the\nno_proxy environment variable for grpc communication with the controller .\n$ no_proxy= $CONTROLLER_ADDR  gpu_assigner --multinode start --scheduler gpu-\naffinity  \n4. Start gpu_assigner  on the controller  node. \u00a0\u00a0 Execute the following on the controller  node. Set\nthe no_proxy environment variable for grpc communication with the executor . \u00a0\u00a0 The controller\nmust be started after all executors  have been started.\n$ no_proxy=$EXECUTOR_ADDR gpu_assigner --multinode start --scheduler gpu-\naffinity  \n5. Execute agarun \u00a0\u00a0 S tart agarun from the controller  first.\n$ NCCL_SOCKET_IFNAME= $IFNAME no_proxy= $CONTROLLER_ADDR  \nAGA_GPU_ALLOC_SERVER_ADDRESS= $CONTROLLER_ADDR  \nAGA_GPU_ALLOC_SERVER_PORT= $CONTROLLER_PORT  agarun -- torchrun --nnodes 2 --\nnproc-per-node 1 --rdzv-id 123 --rdzv-endpoint $CONTROLLER_ADDR :29400 --\nnode-rank 0 --rdzv-backend static  \nsamples/no_restart/multi_node/multinode_torchrun_aga.py 10 10  \nThen, start agarun on the executor :", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acb70555-da42-4a40-a114-606a5345ed08": {"__data__": {"id_": "acb70555-da42-4a40-a114-606a5345ed08", "embedding": null, "metadata": {"page_label": "44", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb34edf8-c25e-46d7-a531-11f983ea4170", "node_type": "4", "metadata": {"page_label": "44", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "278b2dc12f77b8444a099931ea9e498cbcfbd4358c823b2131b8f1927c2f13da", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n44 / 50 Copyright 2025 Fujitsu Limited$ NCCL_SOCKET_IFNAME= $IFNAME no_proxy= $CONTROLLER_ADDR  \nAGA_GPU_ALLOC_SERVER_ADDRESS= $CONTROLLER_ADDR  \nAGA_GPU_ALLOC_SERVER_PORT= $CONTROLLER_PORT  agarun -- torchrun --nnodes 2 --\nnproc-per-node 1 --rdzv-id 123 --rdzv-endpoint $CONTROLLER_ADDR :29400 --\nnode-rank 1 --rdzv-backend static  \nsamples/no_restart/multi_node/multinode_torchrun_aga.py 10 10  \nExplanation of torchrun arguments and environment variables\nNCCL_SOCKET_IFNAME : Specify the network interface for NC CL communication in torchrun using an\nenvironment variable \u00a0\u00a0\u00a0\u00a0\u00a0 --nnodes : Number of nodes \u00a0\u00a0\u00a0\u00a0\u00a0 --nproc-per-node : Number of processes per\nnode \u00a0\u00a0\u00a0\u00a0\u00a0 --rdzv-id : Job ID for torchrun \u00a0\u00a0\u00a0\u00a0\u00a0 Host specified in --rdzv-endpoint : Specify one of the nodes\nrunning torchrun. In the example above, it is set to $CONTROLLER_ADDR . \u00a0\u00a0\u00a0\u00a0\u00a0 P ort specified in --rdzv-\nendpoint : Specify any port that does not conflict with others. The default is 29400. \u00a0\u00a0\u00a0\u00a0\u00a0 --node-rank : Rank of\nthe node \u00a0\u00a0\u00a0\u00a0\u00a0 --rdzv-backend : Default is c10d, but if it doesn't work, set it to static.\n- Explanation of A CB environment variables\nno_proxy : Set no_proxy  for grpc communication with the controller  \u00a0\u00a0\u00a0\u00a0\u00a0\nAGA_GPU_ALLOC_SERVER_ADDRESS : Address of the controller  \u00a0\u00a0\u00a0\u00a0\u00a0 AGA_GPU_ALLOC_SERVER_PORT : Port of\nthe controller\n- Explanation of arguments for the sample program ( multinode_torchrun_aga.py )\nFirst argument: Number of epochs \u00a0\u00a0\u00a0\u00a0\u00a0 Second argument: Checkpoint frequency (number of epochs)\nExecution R esults\nExecution results on the controller\n[RANK0] Epoch 0 | Batchsize: 32 | Steps: 32  \nEpoch 0 | Training snapshot saved at snapshot.pt  \n[RANK0] Epoch 1 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 2 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 3 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 4 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 5 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 6 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 7 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 8 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 9 | Batchsize: 32 | Steps: 32  \nYour program has exited successfully.  \n`agarun` will release the current device before it exits.  \nexecutor \u3067\u306e \u5b9f\u2f8f\u7d50\u679c", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65028548-74a4-4472-af54-a128df403df5": {"__data__": {"id_": "65028548-74a4-4472-af54-a128df403df5", "embedding": null, "metadata": {"page_label": "45", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f54b5e3-a742-4300-b922-a452e619fa54", "node_type": "4", "metadata": {"page_label": "45", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c76a49f3d6f858fca1a8c5182a187ada9b0a296d37aafeeb99faf1bdca922e11", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n45 / 50 Copyright 2025 Fujitsu Limited[RANK0] Epoch 0 | Batchsize: 32 | Steps: 32  \nEpoch 0 | Training snapshot saved at snapshot.pt  \n[RANK0] Epoch 1 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 2 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 3 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 4 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 5 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 6 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 7 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 8 | Batchsize: 32 | Steps: 32  \n[RANK0] Epoch 9 | Batchsize: 32 | Steps: 32  \nYour program has exited successfully.  \n`agarun` will release the current device before it exits.  \nStopping GPU Assigner\ngpu_assigner  can be terminated via CLI. Note that gpu_assigner  ignores SIGHUP and SIGINT signals, so\nsending these signals to gpu_assigner  will not terminate it.\n$ gpu_assigner stop                # Stop GPU assigner  \n \n> INFO: Successfully killed GPU assigner process running on 127.0.0.1:11234  \n(pid=13412)  \nExit Codes and Error Messages List\nExit Code Error MessageOccurr ence\nLocationDescr iption\n1 Error: Invalid value {} for {} agarunInvalid value in environment\nvariables used by the agarun\ncommand\n1 Error: Specify your program to run agarunUser program not specified in\nthe agarun command\n(when agarun command is\nexecuted without arguments)\n1Error: F ailed to communicate with the\ngpu assigner.agarunFailed to communicate with\ngpu_assigner\n1 Error: F ailed to execute {} agarun Failed to start the user program\n130 None agarunagarun command terminated\nwith Ctrl-C", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2439518b-dfdc-4052-87be-d5f47493bd16": {"__data__": {"id_": "2439518b-dfdc-4052-87be-d5f47493bd16", "embedding": null, "metadata": {"page_label": "46", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c02b50c7-064b-4259-9278-a76e907ba61d", "node_type": "4", "metadata": {"page_label": "46", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d2ffdc3ad2eaa47c9caea88da50e06a27c9171614a334eaa17316127d81bf9ef", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n46 / 50 Copyright 2025 Fujitsu LimitedExit Code Error MessageOccurr ence\nLocationDescr iption\nUser\nprogram's\nexit codeDepends on the user programUser\nprogram\nexecuted\nwith agarun\ncommandDepends on the user program\n2 Display command line help message gpu_assigner Invalid command arguments\n0WARNING: No command is specified.\nUse -h or --help option for available\ncommands.gpu_assignerNo command specified\n(start/status/stop)\n75ERROR: GPU assigner is already running\non {}.gpu_assignerFailed to start gpu_assigner\n(already running)\n73ERROR: Could not create the default\ndirectory of A CB (path={}).gpu_assignerFailed to create $HOME/.acb\nduring start\nNon-zero ERROR: F ailed to start GPU assigner. gpu_assigner Failed to start gpu_assigner\n69ERROR: No GPU assigner is running on\n{}.gpu_assignerNo running gpu_assigner during\nstatus/stop\n65 ERROR: Could not get a pid from {}. gpu_assignerFailed to get gpu_assigner's\nprocess ID during status/stop\n65ERROR: Could not get a correct pid from\n{}.gpu_assignerFailed to get gpu_assigner's\nprocess ID due to corruption\nduring status/stop\n69WARNING: Could not find GPU assigner\nprocess (already dead).gpu_assignergpu_assigner's process already\nterminated during stop\n(Supplement) Setup Procedure Using Docker\nOverview\nThis chapter explains how to run the AI Computing Broker and user programs on Docker containers. Using\nDocker can reduce the effort required for environment setup and minimize the impact on the host\nenvironment. Additionally, you can verify the operation of sample programs in the repository with just the\ndocker compose build/up  command.\nCommunication with the GPU utilizes the CUD A library within the container and the GPU driver of the host OS.\n!container_gpu_use\nCommunication between containers uses the docker network, allowing operation without opening ports on\nthe host OS. !container_network", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99c3a82f-35f5-4565-b89e-4fd64e5954e9": {"__data__": {"id_": "99c3a82f-35f5-4565-b89e-4fd64e5954e9", "embedding": null, "metadata": {"page_label": "47", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db58c062-1e57-4dcd-bd76-db1b404f9680", "node_type": "4", "metadata": {"page_label": "47", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "2d2cba12cce6fecc99db5e3262f403f62bc34bbc12ab40c2cc1e26a781443045", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n47 / 50 Copyright 2025 Fujitsu LimitedRequirements\nAn nvidia driver compatible with the CUD A runtime installed in the container (PyT orch container: 12.1,\nTensorFlow container: 12.2).\nDocker and NVIDIA Container T oolkit are installed, and the --gpus  flag is specified to access the GPU\nfrom the docker container.\nVerified Environment\nOS: Ubuntu 18.04\nDocker CE: 23.0.4 (Docker Compose: 2.18.1)\nNVIDIA Container T oolkit: 1.10.0-1\nnvidia driver: 525.147.05\nInstalling and V erifying NVIDIA Container T oolkit\nWhen running A CB in a container, in addition to installing Docker, it is necessary to install the NVIDIA\nContainer T oolkit. The installation method is as follows: As descrbed in NVIDIA's Manual .\nYou can verify the operation of the NVIDIA Container T oolkit using the following command.\n$ docker run --gpus all ubuntu:22.04 nvidia-smi  \nFri Aug 30 00:27:18 2024  \n+-----------------------------------------------------------------------------+  \n| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |  \n|-------------------------------+----------------------+----------------------+  \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  \n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  \n|                               |                      |               MIG M. |  \n|===============================+======================+======================|  \n|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |  \n| N/A   45C    P0    99W / 300W |    861MiB / 16384MiB |     52%      Default |  \n|                               |                      |                  N/A |  \n+-------------------------------+----------------------+----------------------+  \n \n+-----------------------------------------------------------------------------+  \n| Processes:                                                                  |  \n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |  \n|        ID   ID                                                   Usage      |  \n|=============================================================================|  \n+-----------------------------------------------------------------------------+  \nIf you encounter the following error, the installation of the NVIDIA Container T oolkit has failed.\n$ docker run --gpus all ubuntu:22.04 nvidia-smi  \ndocker: Error response from daemon: could not select device driver \"\" with  \ncapabilities: [[gpu]].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "334c6bac-e505-4cd3-8fbf-d81c1cca6a27": {"__data__": {"id_": "334c6bac-e505-4cd3-8fbf-d81c1cca6a27", "embedding": null, "metadata": {"page_label": "48", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac0a55f6-c5fa-4ea3-aabe-9efba2577c94", "node_type": "4", "metadata": {"page_label": "48", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b04b4f29f2fa317cef8be99d27c4c60c14b08d81191766d913b86fccdd9f95b0", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n48 / 50 Copyright 2025 Fujitsu LimitedVerification of Inter-Container Communication\nConfirm that the application container can connect to the GPU Assigner endpoint.\nadaptive-gpu-allocator$  cd docker/sample_workflows/run_samples\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose build\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose --\nprofile debug down\n# Delete any running resources if they exist\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose up  \nconnection_test\nIf the output of docker compose up connection_test  is Success: The GPU Assigner endpoint is\nreachable from this container. , then the docker network has been successfully set up. If the output is\nError: Failed to connect your GPU Assigner container. , then the docker network setup has failed,\nor the GPU Assigner process has terminated abnormally. Check for execution errors in the GPU Assigner and\nensure there are no factors obstructing communication within the docker network.\nRegardless of the output, delete the resources created during the verification process.\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose --\nprofile debug down\nBatch V erification of V arious Sample Programs Using Docker Compose\nBy using Docker Compose, you can verify the dynamic allocation of GPUs while running various sample\nprograms in parallel. At this time, the GPU Assigner and various sample programs are started together as\nseparate containers.\nadaptive-gpu-allocator$  cd docker/sample_workflows/run_samples\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose build\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose up\n# Confirm that all sample programs have terminated normally (with only the GPU  \nAssigner container remaining), and interrupt with Ctrl-C.\nadaptive-gpu-allocator/docker/sample_workflows/run_samples$  docker compose down\nBuilding and Running Containers\nBuilding the Image\nadaptive-gpu-allocator$  ./docker/build_images.sh\nIf BuildKit is not enabled, enable it and then run the command.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a80b651c-dd5c-4df7-ad72-f828a566f456": {"__data__": {"id_": "a80b651c-dd5c-4df7-ad72-f828a566f456", "embedding": null, "metadata": {"page_label": "49", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e58335f-84cd-49e6-b87b-866a440c29c8", "node_type": "4", "metadata": {"page_label": "49", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "ba530e726581e17861f74f800032ed59a8d2f6a56390ae68bdd4c26f9622f5ef", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n49 / 50 Copyright 2025 Fujitsu LimitedGPU Assigner and Sample Program Startup Procedure\nNetwork creation, GPU Assigner startup\nadaptive-gpu-allocator$  docker network create aga-network\nadaptive-gpu-allocator$  docker run -d --gpus all --rm --network aga-network --name  \ngpu_assigner gpu_assigner \nRunning PyT orch Sample Program\nadaptive-gpu-allocator$  docker run --gpus all --rm --network aga-network --env  \nAGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner aga_pytorch\nRunning T ensorFlow Sample Program\nadaptive-gpu-allocator$  docker run --gpus all --rm --network aga-network --env  \nAGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner aga_tensorflow\nNotes:\nThe value of AGA_GPU_ALLOC_SERVER_ADDRESS  must match the container name of the GPU Assigner\ncontainer.\nIn the T ensorFlow container, AGA_USE_RESTART=1  is specified. If using the non-restart version of A CB,\nexplicitly set AGA_USE_RESTART=0 . Conversely, if using the restart version of A CB in the PyT orch\ncontainer, explicitly set AGA_USE_RESTART=1 .\nThis procedure uses docker network for communication between the GPU Assigner and user programs.\nIf starting without using docker network, adjust the arguments accordingly\nGPU Assigner T ermination\nadaptive-gpu-allocator$  docker rm -f gpu_assigner \nadaptive-gpu-allocator$  docker network rm aga-network\nRunning User Programs\nAssume that the GPU Assigner container is already running.\nIf using the non-restart version of A CB to start the modified PyT orch application\n$ ls \nmyapp.py  \n$ docker run --gpus all --rm --network aga-network --env", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21722102-7c6e-42e1-9542-f593dc1d0e0a": {"__data__": {"id_": "21722102-7c6e-42e1-9542-f593dc1d0e0a", "embedding": null, "metadata": {"page_label": "50", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3c470a4-352c-4a1f-8c71-efaee67bc11f", "node_type": "4", "metadata": {"page_label": "50", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c98ddcda5de9aedc9c8f9bbb1094101b5f3c5d3b75b8ddc18085553473059823", "class_name": "RelatedNodeInfo"}}, "text": "userguide_english.md\n50 / 50 Copyright 2025 Fujitsu LimitedAGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner -v ` pwd`:/mnt:ro aga_pytorch  \n/mnt/myapp.py\nACB \u306e b. \u518d\u8d77\u52d5 \u3042\u308a \u7248 \u3092 \u2f64 \u3044\u3066\u3001 \u66f8 \u304d \u63db \u3048 \u6e08 \u307f\u306e TensorFlow \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092 \u8d77\u52d5 \u3059\u308b \u5834\u5408\n$ ls \nmyapp.py  \n$ docker run --gpus all --rm --network aga-network --env  \nAGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner -v ` pwd`:/mnt:ro aga_tensorflow  \n/mnt/myapp.py\nCleanup Procedure\nThe method for terminating the running GPU Assigner container is as previously described.\nTo delete resources created using docker compose , execute docker compose down --rmi all --remove-\norphans  in each directory within docker/sample_workflows .\nUse the following command to delete resources generated by ./docker/build_images.sh .\nadaptive-gpu-allocator$  ./docker/remove_images.sh", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"691180ad-7cdf-4476-b462-2714812fbd0e": {"node_ids": ["95dcb29d-bae4-48ec-bbe7-0f6336652c2f"], "metadata": {"page_label": "1", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "47d6c819-b135-4dc0-aa6d-6edc67a647d7": {"node_ids": ["38677a7e-f076-44e2-b1f3-dd3b69718dda"], "metadata": {"page_label": "2", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "44e2d0fa-d1b0-4586-931a-849602cdf3ce": {"node_ids": ["6c94d557-b880-4a05-a33c-767ab3ed7446"], "metadata": {"page_label": "3", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3a52219e-1667-4dfe-a30b-afbf8e704a4c": {"node_ids": ["d77a5302-11d9-4d78-85e0-964589f9e738"], "metadata": {"page_label": "4", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "0dece9a4-6d84-444a-9884-b3d575ccdad2": {"node_ids": ["68c9ed5e-fbac-4d9e-b806-05ba551eac6e"], "metadata": {"page_label": "5", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "cb258cd1-a5fa-4cb5-a918-b7f7e4881546": {"node_ids": ["4901744f-39a1-4f7a-9b1f-48bcd8169715"], "metadata": {"page_label": "6", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "278d5fe0-bda6-4d4b-82c8-2be316c79a18": {"node_ids": ["ffd918f9-dd59-4a94-8141-a8e0ab87bbdf"], "metadata": {"page_label": "7", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "77d45487-e5e7-45d9-98bc-8164e984c349": {"node_ids": ["fe7177a3-dd92-43a9-9d4d-ee03f665a94d"], "metadata": {"page_label": "8", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "68350c4f-4b00-4ce3-a5d4-c7d08358cc3e": {"node_ids": ["41c9f3da-a8ad-452c-a688-994fec6994d3"], "metadata": {"page_label": "9", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "42aa9509-ded0-4cda-806a-58372745ca09": {"node_ids": ["8ff541db-98d6-4bb8-a843-9671e9dbf4d9"], "metadata": {"page_label": "10", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "a47b251f-b26e-4a12-aac7-73a0ce566c73": {"node_ids": ["6f5259ac-ee27-4e4d-acb3-749386b83e55"], "metadata": {"page_label": "11", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "20b36c96-53fb-4bb0-a950-7d59b22852e2": {"node_ids": ["d5a86808-ba4e-4bb1-9f23-20ad29459d94"], "metadata": {"page_label": "12", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "41e6e3d2-698d-4f82-a07b-5c9cadbc75cb": {"node_ids": ["01e1fb78-0777-40a9-8d7e-a3b7c5a2cb24"], "metadata": {"page_label": "13", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "4116d067-14ed-4677-a599-5612229bff32": {"node_ids": ["1d840031-7153-4418-b266-efc54b0772b6"], "metadata": {"page_label": "14", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e038894b-1615-4281-9ecb-e33bbbebebfa": {"node_ids": ["b7d31b2e-5a23-4da7-b5a9-2df4c6fdf35c"], "metadata": {"page_label": "15", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "fdaa40cf-e999-489f-8aa2-67f42a846d70": {"node_ids": ["f4facc20-e577-4fff-b7f2-4bb5bb923822"], "metadata": {"page_label": "16", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "aefc5c38-7eb2-48f2-86c4-9f7d28e01f88": {"node_ids": ["4ea0cc3c-2d27-45a0-962a-9273aa57817d"], "metadata": {"page_label": "17", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e5927151-5713-45ad-83e4-432b2fba4efc": {"node_ids": ["db430269-d8d0-46ce-8afa-fe1d14045444"], "metadata": {"page_label": "18", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c05a48b2-a873-4943-9d72-b28dab3ee63a": {"node_ids": ["1636df94-ca6d-4f0f-9a73-fa2bbdc88c51"], "metadata": {"page_label": "19", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "d16ffbae-d972-4d57-9044-77eae7472cfc": {"node_ids": ["d77387e8-a24e-4dac-b0f9-165961280b79"], "metadata": {"page_label": "20", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "52813620-09c3-47ef-913b-3d57785beabb": {"node_ids": ["82a21c62-ea2d-4e29-8e9c-bd054dd6a5b5"], "metadata": {"page_label": "21", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3b104bc7-67b4-4738-9cda-049342fd3c5d": {"node_ids": ["1ae4c6eb-0265-4e6e-82a3-0eed2e606052"], "metadata": {"page_label": "22", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ed83d7ca-0484-4a88-8080-20adca13d219": {"node_ids": ["24e80683-9d05-4b37-9f1f-55eac55b23f0"], "metadata": {"page_label": "23", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "b144f4c2-946c-460f-877b-48f1eefb650b": {"node_ids": ["3e52f508-0760-4222-bbd5-6210a0a1d47e"], "metadata": {"page_label": "24", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "98fd8831-f1fe-4eca-82e5-1ee272dc1cf9": {"node_ids": ["d7194299-65eb-4f09-aa09-91be2162a33c"], "metadata": {"page_label": "25", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "83f890e2-6cdb-4bc1-ac16-7978f0810ae6": {"node_ids": ["f93b0a6d-bb07-48a1-bf54-60b79f725505"], "metadata": {"page_label": "26", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "62c6c232-1fc0-4495-861c-f86ea7b02995": {"node_ids": ["3dd06500-42c7-4a11-a4d4-9532edd2999f"], "metadata": {"page_label": "27", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "572be251-e7be-4921-955d-9d7d5f43c7b9": {"node_ids": ["f9788c3a-5102-4c6e-baae-68bbd7842bff"], "metadata": {"page_label": "28", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "d5ae36ab-74bd-446b-bb39-039678271148": {"node_ids": ["47332d78-290d-449f-b119-d012c50dba89"], "metadata": {"page_label": "29", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "f5c95b2a-21cb-4229-808a-f2e5537b8ec5": {"node_ids": ["a871bd97-be48-4423-869e-7cf90a54ad37"], "metadata": {"page_label": "30", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "fe135c7a-9559-4e21-be3c-6a2e3d0521df": {"node_ids": ["e3849e30-c6c3-4327-8300-f5904ed9e6b5"], "metadata": {"page_label": "31", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2c63b837-af89-4512-a1b2-c7e5240eea66": {"node_ids": ["773a9127-14be-4b85-8280-ed1b24ccacea"], "metadata": {"page_label": "32", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "920225d9-772f-4231-88c3-606c50af4422": {"node_ids": ["12df4507-f05a-414e-890e-be4767cb20da"], "metadata": {"page_label": "33", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "25822ed2-3fd4-4f7a-a44c-364dc4b57f81": {"node_ids": ["e0e01798-8da1-49bb-be5a-131c9be9a216"], "metadata": {"page_label": "34", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "b1771bfa-ffc5-41ff-958b-ad96a2245318": {"node_ids": ["264b3773-fcab-4fc3-9654-9a5f9d624ec0"], "metadata": {"page_label": "35", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3388b4ad-8628-4a4b-973f-f2d7ccfdc36c": {"node_ids": ["d534646f-2c79-459d-ba12-6b807ec42780"], "metadata": {"page_label": "36", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ba1363df-32c9-497b-9a5d-5f64be45a202": {"node_ids": ["3a7ebce8-d84f-490f-aa6f-690e84ffda2b"], "metadata": {"page_label": "37", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "5f7ce314-e135-4612-b2bd-56ac4dbbf240": {"node_ids": ["bef9d067-a125-43fa-8093-01c509efd727"], "metadata": {"page_label": "38", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "470d1af1-7d71-4406-abed-d364be33f663": {"node_ids": ["7098a2cf-fd19-46b8-b8b5-034e65ea3b47"], "metadata": {"page_label": "39", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e6b31287-bcb9-4839-a2cc-f550f81c94e5": {"node_ids": ["6031395d-1495-4d34-9cff-ae6430745cc7"], "metadata": {"page_label": "40", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "d48c1980-9844-4ce9-8c6e-8df97b4883e4": {"node_ids": ["bf9cb4b1-e1fa-4fba-9c7c-dafa5f63e101", "bcf5be8e-a206-40c8-87f6-bf04762786e0"], "metadata": {"page_label": "41", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "d8194bf0-14d2-4852-87a8-c0b2ac35a4c1": {"node_ids": ["82a580ed-4242-47c9-858e-5744f124bbc7", "0a72e86e-e4ed-458a-b674-670a4f78cf91", "9115c532-d149-49c3-b641-745a5efe420c"], "metadata": {"page_label": "42", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "da123fb6-ee9c-45a6-8e7f-e8f9290d65d7": {"node_ids": ["a177d4fd-bdd6-4d7d-9b52-7c7656caeb5c"], "metadata": {"page_label": "43", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "eb34edf8-c25e-46d7-a531-11f983ea4170": {"node_ids": ["acb70555-da42-4a40-a114-606a5345ed08"], "metadata": {"page_label": "44", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2f54b5e3-a742-4300-b922-a452e619fa54": {"node_ids": ["65028548-74a4-4472-af54-a128df403df5"], "metadata": {"page_label": "45", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c02b50c7-064b-4259-9278-a76e907ba61d": {"node_ids": ["2439518b-dfdc-4052-87be-d5f47493bd16"], "metadata": {"page_label": "46", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "db58c062-1e57-4dcd-bd76-db1b404f9680": {"node_ids": ["99c3a82f-35f5-4565-b89e-4fd64e5954e9"], "metadata": {"page_label": "47", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ac0a55f6-c5fa-4ea3-aabe-9efba2577c94": {"node_ids": ["334c6bac-e505-4cd3-8fbf-d81c1cca6a27"], "metadata": {"page_label": "48", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6e58335f-84cd-49e6-b87b-866a440c29c8": {"node_ids": ["a80b651c-dd5c-4df7-ad72-f828a566f456"], "metadata": {"page_label": "49", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e3c470a4-352c-4a1f-8c71-efaee67bc11f": {"node_ids": ["21722102-7c6e-42e1-9542-f593dc1d0e0a"], "metadata": {"page_label": "50", "file_name": "userguide_english.pdf", "file_path": "/work/data/userguide_english.pdf", "file_type": "application/pdf", "file_size": 1173715, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}}}