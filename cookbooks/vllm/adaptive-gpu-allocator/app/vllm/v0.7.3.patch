diff --git a/distributed/parallel_state.py b/distributed/parallel_state.py
index 781f870..94f6b18 100644
--- a/distributed/parallel_state.py
+++ b/distributed/parallel_state.py
@@ -194,7 +194,8 @@ class GroupCoordinator:
 
         # TODO: fix it for other platforms
         if current_platform.is_cuda_alike():
-            self.device = torch.device(f"cuda:{local_rank}")
+            from vllm.aga import AGAManager
+            self.device = AGAManager.device
         else:
             self.device = torch.device("cpu")
 
diff --git a/engine/multiprocessing/engine.py b/engine/multiprocessing/engine.py
index ce24aa2..2b9e0c1 100644
--- a/engine/multiprocessing/engine.py
+++ b/engine/multiprocessing/engine.py
@@ -186,6 +186,8 @@ class MQLLMEngine:
 
         while True:
             if not self.engine.has_unfinished_requests():
+                from vllm.aga import AGAManager
+                AGAManager.on_device_end()
                 # Poll until there is work to do.
                 while self.input_socket.poll(timeout=POLLING_TIMEOUT_MS) == 0:
                     # When there's no work, check on engine health and send
diff --git a/worker/worker.py b/worker/worker.py
index ff38e3b..ab9fea4 100644
--- a/worker/worker.py
+++ b/worker/worker.py
@@ -31,6 +31,7 @@ from vllm.worker.model_runner import GPUModelRunnerBase, ModelRunner
 from vllm.worker.pooling_model_runner import PoolingModelRunner
 from vllm.worker.worker_base import (LocalOrDistributedWorkerBase, WorkerBase,
                                      WorkerInput)
+from vllm.aga import AGAManager
 
 logger = init_logger(__name__)
 
@@ -151,8 +152,15 @@ class Worker(LocalOrDistributedWorkerBase):
 
             # This env var set by Ray causes exceptions with graph building.
             os.environ.pop("NCCL_ASYNC_ERROR_HANDLING", None)
-            self.device = torch.device(f"cuda:{self.local_rank}")
-            torch.cuda.set_device(self.device)
+            torch.distributed.init_process_group(
+                backend="gloo",
+                init_method=self.distributed_init_method,
+                world_size=self.parallel_config.world_size,
+                rank=self.rank)
+            # Initialize AGA & begin device use (must be placed before init_worker_distributed_environment)
+            AGAManager.init(world_size=self.parallel_config.world_size, world_rank=self.rank, group=None)
+            AGAManager.on_device_begin()
+            self.device = AGAManager.device
 
             _check_if_gpu_supports_dtype(self.model_config.dtype)
             gc.collect()
@@ -171,6 +179,8 @@ class Worker(LocalOrDistributedWorkerBase):
 
     def load_model(self):
         if self.vllm_config.model_config.enable_sleep_mode:
+            raise RuntimeError("Sleep mode cannot be used with ACB. "
+                               "Please remove the --enable-sleep-mode option")
             allocator = CuMemAllocator.get_instance()
             assert allocator.get_current_usage() == 0, (
                 "Sleep mode can only be "
@@ -181,6 +191,7 @@ class Worker(LocalOrDistributedWorkerBase):
             context = nullcontext()
         with context:
             self.model_runner.load_model()
+            AGAManager.register_model(self.model_runner.model)
 
     def save_sharded_state(
         self,
@@ -306,6 +317,9 @@ class Worker(LocalOrDistributedWorkerBase):
             self._init_cache_engine()
         self._warm_up_model()
 
+        AGAManager.register_kv_cache(self.cache_engine)
+        AGAManager.on_device_end()
+
     def _init_cache_engine(self):
         assert self.cache_config.num_gpu_blocks is not None
         self.cache_engine = [
@@ -490,6 +504,16 @@ class Worker(LocalOrDistributedWorkerBase):
                                                 self.model_config,
                                                 self.parallel_config)
 
+    # override
+    def execute_model(
+        self,
+        execute_model_req = None
+    ):
+        AGAManager.on_device_begin()
+        res = super().execute_model(execute_model_req=execute_model_req)
+        if res is None:
+            AGAManager.on_device_end()
+        return res
 
 def init_worker_distributed_environment(
     vllm_config: VllmConfig,
