{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to Fujitsu AI Computing Broker Documentation","text":"<p>Explore the five core resources designed to help you get the most out of Fujitsu AI Computing Broker. (These documents are also available at TSU-MVPE/aicomputingbroker-docs)</p> \ud83c\udfe0 Quickstart Guide <p>Everything you need to set up and run your first ACB job \u2014 from installation to verifying GPU utilization in 5 minutes.</p> \ud83d\udcd8 User Manual <p>A comprehensive user manual covering ACB features, configuration options, job scheduling, and Docker integration for production environments.</p> \ud83d\udca1 Cookbook <p>   Step-by-step tutorial of utilizing ACB to manage memory oversubscription </p> \ud83d\udda5\ufe0f Multi-Node Setup <p>Comprehensive guide for setting up distributed training across multiple nodes with step-by-step configuration and troubleshooting instructions.</p> \ud83e\udde9 API Reference <p>Documentation of APIs.</p>"},{"location":"acb_manual.html","title":"AI Computing Broker (ACB) User Manual","text":""},{"location":"acb_manual.html#1-introduction","title":"1. Introduction","text":""},{"location":"acb_manual.html#11-overview","title":"1.1. Overview","text":"<p>Fujitsu AI Computing Broker (ACB) is a runtime-aware middleware that optimizes GPU allocation and manages memory oversubscription, resulting in improved efficiency, higher throughput, and reduced computing costs.</p> <p>Key Features:</p> <ul> <li>Runtime-Aware GPU Allocation: Monitors AI frameworks to allocate GPUs dynamically as needed</li> <li>Full Memory Access: Active programs have access to the full GPU memory</li> <li>Advanced Scheduling: Employs techniques like backfilling to optimize job placement and maximize aggregate utilization</li> <li>Fast Deployment: No code changes required in Automatic Mode</li> </ul> <p></p>"},{"location":"acb_manual.html#12-acb-operation-modes","title":"1.2. ACB Operation Modes","text":"<p>ACB supports two primary operation modes:</p> Mode Description Use Case Code Changes Required Automatic Mode Uses <code>agarun</code> driver to automatically manage GPU allocation Standard PyTorch applications None - zero code changes Manual Mode Direct API integration for fine-grained control Performance-critical apps, specialized workflows Minimal - add API calls <p>Both modes support: - Single-GPU jobs: Standard training/inference on one GPU - Multi-GPU DDP (Distributed Data Parallel): Distributed training across multiple GPUs</p>"},{"location":"acb_manual.html#13-quick-start-guide","title":"1.3. Quick Start Guide","text":"What do you want to do? Go to Section Install ACB Section 2: Installation &amp; Setup Start GPU Assigner Section 3.1: Launching GPU Assigner Run existing PyTorch code without changes Section 4: Automatic Mode Fine-tune GPU allocation with API calls Section 5: Manual Mode Train with DDP (multi-GPU) Section 6: DDP Mode Deploy ACB with Docker Section 9: Docker Deployment Troubleshoot errors Section 8: Exit Codes"},{"location":"acb_manual.html#14-recommended-environment","title":"1.4. Recommended Environment","text":"Component Version / Details OS Ubuntu 20.04.4 LTS + GPU Driver Driver: 535.171.04+CUDA: 12.2+ Python Python 3.10+ Python Libraries torch 2.2.1+ <p>Note: Compatibility with newer versions of TensorFlow (2.16+) and Keras 3 is not currently supported. For best results, use the recommended versions listed above.</p>"},{"location":"acb_manual.html#2-installation-setup","title":"2. Installation &amp; Setup","text":""},{"location":"acb_manual.html#21-installation","title":"2.1. Installation","text":"<pre><code>pip install ai-computing-broker\n</code></pre>"},{"location":"acb_manual.html#22-license-setup","title":"2.2. License Setup","text":""},{"location":"acb_manual.html#on-premises-license","title":"On-Premises License","text":"<p>Place the <code>license.lic</code> file in the working directory where you run <code>gpu_assigner</code>. The application will automatically verify the license upon startup.</p>"},{"location":"acb_manual.html#cloud-license","title":"Cloud License","text":"<p>Configure the following environment variable with the API key provided to you: <pre><code>export ACB_KEY=\"YOUR_ACB_KEY_HERE\"\n</code></pre></p> <p>Note: Both <code>license.lic</code> and <code>ACB_KEY</code> are provided by Fujitsu. To request access, please visit: https://en-documents.research.global.fujitsu.com/ai-computing-broker/</p>"},{"location":"acb_manual.html#3-using-gpu-assigner","title":"3. Using GPU Assigner","text":""},{"location":"acb_manual.html#31-launching-gpu-assigner","title":"3.1. Launching GPU Assigner","text":"<p>Before running a user program that supports ACB, start <code>gpu_assigner</code>: Start GPU assigner<pre><code>gpu_assigner start                     \n</code></pre> Output<pre><code>&gt; INFO: Successfully started GPU Assigner on 127.0.0.1:11234\n</code></pre></p> <p>Confirm that it has been started: GPU assigner status<pre><code>gpu_assigner status              \n</code></pre> Output<pre><code>&gt; INFO: GPU assigner is running on 127.0.0.1:11234 (pid=13412)\n</code></pre></p>"},{"location":"acb_manual.html#32-selecting-gpu-devices","title":"3.2. Selecting GPU Devices","text":"<p>In systems with multiple GPUs, ACB allows you to specify which devices should be managed by the GPU Assigner.</p> <p>By default, <code>gpu_assigner</code> allocates all available GPUs to user programs. To restrict ACB to specific devices, use the <code>--gpu-list</code> option when starting the assigner.</p> <ul> <li>Device IDs correspond to those shown by the <code>nvidia-smi</code> command</li> <li>Multiple devices should be comma-separated (e.g., <code>0,1,2</code>)</li> </ul> <p>Example:</p> <p>To start the GPU Assigner using GPU devices 0, 1, and 2:</p> Start GPU assigner with GPU list<pre><code>gpu_assigner start --gpu-list 0,1,2\n</code></pre>"},{"location":"acb_manual.html#33-job-scheduling-modes","title":"3.3. Job Scheduling Modes","text":"<p>The AI Computing Broker (ACB) supports multiple scheduling strategies to optimize GPU utilization across diverse workloads. Each scheduler is designed for specific use cases and can be selected via the <code>--scheduler</code> flag when starting the GPU Assigner.</p>"},{"location":"acb_manual.html#331-simple-first-in-first-out-scheduling-default","title":"3.3.1. <code>simple</code>: First-In, First-Out Scheduling (Default)","text":"<p>This is the default scheduler, operating on a First-In, First-Out (FIFO) basis. Jobs are assigned to GPUs in the order they are submitted. Each job receives dedicated GPU resources, and the scheduler prioritizes completing jobs in their submission order. This scheduler is suitable for single-GPU, single-node workloads where simplicity and predictable job execution order are paramount.</p> <p>Use <code>--scheduler simple</code> to explicitly enable this scheduler.</p>"},{"location":"acb_manual.html#332-gpu-sharing-concurrent-jobs-on-a-single-gpu","title":"3.3.2. <code>gpu-sharing</code>: Concurrent Jobs on a Single GPU","text":"<p>This scheduler enables multiple jobs to share a single GPU, provided their combined memory usage fits within the GPU's capacity. It is ideal for lightweight models or inference tasks that do not require full GPU memory.</p> <p>To use the gpu-sharing scheduler, specify <code>--scheduler gpu-sharing</code> when starting <code>gpu_assigner</code>.</p>"},{"location":"acb_manual.html#333-gpu-affinity-optimized-scheduling-for-multi-gpu-jobs-recommended","title":"3.3.3. <code>gpu-affinity</code>: Optimized Scheduling for Multi-GPU Jobs (Recommended)","text":"<p>The <code>gpu-affinity</code> scheduler is designed for workloads that span multiple GPUs. It builds on FIFO scheduling but introduces two advanced features to improve efficiency:</p> <ul> <li>Backfill Scheduling: Opportunistically fills idle GPU slots with smaller jobs while larger jobs are queued, increasing overall GPU utilization without strictly following job submission order</li> <li>GPU Affinity: Prevents GPU migration during job execution by assigning jobs to specific GPUs, reducing memory overhead and improving performance by maintaining memory locality</li> </ul> <p>This scheduler is ideal for environments where job sizes vary and minimizing memory fragmentation is critical.</p> <p>To enable <code>gpu-affinity</code>, start the GPU Assigner with the following command:</p> Start GPU assigner with gpu affinity scheduling<pre><code>gpu_assigner start --scheduler gpu-affinity\n</code></pre>"},{"location":"acb_manual.html#34-gpu_assigner-cli-reference","title":"3.4. <code>gpu_assigner</code> CLI Reference","text":"<ul> <li>GPU Assigner Options Help: <code>gpu_assigner -h</code></li> </ul> <p>Displays a list of GPU Assigner options.   <pre><code>$ gpu_assigner -h\nusage: gpu_assigner [-h] \n                    [--address ADDRESS] \n                    [--port PORT] \n                    [--pid-file-path PID_FILE_PATH]\n                    {start,status,stop} ...\n\npositional arguments:\n  {start,status,stop}   GPU Assigner commands\n    start               start GPU Assigner\n    status              show the status of GPU Assigner\n    stop                stop GPU Assigner\n\noptions:\n  -h, --help            show this help message and exit\n\nGPU Assigner options:\n  --address ADDRESS     GPU Assigner address (default: 127.0.0.1)\n  --port PORT           GPU Assigner port (default: 11234)\n\nPid File options:\n  --pid-file-path PID_FILE_PATH\n                        pid file path (default: $HOME/.acb/gpu-assigner.pid)\n</code></pre></p> <ul> <li>Changing the Address (<code>--address</code>)</li> </ul> <p>The default address used by GPU Assigner is <code>127.0.0.1</code>.   To use a different address, set it with the <code>--address</code> option at runtime.   The <code>--address</code> option must be specified before the <code>start</code>/<code>status</code>/<code>stop</code> commands.</p> <pre><code>```sh title=\"GPU_assigner options\"\ngpu_assigner --address &lt;IP address&gt; [start|status|stop]\n```\n</code></pre> <ul> <li>Changing the Port Number (<code>--port</code>)</li> </ul> <p>The default port number used by GPU Assigner is <code>11234</code>.   To use a different port number, set it with the <code>--port</code> option at runtime.   The <code>--port</code> option must be specified before the <code>start</code>/<code>status</code>/<code>stop</code> commands.</p> <pre><code>gpu_assigner --port &lt;port number&gt; [start|status|stop]\n</code></pre> <p>When executing a user program using ACB with a custom port, set the port number in the environment variable <code>AGA_GPU_ALLOC_SERVER_PORT</code> before running <code>agarun</code>. For details, refer to section 4.1.</p> <ul> <li>Specifying the PID File (<code>--pid-file-path</code>)</li> </ul> <p>A PID file is created to manage the process ID and state of the GPU Assigner. By default, <code>$HOME/.acb/gpu-assigner.pid</code> is created.   To change the path of the PID file, specify it with the <code>--pid-file-path</code> option at runtime.   The <code>--pid-file-path</code> option must be specified before the <code>start</code>/<code>status</code>/<code>stop</code> commands.</p> <pre><code>gpu_assigner --pid-file-path &lt;PID file path&gt; [start|status|stop]\n</code></pre>"},{"location":"acb_manual.html#35-gpu_assigner-start-arguments","title":"3.5. <code>gpu_assigner start</code> Arguments","text":"<p>Arguments for the <code>gpu_assigner start</code> command should be specified after the <code>start</code> command.</p> <ul> <li>Help for the <code>gpu_assigner start</code> command: <code>gpu_assigner start -h</code></li> </ul> <p>Displays a list of arguments for <code>gpu_assigner start</code>.</p> <pre><code>```\n$ gpu_assigner start -h\nusage: gpu_assigner start [-h] [--exe-est-start-count EXE_EST_START_COUNT]\n                            [--exe-est-latest-count EXE_EST_LATEST_COUNT] [--exe-est-interval EXE_EST_INTERVAL]\n                            [--exe-est-percentile EXE_EST_PERCENTILE]\n                            [--exe-est-default-execution-time EXE_EST_DEFAULT_EXECUTION_TIME]\n                            [--gpu-assigner-working-directory GPU_ASSIGNER_WORKING_DIRECTORY]\n                            [--gpu-assigner-max-workers GPU_ASSIGNER_MAX_WORKERS]\n                            [--gpu-assigner-execution-time GPU_ASSIGNER_EXECUTION_TIME] [--log-path LOG_PATH] [-v]\n                            [--scheduler SCHEDULER] [--gpu-list GPU_LIST]\n                            [--mem-est-start-count MEM_EST_START_COUNT] [--mem-est-interval MEM_EST_INTERVAL]\n                            [--mem-est-data-max-len MEM_EST_DATA_MAX_LEN]\n                            [--mem-est-percentile MEM_EST_PERCENTILE] [--mem-est-fixed-ratio MEM_EST_FIXED_RATIO]\n                            [--mem-est-select-strategy MEM_EST_SELECT_STRATEGY]\n                            [--res-mem-manager-resident-memory-threshold RES_MEM_MANAGER_RESIDENT_MEMORY_THRESHOLD]\n\noptions:\n    -h, --help            show this help message and exit\n\nExecution Time Estimator (exe-est) options:\n    --exe-est-start-count EXE_EST_START_COUNT\n                        the count at which the estimator starts (default: 10)\n    --exe-est-latest-count EXE_EST_LATEST_COUNT\n                        the latest count of execution time which the estimator uses. (default: 100)\n    --exe-est-interval EXE_EST_INTERVAL\n                        the interval at which the estimator updates (default: 10)\n    --exe-est-percentile EXE_EST_PERCENTILE\n                        the percentile used by the estimator. (default: 0.9)\n    --exe-est-default-execution-time EXE_EST_DEFAULT_EXECUTION_TIME\n                        set the default execution time of a job with unknown execution time (msec). (default: 0)\n\nGPU Assigner options:\n    --gpu-assigner-working-directory GPU_ASSIGNER_WORKING_DIRECTORY\n                        working directory of GPU Assigner (default: None)\n    --gpu-assigner-max-workers GPU_ASSIGNER_MAX_WORKERS\n                        max workers of GPU Assigner (default: 128)\n    --gpu-assigner-execution-time GPU_ASSIGNER_EXECUTION_TIME\n                        Execution time of GPU Assigner (default: None)\n\nLog options:\n    --log-path LOG_PATH   log path (default: $HOME/.acb/gpu-assigner.log)\n    -v, --verbose         verbose logging (default: False)\n\nScheduler options:\n    --scheduler SCHEDULER\n                        scheduler type (default: simple)\n    --gpu-list GPU_LIST   allowed GPU device indices (default: None)\n\nPeak Memory Estimator (mem-est) options:\n    --mem-est-start-count MEM_EST_START_COUNT\n                        the count at which the estimator starts (default: 10)\n    --mem-est-interval MEM_EST_INTERVAL\n                        the interval at which the estimator updates (default: 10)\n    --mem-est-data-max-len MEM_EST_DATA_MAX_LEN\n                        the latest count of peak memory usage which the estimator uses. (default: 100)\n    --mem-est-percentile MEM_EST_PERCENTILE\n                        the percentile used by the estimator. (default: 0.99)\n    --mem-est-fixed-ratio MEM_EST_FIXED_RATIO\n                        the ratio by which the estimator multiply the max peak memory usage. (default: 1.1)\n    --mem-est-select-strategy MEM_EST_SELECT_STRATEGY\n                        whether to use max or min for calculating the allocation memory from user-given and\n                        system-detected memories (default: min)\n\nResident memory manager options:\n    --res-mem-manager-resident-memory-threshold RES_MEM_MANAGER_RESIDENT_MEMORY_THRESHOLD\n                        set the resident memory threshold of GPU memory (default: 0.25)\n```\n</code></pre> <ul> <li>Specifying the Log File (<code>--log-path</code>)</li> </ul> <p>The log file created by GPU Assigner is <code>$HOME/.acb/gpu-assigner.log</code> by default.   To change the log file path, specify it with the <code>--log-path</code> option at runtime.</p> <pre><code>gpu_assigner start --log-path &lt;path_to_log_file&gt;\n</code></pre> <ul> <li>Specifying the Scheduler (<code>--scheduler</code>)</li> </ul> <p>Argument to specify the type of scheduler:   - <code>simple</code>: Default scheduler. Use when all jobs are single-GPU jobs.   - <code>gpu-sharing</code>: Use when all jobs are single-GPU jobs and the memory consumption of the jobs is smaller than the memory size of one GPU.</p> <pre><code>```sh\ngpu_assigner start --scheduler gpu-sharing\n```\n</code></pre> <ul> <li> <p><code>gpu-affinity</code>: Use when executing multi-GPU jobs.</p> <pre><code>gpu_assigner start --scheduler gpu-affinity\n</code></pre> </li> <li> <p>Specifying GPU Devices (<code>--gpu-list</code>)</p> </li> </ul> <p>Argument to specify which GPUs to use. Pass a comma-separated list of GPU device indices. The default is to use all available GPUs.</p> <pre><code>gpu_assigner start --gpu-list 0,1,2\n</code></pre> <ul> <li> <p>Execution Time Prediction Feature Options</p> </li> <li> <p><code>--exe-est-default-execution-time &lt;time:int&gt;</code>: Maximum execution time (in milliseconds) used when the job's execution time is not registered (default: 0). Used to find backfillable jobs when the execution time of a job scheduled from the FIFO queue is unknown.</p> </li> <li><code>--exe-est-start-count &lt;start_count:int&gt;</code>: The number of data points required to start estimating the maximum execution time (default: 10)</li> <li><code>--exe-est-latest-count &lt;latest_count:int&gt;</code>: The number of recent data points to use for maximum execution time estimation (default: 100)</li> <li><code>--exe-est-interval &lt;interval:int&gt;</code>: The number of data points between updates to the maximum execution time estimate (default: 10)</li> <li> <p><code>--exe-est-percentile &lt;percentile:float&gt;</code>: The percentile value for maximum execution time estimation (default: 0.9)</p> </li> <li> <p>Runtime Context Memory Management Feature Options</p> </li> <li> <p><code>--res-mem-manager-resident-memory-threshold &lt;threshold:float&gt;</code>: Threshold for limiting GPU migration of single-GPU jobs, specified as the ratio of runtime context memory in GPU memory (default: 0.25). If the amount of context memory in GPU memory is less than the threshold, allocation ignoring GPU Affinity is possible.</p> </li> </ul>"},{"location":"acb_manual.html#4-operation-mode-1-automatic-mode","title":"4. Operation Mode 1: Automatic Mode","text":""},{"location":"acb_manual.html#41-overview","title":"4.1. Overview","text":"<p>Automatic Mode allows you to run unmodified PyTorch programs with ACB by simply using the <code>agarun</code> driver command. This mode requires: - Zero code changes in your application - Setting the <code>AGA_ENABLE_AUTO=1</code> environment variable - Running your program through <code>agarun</code></p>"},{"location":"acb_manual.html#42-basic-usage","title":"4.2. Basic Usage","text":"<p>Execute user programs by passing them as arguments to the driver command:</p> <pre><code>AGA_ENABLE_AUTO=1 agarun python sample.py\n</code></pre> <p>If your program uses command-line options, add <code>--</code> after <code>agarun</code>:</p> <pre><code>AGA_ENABLE_AUTO=1 agarun -- python sample.py --batch-size 32 --epochs 10\n</code></pre>"},{"location":"acb_manual.html#43-environment-variables","title":"4.3. Environment Variables","text":"Variable Description Default Example <code>AGA_ENABLE_AUTO</code> Enable Automatic Mode (required) - <code>1</code> <code>AGA_GPU_ALLOC_SERVER_PORT</code> GPU Assigner port number <code>11234</code> <code>12345</code> <code>AGA_REQUEST_DEVICE</code> Device to request <code>any</code> <code>cuda</code>, <code>cpu</code> <p>Changing the Port Number:</p> <pre><code>AGA_GPU_ALLOC_SERVER_PORT=12345 AGA_ENABLE_AUTO=1 agarun python sample.py\n</code></pre> <p>Specifying Device Type:</p> <ul> <li><code>any</code>: Request GPU, run on CPU if unavailable (default)</li> <li><code>cuda</code> or <code>gpu</code>: Request GPU, wait if none available</li> <li><code>cpu</code>: Use CPU only</li> </ul> <pre><code>AGA_REQUEST_DEVICE=cuda AGA_ENABLE_AUTO=1 agarun python sample.py\n</code></pre>"},{"location":"acb_manual.html#44-agarun-command-reference","title":"4.4. <code>agarun</code> Command Reference","text":"<pre><code>usage: agarun [-h] [--without-aga] [--history-path HISTORY_PATH]\n              [--checkpoints-topdir CHECKPOINTS_TOPDIR]\n              args [args ...]\n\nCLI to run your program with AGA enabled\n\npositional arguments:\n  args                  Command to run with AGA enabled\n\noptions:\n  -h, --help            show this help message and exit\n  --without-aga         Disable GPU scheduling with AGA (for assessment purpose)\n  --history-path HISTORY_PATH\n                        Path of the job history file (default: $HOME/.acb/job-history.log)\n  --checkpoints-topdir CHECKPOINTS_TOPDIR\n                        Directory where checkpoint directories for each job are stored\n</code></pre> <p>Job History Tracking (<code>--history-path</code>):</p> <p>Job history records timestamps and call arguments during ACB job execution, useful for analyzing job execution status and ACB effectiveness.</p> <pre><code>AGA_ENABLE_AUTO=1 agarun --history-path ./my-job-history.log python sample.py\n</code></pre>"},{"location":"acb_manual.html#45-example-running-single-gpu-jobs","title":"4.5. Example: Running Single-GPU Jobs","text":""},{"location":"acb_manual.html#step-1-create-sample-program","title":"Step 1: Create Sample Program","text":"<p>Create <code>sample_pytorch.py</code>:</p> <pre><code>\"\"\"Sample script using Automatic AGA (pytorch version).\"\"\"\n\nimport random\nfrom time import sleep\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# [AGA] When using AGA, use get_raw_device function to get the actual device\ntry:\n    # [AGA] Ensure running in auto AGA mode if the program was launched from agarun\n    from adaptive_gpu_allocator.env import ensure_auto_mode, launched_from_agarun\n    from adaptive_gpu_allocator.pytorch_automatic import get_raw_device as get_device\n\n    if launched_from_agarun():\n        ensure_auto_mode()\nexcept ImportError:\n\n    def get_device(tensor):\n        return tensor.device\n\n\n# Time to sleep for checking GPU auto-release\nsleep_time = 3\n\n\n# Define the network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(20, 10)  # Input layer\n        self.fc3 = nn.Linear(10, 10)\n        self.fc2 = nn.Linear(10, 1)  # Output layer\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        for i in range(20):\n            x = torch.relu(self.fc3(x))\n        x = self.fc2(x)\n        return x\n\n\ndef fix_seeds():\n    # fix seeds for reproducibility\n    seed = 0\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef get_training_data():\n    # Assume we have some data in X_train and y_train\n    X_train = torch.randn(10000000, 20, device=\"cuda\")  # 10000000 samples, 20 features each\n    y_train = torch.randn(10000000, 1, device=\"cuda\")  # 10000000 samples, 1 target value each\n    return X_train, y_train\n\n\ndef init_model_and_optimizer():\n    # Create the network\n    net = Net()\n\n    # Define a Loss function and optimizer\n    optimizer = optim.Adam(net.parameters())\n\n    return net, optimizer\n\n\ndef train_step(model, optimizer, X_train, y_train, epoch):\n    # Zero the gradients\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = model(X_train)\n\n    # Calculate loss\n    criterion = nn.MSELoss()\n    loss = criterion(outputs, y_train)\n\n    # Backward pass and optimization\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item()} on {get_device(loss)}\")\n\n    # Sleep for a few seconds to auto-release GPU\n    print(f\"sleep {sleep_time} sec\")\n    sleep(sleep_time)\n\n\ndef main():\n    fix_seeds()\n\n    # init model and aga client\n    net, optimizer = init_model_and_optimizer()\n\n    X_train, y_train = get_training_data()\n    net.to(device=\"cuda\")\n\n    n_epoch = 5\n    for epoch in range(n_epoch):\n        train_step(net, optimizer, X_train, y_train, epoch)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"acb_manual.html#step-2-run-single-job","title":"Step 2: Run Single Job","text":"<p>Execute the program using <code>agarun</code>:</p> <pre><code>AGA_ENABLE_AUTO=1 agarun python sample_pytorch.py\n</code></pre> <p>Expected Output:</p> <pre><code>Epoch 0, Loss: 1.0307303667068481 on cuda:0\nsleep 3 sec\nEpoch 1, Loss: 1.029717206954956 on cuda:1\nsleep 3 sec\nEpoch 2, Loss: 1.0287171602249146 on cuda:0\nsleep 3 sec\n[...]\n</code></pre> <p>Note how the GPU device changes dynamically between epochs as ACB manages GPU allocation.</p>"},{"location":"acb_manual.html#46-example-running-multiple-jobs","title":"4.6. Example: Running Multiple Jobs","text":"<p>This example shows how ACB manages more jobs than available GPUs (e.g., 3 jobs on 2 GPUs).</p>"},{"location":"acb_manual.html#running-3-processes-in-a-2-gpu-environment","title":"Running 3 Processes in a 2-GPU Environment","text":"<pre><code>AGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log1 &amp;\nAGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log2 &amp;\nAGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log3 &amp;\n</code></pre> <p>Key Observations:</p> <ul> <li>GPUs are allocated dynamically when needed</li> <li>Each process may use different GPUs at different times</li> <li>ACB automatically schedules GPU access to maximize utilization</li> </ul> <p>Sample Output (log1): <pre><code>Epoch 0, Loss: 1.0307303667068481 on cuda:1\nsleep 3 sec\nEpoch 1, Loss: 1.029717206954956 on cuda:1\nsleep 3 sec\nEpoch 2, Loss: 1.0287171602249146 on cuda:1\nsleep 3 sec\nEpoch 3, Loss: 1.0277304649353027 on cuda:0  # GPU switched\nsleep 3 sec\n[...]\n</code></pre></p>"},{"location":"acb_manual.html#5-operation-mode-2-manual-mode","title":"5. Operation Mode 2: Manual Mode","text":""},{"location":"acb_manual.html#51-overview","title":"5.1. Overview","text":"<p>Manual Mode provides direct API integration for fine-grained control over GPU allocation. Use this mode when:</p> <ul> <li>You need maximum performance tuning</li> <li>Your application has specialized GPU workflows</li> <li>You want explicit control over GPU allocation timing</li> </ul> <p>Key Differences from Automatic Mode:</p> Aspect Automatic Mode Manual Mode Code changes None Minimal API calls required Control Automatic Explicit control points Use case Standard applications Performance-critical apps"},{"location":"acb_manual.html#52-integration-steps","title":"5.2. Integration Steps","text":"<p>Follow these steps to integrate ACB Manual Mode into your PyTorch application:</p> <ol> <li>Import <code>PyTorchAdaptiveGPUAllocator</code></li> <li>Import <code>ensure_manual_mode_without_restart</code></li> <li>Execute <code>ensure_manual_mode_without_restart()</code> - validates environment variables</li> <li>Initialize <code>PyTorchAdaptiveGPUAllocator</code> with model and optimizer</li> <li>Mark GPU processing boundaries using <code>on_device_begin()</code> and <code>on_device_end()</code></li> </ol>"},{"location":"acb_manual.html#53-sample-code","title":"5.3. Sample Code","text":"<pre><code>import random\nfrom time import sleep\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# [AGA] Import Manual Mode API\nfrom adaptive_gpu_allocator.env import ensure_manual_mode_without_restart\nfrom adaptive_gpu_allocator.pytorch import PyTorchAdaptiveGPUAllocator\n\n# [AGA] Ensure running in manual mode\nensure_manual_mode_without_restart()\n\n# Global AGA client variable\naga = None\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(20, 10)\n        self.fc3 = nn.Linear(10, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        for i in range(20):\n            x = torch.relu(self.fc3(x))\n        x = self.fc2(x)\n        return x\n\ndef train_step(model, optimizer, X_train, y_train, epoch):\n    global aga\n\n    # [AGA] Mark GPU processing start\n    aga.on_device_begin()\n\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    criterion = nn.MSELoss()\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n    # [AGA] Mark GPU processing end\n    aga.on_device_end()\n\n    print(f\"sleep 3 sec\")\n    sleep(3)\n\ndef main():\n    global aga\n\n    # Initialize model and optimizer\n    net = Net()\n    optimizer = optim.Adam(net.parameters())\n\n    # [AGA] Initialize ACB client\n    aga = PyTorchAdaptiveGPUAllocator(net, optimizer, device=\"cuda\")\n\n    # Prepare training data\n    X_train = torch.randn(10000000, 20, device=\"cuda\")\n    y_train = torch.randn(10000000, 1, device=\"cuda\")\n\n    # Training loop\n    n_epoch = 5\n    for epoch in range(n_epoch):\n        train_step(net, optimizer, X_train, y_train, epoch)\n\n    # [AGA] Finalize ACB\n    aga.finalize()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"acb_manual.html#54-running-manual-mode-programs","title":"5.4. Running Manual Mode Programs","text":"<p>Execute Manual Mode programs using <code>agarun</code> without <code>AGA_ENABLE_AUTO</code>:</p> <pre><code>agarun python manual_sample.py\n</code></pre>"},{"location":"acb_manual.html#55-key-api-methods","title":"5.5. Key API Methods","text":"Method Description When to Use <code>on_device_begin()</code> Mark start of GPU processing Before GPU-intensive computations <code>on_device_end()</code> Mark end of GPU processing After GPU-intensive computations <code>finalize()</code> Clean up ACB resources At program end"},{"location":"acb_manual.html#6-distributed-data-parallel-ddp-mode","title":"6. Distributed Data Parallel (DDP) Mode","text":""},{"location":"acb_manual.html#61-overview","title":"6.1. Overview","text":"<p>ACB supports PyTorch Distributed Data Parallel (DDP) for multi-GPU training. DDP mode works with both Automatic and Manual modes, allowing distributed training across multiple GPUs with ACB's dynamic allocation.</p> <p>Key Features:</p> <ul> <li>Supports <code>torchrun</code> launcher</li> <li>Compatible with NCCL (GPU) and GLOO (CPU) backends</li> <li>Dynamic GPU allocation per process</li> <li>Automatic model and optimizer state management</li> </ul>"},{"location":"acb_manual.html#62-integration-steps-for-ddp","title":"6.2. Integration Steps for DDP","text":"<ol> <li>Import <code>PyTorchDDPAdaptiveGPUAllocator</code></li> <li>Remove explicit GPU specifications (<code>set_device</code>, <code>to</code>, <code>device_ids</code>, etc.)</li> <li>Leave backend empty in <code>init_process_group()</code> - ACB prepares both NCCL and GLOO</li> <li>Use <code>local_rank</code> for process identification</li> <li>Initialize <code>PyTorchDDPAdaptiveGPUAllocator</code> with model, optimizer, world_size, and rank</li> <li>Mark GPU processing boundaries with <code>on_device_begin()</code> and <code>on_device_end()</code></li> <li>Use <code>move_tensor_to_device()</code> instead of <code>.to()</code> for tensor transfers</li> <li>Call <code>aga.finalize()</code> before <code>destroy_process_group()</code></li> </ol>"},{"location":"acb_manual.html#63-sample-ddp-code","title":"6.3. Sample DDP Code","text":"<pre><code>\"\"\"\nUsage:\nagarun -- torchrun --standalone --nproc_per_node=2 ddp_sample.py\n\"\"\"\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\n# [AGA] Import DDP ACB client\nfrom adaptive_gpu_allocator.pytorch_ddp import PyTorchDDPAdaptiveGPUAllocator\n\ndef ddp_setup():\n    # [AGA] Leave backend empty to prepare both NCCL and GLOO\n    init_process_group()\n\nclass Trainer:\n    def __init__(self, model, optimizer, aga=None):\n        self.local_rank = int(os.environ[\"LOCAL_RANK\"])\n        self.model = DDP(model)\n        self.optimizer = optimizer\n        self.aga = aga\n\n    def train_step(self, data, target):\n        # [AGA] Mark GPU processing start\n        self.aga.on_device_begin()\n\n        self.optimizer.zero_grad()\n        output = self.model(data)\n        loss = nn.functional.mse_loss(output, target)\n        loss.backward()\n        self.optimizer.step()\n\n        # [AGA] Mark GPU processing end\n        self.aga.on_device_end()\n\n        return loss.item()\n\ndef main():\n    ddp_setup()\n\n    # Initialize model and optimizer\n    model = nn.Linear(20, 1)\n    optimizer = optim.Adam(model.parameters())\n\n    # [AGA] Initialize DDP ACB client\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n\n    aga = PyTorchDDPAdaptiveGPUAllocator(\n        model,\n        optimizer,\n        world_size=world_size,\n        rank=local_rank,\n        device=\"cuda\"\n    )\n\n    trainer = Trainer(model, optimizer, aga)\n\n    # Training loop\n    for epoch in range(5):\n        # Create sample data\n        data = torch.randn(100, 20)\n        target = torch.randn(100, 1)\n\n        # [AGA] Move tensors to device\n        data = aga.move_tensor_to_device(data)\n        target = aga.move_tensor_to_device(target)\n\n        loss = trainer.train_step(data, target)\n\n        if local_rank == 0:\n            print(f\"Epoch {epoch}, Loss: {loss}\")\n\n    # [AGA] Finalize before destroying process group\n    aga.finalize()\n    destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"acb_manual.html#64-running-ddp-programs","title":"6.4. Running DDP Programs","text":"<p>Use <code>torchrun</code> with <code>agarun</code>:</p> <pre><code>agarun -- torchrun --standalone --nproc_per_node=2 ddp_sample.py\n</code></pre> <p>Parameters:</p> <ul> <li><code>--standalone</code>: Single-node training</li> <li><code>--nproc_per_node=N</code>: Number of processes (typically = number of GPUs)</li> </ul>"},{"location":"acb_manual.html#65-ddp-api-reference","title":"6.5. DDP API Reference","text":""},{"location":"acb_manual.html#pytorchddpadaptivegpuallocator__init__","title":"<code>PyTorchDDPAdaptiveGPUAllocator.__init__()</code>","text":"<p>Arguments:</p> Parameter Type Description Required <code>model</code> <code>torch.nn.Module</code> PyTorch model Yes <code>optimizer</code> <code>torch.optim.Optimizer</code> Optimizer (e.g., Adam) Yes <code>world_size</code> <code>int</code> Total number of processes Yes <code>rank</code> <code>int</code> Process ID (0 to world_size-1) Yes <code>device</code> <code>str</code> Device type: <code>any</code>, <code>cuda</code>, <code>gpu</code>, <code>cpu</code> Yes <code>scheduler</code> <code>torch.optim.lr_scheduler</code> Learning rate scheduler No <p>Example:</p> <pre><code>aga = PyTorchDDPAdaptiveGPUAllocator(\n    model=model,\n    optimizer=optimizer,\n    world_size=int(os.environ[\"WORLD_SIZE\"]),\n    rank=int(os.environ[\"LOCAL_RANK\"]),\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"acb_manual.html#move_tensor_to_devicetensor","title":"<code>move_tensor_to_device(tensor)</code>","text":"<p>Moves a tensor to the appropriate device managed by ACB.</p> <p>Arguments: - <code>tensor</code>: Input tensor</p> <p>Returns: - Tensor on the target device</p> <p>Example: <pre><code>data = aga.move_tensor_to_device(data)\n</code></pre></p>"},{"location":"acb_manual.html#7-stopping-the-gpu-assigner","title":"7. Stopping the GPU Assigner","text":"<p><code>gpu_assigner</code> can be terminated via the CLI.</p> <p>Note that <code>gpu_assigner</code> ignores SIGHUP and SIGINT signals, so sending these signals will not terminate it.</p> <p><pre><code>$ gpu_assigner stop                \n</code></pre> Output<pre><code>&gt; INFO: Successfully killed GPU assigner process running on 127.0.0.1:11234 (pid=13412)\n</code></pre></p>"},{"location":"acb_manual.html#8-exit-codes-error-reference","title":"8. Exit Codes &amp; Error Reference","text":""},{"location":"acb_manual.html#81-agarun-exit-codes","title":"8.1. agarun Exit Codes","text":"Exit Code Error Message Description 1 <code>Error: Invalid value {} for {}</code> Invalid value in environment variables used by agarun 1 <code>Error: Specify your program to run</code> No user program specified (agarun executed without arguments) 1 <code>Error: Failed to communicate with the gpu assigner.</code> Cannot communicate with gpu_assigner (check if it's running) 1 <code>Error: Failed to execute {}</code> Failed to start the user program 130 (None) agarun terminated with Ctrl-C Varies (Depends on program) User program's exit code"},{"location":"acb_manual.html#82-gpu_assigner-exit-codes","title":"8.2. gpu_assigner Exit Codes","text":"Exit Code Error Message Description 0 <code>WARNING: No command is specified...</code> No start/status/stop command specified 2 (Help message displayed) Invalid command arguments 65 <code>ERROR: Could not get a pid from {}.</code> Failed to retrieve process ID during status/stop 65 <code>ERROR: Could not get a correct pid from {}.</code> Process ID corrupted during status/stop 69 <code>ERROR: No GPU assigner is running on {}.</code> No running gpu_assigner found during status/stop 69 <code>WARNING: Could not find GPU assigner process...</code> gpu_assigner already terminated during stop 73 <code>ERROR: Could not create the default directory...</code> Failed to create <code>$HOME/.acb</code> during start 75 <code>ERROR: GPU assigner is already running on {}.</code> gpu_assigner already running during start Non-zero <code>ERROR: Failed to start GPU assigner.</code> Failed to start gpu_assigner"},{"location":"acb_manual.html#9-docker-deployment","title":"9. Docker Deployment","text":""},{"location":"acb_manual.html#91-overview","title":"9.1. Overview","text":"<p>Architecture: - GPU Assigner Container: Runs <code>gpu_assigner</code> on <code>0.0.0.0:11234</code> - User App Container(s): Connects via <code>AGA_GPU_ALLOC_SERVER_ADDRESS</code> - Network: Containers communicate via Docker bridge network</p>"},{"location":"acb_manual.html#92-gpu-assigner-container","title":"9.2. GPU Assigner Container","text":"<p>Dockerfile.gpu_assigner:</p> <pre><code>FROM ubuntu:22.04\nRUN apt-get update &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --no-cache-dir ai-computing-broker\nEXPOSE 11234\nCMD [\"gpu_assigner\", \"--address\", \"0.0.0.0\", \"start\"]\n</code></pre> <p>Key: <code>--address 0.0.0.0</code> enables container-to-container communication (default <code>127.0.0.1</code> blocks external access).</p>"},{"location":"acb_manual.html#93-user-application-containers","title":"9.3. User Application Containers","text":"<p>Automatic Mode - Dockerfile.user_app_auto:</p> <pre><code>FROM ubuntu:22.04\nRUN apt-get update &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --no-cache-dir torch==2.2.1 \"numpy&lt;2\" ai-computing-broker\nENV AGA_ENABLE_AUTO=1\nENTRYPOINT [\"agarun\", \"python3\"]\n</code></pre> <p>Manual Mode - Dockerfile.user_app_manual:</p> <pre><code>FROM ubuntu:22.04\nRUN apt-get update &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --no-cache-dir torch==2.2.1 \"numpy&lt;2\" ai-computing-broker\nENTRYPOINT [\"agarun\", \"python3\"]\n</code></pre> <p>DDP Mode - Dockerfile.user_app_ddp:</p> <pre><code>FROM ubuntu:22.04\nRUN apt-get update &amp;&amp; \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip3 install --no-cache-dir torch==2.2.1 \"numpy&lt;2\" ai-computing-broker\nENTRYPOINT [\"agarun\", \"--\"]\nCMD [\"torchrun\", \"--standalone\", \"--nproc_per_node=2\"]\n</code></pre>"},{"location":"acb_manual.html#94-docker-compose-configuration","title":"9.4. Docker Compose Configuration","text":"<p>docker-compose.yaml:</p> <pre><code>services:\n  gpu_assigner:\n    build:\n      dockerfile: Dockerfile.gpu_assigner\n    container_name: acb_gpu_assigner\n    networks: [acb-network]\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    healthcheck:\n      test: [\"CMD\", \"gpu_assigner\", \"status\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  user_app:\n    build:\n      dockerfile: Dockerfile.user_app_auto\n    depends_on:\n      gpu_assigner:\n        condition: service_healthy\n    environment:\n      - AGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner\n    volumes:\n      - ./user_code:/app:ro\n    working_dir: /app\n    command: train.py\n    networks: [acb-network]\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\nnetworks:\n  acb-network:\n    driver: bridge\n</code></pre> <p>Key Settings: - <code>AGA_GPU_ALLOC_SERVER_ADDRESS=gpu_assigner</code>: Container name resolution - <code>healthcheck</code>: User app waits for GPU Assigner readiness - <code>volumes</code>: Mount user code (read-only recommended)</p>"},{"location":"acb_manual.html#95-running-with-docker","title":"9.5. Running with Docker","text":"<p>Docker Compose:</p> <pre><code>docker compose build\ndocker compose up\n</code></pre> <p>Docker Run:</p> <pre><code># GPU Assigner\ndocker run -d --name acb_gpu_assigner --gpus all --network acb-network acb-gpu-assigner:latest\n\n# User App\ndocker run --rm --gpus all --network acb-network \\\n  -e AGA_GPU_ALLOC_SERVER_ADDRESS=acb_gpu_assigner \\\n  -v $(pwd)/code:/app:ro -w /app \\\n  acb-user-app-auto:latest train.py\n\n# Multiple Jobs\ndocker run -d --name job1 --gpus all --network acb-network \\\n  -e AGA_GPU_ALLOC_SERVER_ADDRESS=acb_gpu_assigner \\\n  -v $(pwd)/code:/app:ro acb-user-app-auto:latest model_a.py\n\ndocker run -d --name job2 --gpus all --network acb-network \\\n  -e AGA_GPU_ALLOC_SERVER_ADDRESS=acb_gpu_assigner \\\n  -v $(pwd)/code:/app:ro acb-user-app-auto:latest model_b.py\n</code></pre> <p>DDP:</p> <pre><code>docker run --rm --gpus all --network acb-network --shm-size=2g \\\n  -e AGA_GPU_ALLOC_SERVER_ADDRESS=acb_gpu_assigner \\\n  -v $(pwd)/code:/app:ro -w /app \\\n  acb-user-app-ddp:latest torchrun --standalone --nproc_per_node=2 ddp_train.py\n</code></pre> <p>Note: <code>--shm-size</code> required for DDP inter-process communication.</p>"},{"location":"acb_manual.html#96-license-port-configuration","title":"9.6. License &amp; Port Configuration","text":"<p>License (Cloud): <pre><code>services:\n  gpu_assigner:\n    environment:\n      - ACB_KEY=YOUR_KEY\n</code></pre></p> <p>License (On-Premises): <pre><code>services:\n  gpu_assigner:\n    volumes:\n      - ./license.lic:/license.lic:ro\n    working_dir: /\n</code></pre></p> <p>Custom Port: <pre><code>services:\n  gpu_assigner:\n    command: [\"gpu_assigner\", \"--address\", \"0.0.0.0\", \"--port\", \"11235\", \"start\"]\n  user_app:\n    environment:\n      - AGA_GPU_ALLOC_SERVER_PORT=11235\n</code></pre></p>"},{"location":"api-reference.html","title":"API Reference","text":"<p>This document provides a comprehensive reference for the AI Computing Broker (ACB) client library APIs. For usage examples and integration guides, please refer to the User Manual.</p> <p>ACB supports two operation modes: - Automatic Mode: Zero code changes required, uses <code>agarun</code> with <code>AGA_ENABLE_AUTO=1</code> - Manual Mode: Direct API integration for fine-grained control (documented below)</p>"},{"location":"api-reference.html#pytorch-api","title":"PyTorch API","text":"<p>The following APIs are available in the <code>adaptive_gpu_allocator.pytorch</code> module for single-GPU PyTorch applications.</p>"},{"location":"api-reference.html#pytorchadaptivegpuallocator","title":"<code>PyTorchAdaptiveGPUAllocator</code>","text":"<p>Main class for integrating ACB with PyTorch applications in Manual Mode.</p>"},{"location":"api-reference.html#__init__model-optimizer-scheduler-device","title":"<code>__init__(model, optimizer, scheduler, device)</code>","text":"<p>Initialize the ACB client for PyTorch.</p> <p>Parameters:</p> Parameter Type Required Default Description <code>model</code> <code>torch.nn.Module</code> No <code>None</code> PyTorch model to manage <code>optimizer</code> <code>torch.optim.Optimizer</code> No <code>None</code> Optimizer (e.g., <code>Adam</code>, <code>SGD</code>) <code>scheduler</code> <code>torch.optim.lr_scheduler</code> No <code>None</code> Learning rate scheduler <code>device</code> <code>str</code> No <code>\"any\"</code> Device type: <code>\"any\"</code>, <code>\"cuda\"</code>, <code>\"gpu\"</code>, <code>\"cpu\"</code> <p>Returns: <code>PyTorchAdaptiveGPUAllocator</code> instance</p> <p>Example: <pre><code>aga = PyTorchAdaptiveGPUAllocator(model, optimizer, device=\"cuda\")\n</code></pre></p>"},{"location":"api-reference.html#on_device_begindevice-memory_usage_hint","title":"<code>on_device_begin(device, memory_usage_hint)</code>","text":"<p>Mark the start of GPU processing and request GPU allocation.</p> <p>Parameters:</p> Parameter Type Required Default Description <code>device</code> <code>str</code> No <code>\"any\"</code> Device to request: <code>\"any\"</code>, <code>\"cuda\"</code>, <code>\"gpu\"</code>, <code>\"cpu\"</code> <code>memory_usage_hint</code> <code>int</code> No <code>None</code> Estimated memory usage in bytes <p>Returns: <code>None</code></p> <p>Example: <pre><code>aga.on_device_begin()\n</code></pre></p>"},{"location":"api-reference.html#on_device_endno_release","title":"<code>on_device_end(no_release)</code>","text":"<p>Mark the end of GPU processing and release the allocated GPU.</p> <p>Parameters:</p> Parameter Type Required Default Description <code>no_release</code> <code>bool</code> No <code>False</code> If <code>True</code>, GPU is not released (deferred until <code>finalize()</code>) <p>Returns: <code>None</code></p> <p>Example: <pre><code>aga.on_device_end()\n</code></pre></p>"},{"location":"api-reference.html#move_tensor_to_devicetensor_data","title":"<code>move_tensor_to_device(tensor_data)</code>","text":"<p>Move a tensor to the currently allocated device.</p> <p>Parameters:</p> Parameter Type Required Description <code>tensor_data</code> <code>torch.Tensor</code> Yes Tensor to move <p>Returns: <code>torch.Tensor</code> - Tensor on the target device</p> <p>Example: <pre><code>X = aga.move_tensor_to_device(X_train)\n</code></pre></p>"},{"location":"api-reference.html#finalize","title":"<code>finalize()</code>","text":"<p>Finalize ACB processing and release all resources. Call this at the end of your program.</p> <p>Parameters: None</p> <p>Returns: <code>None</code></p> <p>Example: <pre><code>aga.finalize()\n</code></pre></p>"},{"location":"api-reference.html#get_device","title":"<code>get_device()</code>","text":"<p>Get the currently allocated device.</p> <p>Parameters: None</p> <p>Returns: <code>str</code> - Device identifier (e.g., <code>\"cuda:0\"</code>, <code>\"cpu\"</code>)</p> <p>Example: <pre><code>device = aga.get_device()\n</code></pre></p>"},{"location":"api-reference.html#pytorch-ddp-api","title":"PyTorch DDP API","text":"<p>The following APIs are available in the <code>adaptive_gpu_allocator.pytorch_ddp</code> module for multi-GPU Distributed Data Parallel applications.</p>"},{"location":"api-reference.html#pytorchddpadaptivegpuallocator","title":"<code>PyTorchDDPAdaptiveGPUAllocator</code>","text":"<p>Extends <code>PyTorchAdaptiveGPUAllocator</code> with support for PyTorch Distributed Data Parallel.</p>"},{"location":"api-reference.html#__init__model-optimizer-scheduler-device-world_size-rank","title":"<code>__init__(model, optimizer, scheduler, device, world_size, rank)</code>","text":"<p>Initialize the ACB client for PyTorch DDP.</p> <p>Parameters:</p> Parameter Type Required Default Description <code>model</code> <code>torch.nn.Module</code> No <code>None</code> PyTorch model to manage <code>optimizer</code> <code>torch.optim.Optimizer</code> No <code>None</code> Optimizer (e.g., <code>Adam</code>, <code>SGD</code>) <code>scheduler</code> <code>torch.optim.lr_scheduler</code> No <code>None</code> Learning rate scheduler <code>device</code> <code>str</code> No <code>\"any\"</code> Device type: <code>\"any\"</code>, <code>\"cuda\"</code>, <code>\"gpu\"</code>, <code>\"cpu\"</code> <code>world_size</code> <code>int</code> Yes - Total number of processes <code>rank</code> <code>int</code> Yes - Process rank (0 to world_size-1) <p>Returns: <code>PyTorchDDPAdaptiveGPUAllocator</code> instance</p> <p>Example: <pre><code>aga = PyTorchDDPAdaptiveGPUAllocator(\n    model, optimizer,\n    world_size=int(os.environ[\"WORLD_SIZE\"]),\n    rank=int(os.environ[\"LOCAL_RANK\"]),\n    device=\"cuda\"\n)\n</code></pre></p>"},{"location":"api-reference.html#inherited-methods","title":"Inherited Methods","text":"<p><code>PyTorchDDPAdaptiveGPUAllocator</code> inherits the following methods from <code>PyTorchAdaptiveGPUAllocator</code>:</p> <ul> <li><code>on_device_begin(device, memory_usage_hint)</code> - Mark GPU processing start</li> <li><code>on_device_end(no_release)</code> - Mark GPU processing end</li> <li><code>move_tensor_to_device(tensor_data)</code> - Move tensor to device</li> <li><code>finalize()</code> - Finalize and release resources</li> <li><code>get_device()</code> - Get current device</li> </ul> <p>Refer to the PyTorch Single-GPU API section above for detailed parameter descriptions.</p>"},{"location":"api-reference.html#environment-variables","title":"Environment Variables","text":"<p>These environment variables can be used to configure ACB behavior:</p> Variable Description Default Example <code>AGA_ENABLE_AUTO</code> Enable Automatic Mode - <code>1</code> <code>AGA_REQUEST_DEVICE</code> Device to request <code>any</code> <code>cuda</code>, <code>cpu</code> <code>AGA_GPU_ALLOC_SERVER_ADDRESS</code> GPU Assigner address <code>127.0.0.1</code> <code>192.168.1.100</code> <code>AGA_GPU_ALLOC_SERVER_PORT</code> GPU Assigner port <code>11234</code> <code>12345</code>"},{"location":"api-reference.html#helper-functions","title":"Helper Functions","text":""},{"location":"api-reference.html#ensure_manual_mode_without_restart","title":"<code>ensure_manual_mode_without_restart()</code>","text":"<p>Validates that the environment is correctly configured for Manual Mode.</p> <p>Module: <code>adaptive_gpu_allocator.env</code></p> <p>Parameters: None</p> <p>Returns: <code>None</code></p> <p>Raises: Exception if environment is not correctly configured</p> <p>Example: <pre><code>from adaptive_gpu_allocator.env import ensure_manual_mode_without_restart\nensure_manual_mode_without_restart()\n</code></pre></p>"},{"location":"api-reference.html#ensure_auto_mode","title":"<code>ensure_auto_mode()</code>","text":"<p>Validates that the environment is correctly configured for Automatic Mode.</p> <p>Module: <code>adaptive_gpu_allocator.env</code></p> <p>Parameters: None</p> <p>Returns: <code>None</code></p> <p>Raises: Exception if environment is not correctly configured</p> <p>Example: <pre><code>from adaptive_gpu_allocator.env import ensure_auto_mode\nif launched_from_agarun():\n    ensure_auto_mode()\n</code></pre></p>"},{"location":"api-reference.html#launched_from_agarun","title":"<code>launched_from_agarun()</code>","text":"<p>Check if the program was launched via <code>agarun</code>.</p> <p>Module: <code>adaptive_gpu_allocator.env</code></p> <p>Parameters: None</p> <p>Returns: <code>bool</code> - <code>True</code> if launched from <code>agarun</code>, <code>False</code> otherwise</p> <p>Example: <pre><code>from adaptive_gpu_allocator.env import launched_from_agarun\nif launched_from_agarun():\n    # ACB-specific code\n    pass\n</code></pre></p>"},{"location":"api-reference.html#see-also","title":"See Also","text":"<ul> <li>User Manual - Complete usage guide with examples</li> <li>Quickstart Guide - Getting started with ACB</li> <li>Cookbook - Common use cases and recipes</li> </ul>"},{"location":"cookbook.html","title":"\ud83d\udca1 Cookbook","text":""},{"location":"cookbook.html#ai-computing-broker-with-vllm","title":"AI Computing Broker with vLLM","text":"<p>This guide demonstrates how to deploy and run the AI Computing Broker (ACB) using the vLLM framework to host several LLMs on a single vLLM server.</p> <p>(This document is also available at TSU-MVPE/aicomputingbroker-docs)</p>"},{"location":"cookbook.html#installation","title":"Installation","text":"<ol> <li> <p>Create a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre>    Note: If a Python environment is already configured, this step may be skipped.</p> </li> <li> <p>Install AI Computing Broker: see \ud83d\ude80 Quickstart Guide</p> </li> <li> <p>Install demo dependencies: <pre><code>pip install llama-index==0.11.2 llama-index-embeddings-ollama llama-index-llms-openai-like\n</code></pre></p> </li> <li> <p>Setting Configuration: Create an <code>.env</code> file under the docker folder: <pre><code># Proxy settings\nhttp_proxy=\nhttps_proxy=\nHTTP_PROXY=\nHTTPS_PROXY=\nno_proxy=localhost,127.0.0.1\nNO_PROXY=localhost,127.0.0.1\n\n# ACB installation directory or one-level above this cookbooks directory\nACB_DIR=/path/to/client/folder\n\n# User permissions (run these commands to get values)\nUID=XXXX  # Get from: id -u\nGID=XXXX  # Get from: id -g\n\n# Hugging Face cache directory\nHF_HOME=/tmp/hf-cache\n# Go to huggingface.co\n# Create a new token with \"Read\" permissions\nHUGGING_FACE_HUB_TOKEN=\"YOUR_HF_TOKEN\"\n\n# GPU scheduler type\nSCHEDULER=gpu-affinity\n</code></pre></p> </li> </ol>"},{"location":"cookbook.html#run-vllm-server-with-acb","title":"Run vLLM Server with ACB","text":"<p>All required Docker containers are provided. For detailed instructions, refer to the Docker section in the user manual.</p> <p>This demo demonstrates usage with <code>Phi-4</code>, <code>TinySwallow-1.5B-Instruct</code>, and <code>TinyLlama-1.1B-Chat-v1.0</code>, integrated with RAG-based data sources.</p> <pre><code>cd docker\ndocker compose up -d\n</code></pre>"},{"location":"cookbook.html#simple-run","title":"Simple Run","text":"<pre><code>python main.py\n</code></pre>"},{"location":"getting-started.html","title":"AI Computing Broker (ACB) \u2014 Quickstart Guide","text":"<p>This guide walks you through the initial setup of the AI Computing Broker (ACB) and helps you run your first job in under 5 minutes.</p> <p>Looking for more details?</p> <p>For a conceptual overview, refer to the Fujitsu Research Portal. For comprehensive documentation, see the User Manual.</p>"},{"location":"getting-started.html#step-1-installation","title":"Step 1: Installation","text":""},{"location":"getting-started.html#install-acb-package","title":"Install ACB Package","text":"<pre><code>pip install ai-computing-broker\n</code></pre>"},{"location":"getting-started.html#set-up-license","title":"Set Up License","text":"<p>Choose one of the following methods:</p> <p>=== \"Cloud License (Recommended)\"</p> <pre><code>Configure the environment variable with your API key:\n```bash\nexport ACB_KEY=\"YOUR_ACB_KEY_HERE\"\n```\n</code></pre> <p>=== \"On-Premises License\"</p> <pre><code>Place the `license.lic` file in the directory where you'll run `gpu_assigner`.\n</code></pre> <p>Requesting a License</p> <p>Both license types are provided by Fujitsu. Request access at the Fujitsu Research Portal.</p>"},{"location":"getting-started.html#step-2-launch-gpu-assigner","title":"Step 2: Launch GPU Assigner","text":"<p>The GPU Assigner service manages GPU allocation across your jobs.</p>"},{"location":"getting-started.html#start-the-service","title":"Start the Service","text":"<pre><code>gpu_assigner start\n</code></pre> <p>Expected Output: <pre><code>INFO: Successfully started GPU Assigner on 127.0.0.1:11234\n</code></pre></p>"},{"location":"getting-started.html#verify-status","title":"Verify Status","text":"<pre><code>gpu_assigner status\n</code></pre> <p>Expected Output: <pre><code>INFO: GPU assigner is running on 127.0.0.1:11234 (pid=13412)\n</code></pre></p> <p>License Required</p> <p>Ensure the license file is present in the working directory or <code>ACB_KEY</code> is set before starting the GPU Assigner.</p>"},{"location":"getting-started.html#step-3-run-your-first-job","title":"Step 3: Run Your First Job","text":""},{"location":"getting-started.html#install-pytorch-if-needed","title":"Install PyTorch (if needed)","text":"<pre><code>pip install torch\n</code></pre>"},{"location":"getting-started.html#create-a-minimal-sample-script","title":"Create a Minimal Sample Script","text":"<p>Create <code>sample_pytorch.py</code>:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom time import sleep\n\n# Simple neural network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(20, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.fc2(torch.relu(self.fc1(x)))\n\n# Setup\nmodel = Net().to(\"cuda\")\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.MSELoss()\n\n# Training data\nX = torch.randn(1000, 20, device=\"cuda\")\ny = torch.randn(1000, 1, device=\"cuda\")\n\n# Training loop\nfor epoch in range(5):\n    optimizer.zero_grad()\n    loss = criterion(model(X), y)\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n    sleep(2)  # Allow ACB to observe idle time\n</code></pre> <p>Using Your Own Code</p> <p>ACB's Automatic Mode works with existing PyTorch code without modifications. Simply run your programs through <code>agarun</code> with <code>AGA_ENABLE_AUTO=1</code>.</p>"},{"location":"getting-started.html#submit-the-job","title":"Submit the Job","text":"<pre><code>AGA_ENABLE_AUTO=1 agarun python sample_pytorch.py\n</code></pre> <p>Expected Output: <pre><code>Epoch 0, Loss: 1.0234\nEpoch 1, Loss: 0.9876\nEpoch 2, Loss: 0.9543\n...\n</code></pre></p>"},{"location":"getting-started.html#step-4-test-multi-job-gpu-sharing","title":"Step 4: Test Multi-Job GPU Sharing","text":"<p>To see ACB's dynamic GPU allocation in action, submit multiple jobs:</p> <pre><code>AGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log1.txt &amp;\nAGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log2.txt &amp;\nAGA_ENABLE_AUTO=1 agarun python sample_pytorch.py &gt;log3.txt &amp;\n</code></pre> <p>ACB will automatically schedule these jobs across available GPUs, maximizing utilization.</p> <p>What's Happening?</p> <ul> <li>Jobs request GPU access during active computation</li> <li>ACB allocates GPUs dynamically based on availability</li> <li>GPUs are released during idle periods (e.g., <code>sleep()</code>)</li> <li>Multiple jobs share GPUs efficiently without conflicts</li> </ul>"},{"location":"getting-started.html#step-5-stop-the-gpu-assigner","title":"Step 5: Stop the GPU Assigner","text":"<p>When you're done, stop the service:</p> <pre><code>gpu_assigner stop\n</code></pre> <p>Expected Output: <pre><code>INFO: Successfully killed GPU assigner process running on 127.0.0.1:11234\n</code></pre></p>"},{"location":"getting-started.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started.html#gpu-assigner-wont-start","title":"GPU Assigner Won't Start","text":"<p>Problem: <code>ERROR: GPU assigner is already running</code></p> <p>Solution: Stop the existing instance first: <pre><code>gpu_assigner stop\ngpu_assigner start\n</code></pre></p>"},{"location":"getting-started.html#license-errors","title":"License Errors","text":"<p>Problem: <code>License validation failed</code></p> <p>Solutions: - Verify <code>ACB_KEY</code> environment variable is set correctly - Ensure <code>license.lic</code> is in the current working directory - Check your license hasn't expired</p>"},{"location":"getting-started.html#connection-errors","title":"Connection Errors","text":"<p>Problem: <code>Error: Failed to communicate with the gpu assigner</code></p> <p>Solutions: - Verify GPU Assigner is running: <code>gpu_assigner status</code> - Check if port 11234 is available: <code>netstat -an | grep 11234</code> - Restart GPU Assigner: <code>gpu_assigner stop &amp;&amp; gpu_assigner start</code></p>"},{"location":"getting-started.html#jobs-not-using-gpu","title":"Jobs Not Using GPU","text":"<p>Problem: Jobs run but don't use GPU</p> <p>Solutions: - Ensure <code>AGA_ENABLE_AUTO=1</code> is set - Verify CUDA is available: <code>python -c \"import torch; print(torch.cuda.is_available())\"</code> - Check GPU Assigner logs: <code>cat $HOME/.acb/gpu-assigner.log</code></p>"},{"location":"getting-started.html#next-steps","title":"Next Steps","text":"<p>Now that you've successfully run ACB, explore these resources:</p> Topic Documentation Automatic Mode Details User Manual: Automatic Mode Manual Mode API API Reference Advanced Scheduling User Manual: Job Scheduling Multi-GPU (DDP) Training User Manual: DDP Mode Docker Deployment User Manual: Docker Deployment Multi-Node Setup Multi-Node Setup Guide Practical Examples Cookbook"},{"location":"getting-started.html#quick-reference","title":"Quick Reference","text":""},{"location":"getting-started.html#essential-commands","title":"Essential Commands","text":"<pre><code># GPU Assigner\ngpu_assigner start              # Start the service\ngpu_assigner status             # Check status\ngpu_assigner stop               # Stop the service\n\n# Running Jobs\nAGA_ENABLE_AUTO=1 agarun python script.py              # Single job\nAGA_ENABLE_AUTO=1 agarun -- python script.py --args   # With arguments\nAGA_REQUEST_DEVICE=cuda agarun python script.py       # Force GPU\n\n# Logs\ncat $HOME/.acb/gpu-assigner.log     # GPU Assigner logs\ncat $HOME/.acb/job-history.log      # Job execution history\n</code></pre>"},{"location":"getting-started.html#environment-variables","title":"Environment Variables","text":"Variable Purpose Example <code>AGA_ENABLE_AUTO</code> Enable Automatic Mode <code>1</code> <code>AGA_REQUEST_DEVICE</code> Request specific device <code>cuda</code>, <code>cpu</code>, <code>any</code> <code>AGA_GPU_ALLOC_SERVER_PORT</code> Custom port <code>12345</code> <code>ACB_KEY</code> Cloud license key <code>your-api-key</code> <p>Need Help? Consult the User Manual or visit the Fujitsu Research Portal.</p>"},{"location":"multinode-setup.html","title":"Multi-Node Setup Manual","text":"<p>This manual provides comprehensive instructions for setting up and running multi-node distributed training with the AI Computing Broker (ACB) and Adaptive GPU Allocator (AGA) system.</p>"},{"location":"multinode-setup.html#overview","title":"Overview","text":"<p>Multi-node distributed training allows you to scale your machine learning workloads across multiple machines, each equipped with one or more GPUs. The ACB/AGA system provides dynamic GPU allocation and resource management across these nodes.</p>"},{"location":"multinode-setup.html#architecture","title":"Architecture","text":"<pre><code>graph TD\n\n    subgraph \"Controller Node\"\n        CGS[GPU Assigner Service]\n        CTP[Training Process&lt;br&gt;torchrun - rank 0]\n    end\n\n    subgraph \"Executor Node 2\"\n        E2GS[GPU Assigner Service]\n        E2TP[Training Process&lt;br&gt;torchrun - rank 2]\n    end\n\n    subgraph \"Executor Node 1\"\n        E1GS[GPU Assigner Service]\n        E1TP[Training Process&lt;br&gt;torchrun - rank 1]\n    end\n\n    %% GPU Assigner Communication - Controller manages all executors\n    CGS &lt;--&gt; E1GS\n    CGS &lt;--&gt; E2GS\n\n    %% Training Process Distribution - Controller distributes to executors\n    CTP --&gt; E1TP\n    CTP --&gt; E2TP\n\n    classDef controller fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef executor fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef service fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef process fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px\n\n    class CGS,CTP controller\n    class E1GS,E2GS,E1TP,E2TP executor</code></pre> <p>Components:</p> <ul> <li>Controller Node: The primary coordination node that manages distributed training coordination</li> <li>Executor Node(s): Additional compute nodes that participate in distributed training  </li> <li>GPU Assigner Service: Background service running on each node to manage GPU resources</li> <li>Training Process: Your actual machine learning training script executed via <code>torchrun</code></li> </ul>"},{"location":"multinode-setup.html#prerequisites","title":"Prerequisites","text":""},{"location":"multinode-setup.html#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Linux (Ubuntu 18.04+ recommended)</li> <li>Python: 3.9 or higher</li> <li>PyTorch: 2.5.1 (specific version required)</li> <li>Network: TCP connectivity between all nodes</li> <li>Privileges: sudo access for system configuration</li> </ul>"},{"location":"multinode-setup.html#required-tools","title":"Required Tools","text":"<p>Install these tools on all nodes:</p> <pre><code># Install jq for JSON processing\nsudo apt-get update\nsudo apt-get install jq\n\n# Optional: create a Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install PyTorch 2.5.1 (exact version required)\npip install torch==2.5.1\n\n# Install ACB/AGA system (replace with actual path)\npip install -e /path/to/adaptive-gpu-allocator/\n</code></pre>"},{"location":"multinode-setup.html#network-configuration","title":"Network Configuration","text":"<p>Ensure the following ports are available and not blocked by firewalls:</p> <ul> <li>GPU Assigner Service Port: Default 11234 (configurable)</li> <li>PyTorch Rendezvous Port: Default 29400 (configurable)</li> </ul> <p>Test connectivity between nodes: <pre><code># Test basic connectivity\nping &lt;target_node_ip&gt;\n\n# Test specific port connectivity\nnc -zv &lt;target_node_ip&gt; &lt;port_number&gt;\n</code></pre></p>"},{"location":"multinode-setup.html#step-1-environment-setup","title":"Step 1: Environment Setup","text":""},{"location":"multinode-setup.html#11-gather-node-information","title":"1.1 Gather Node Information","text":"<p>On each node, collect the required network information:</p> <pre><code># Get IP address\nhostname -I\n\n# Get hostname\nhostname\n</code></pre> <p>Record this information as you'll need it for configuration.</p>"},{"location":"multinode-setup.html#12-create-configuration-file","title":"1.2 Create Configuration File","text":"<p>Create a <code>config.json</code> file with your specific network and training parameters (replace the IP addresses, hostnames, and paths with your actual values):</p> <pre><code>{\n  \"network\": {\n    \"controller\": {\n      \"ip\": \"192.168.1.100\",\n      \"hostname\": \"controller-node\"\n    },\n    \"executor\": {\n      \"ip\": \"192.168.1.101\", \n      \"hostname\": \"executor-node\"\n    },\n    \"ports\": {\n      \"gpu_assigner\": 11234,\n      \"rdzv\": 29400\n    }\n  },\n  \"training\": {\n    \"total_epochs\": 100,\n    \"save_every\": 10,\n    \"rdzv_id\": 456,\n    \"nnodes\": 2,\n    \"nproc_per_node\": 1,\n    \"script_path\": \"path/to/your/training/script.py\"\n  }\n}\n</code></pre>"},{"location":"multinode-setup.html#configuration-parameters","title":"Configuration Parameters","text":"Parameter Description Example Value <code>network.controller.ip</code> Controller node IP address <code>\"192.168.1.100\"</code> <code>network.controller.hostname</code> Controller node hostname <code>\"controller-node\"</code> <code>network.executor.ip</code> Executor node IP address <code>\"192.168.1.101\"</code> <code>network.executor.hostname</code> Executor node hostname <code>\"executor-node\"</code> <code>network.ports.gpu_assigner</code> GPU assigner service port <code>11234</code> <code>network.ports.rdzv</code> PyTorch rendezvous port <code>29400</code> <code>training.total_epochs</code> Total training epochs <code>100</code> <code>training.save_every</code> Checkpoint save frequency (epochs) <code>10</code> <code>training.rdzv_id</code> Unique rendezvous identifier <code>456</code> <code>training.nnodes</code> Total number of nodes <code>2</code> <code>training.nproc_per_node</code> Processes per node (usually 1) <code>1</code> <code>training.script_path</code> Path to training script <code>\"samples/training_script.py\"</code>"},{"location":"multinode-setup.html#step-2-node-configuration","title":"Step 2: Node Configuration","text":"<p>This step configures each node's system settings and generates the required configuration files.</p>"},{"location":"multinode-setup.html#21-extract-configuration-and-create-acb-directory","title":"2.1 Extract Configuration and Create ACB Directory","text":"<p>On all nodes, extract configuration values and create the ACB configuration directory:</p> <pre><code># Ensure config.json exists in current directory\nls config.json\n\n# Extract configuration values from config.json\nCONTROLLER_IP=$(jq -r '.network.controller.ip' config.json)\nEXECUTOR_IP=$(jq -r '.network.executor.ip' config.json)\nCONTROLLER_HOSTNAME=$(jq -r '.network.controller.hostname' config.json)\nEXECUTOR_HOSTNAME=$(jq -r '.network.executor.hostname' config.json)\nGPU_ASSIGNER_PORT=$(jq -r '.network.ports.gpu_assigner' config.json)\n\n# Determine ACB directory path\nif [[ -n \"${SUDO_USER:-}\" ]]; then\n    ACB_DIR=\"/home/$SUDO_USER/.acb\"\nelse\n    ACB_DIR=\"$HOME/.acb\"\nfi\n\n# Create ACB configuration directory\nmkdir -p \"$ACB_DIR\"\necho \"ACB directory created at: $ACB_DIR\"\n\n# Generate multinode.toml configuration file\ncat &gt; \"$ACB_DIR/multinode.toml\" &lt;&lt; EOF\n[$CONTROLLER_HOSTNAME]\ntype = \"controller\"\nipaddr = \"$CONTROLLER_IP\"\nlogfile = \"$ACB_DIR/gpu-assigner.$CONTROLLER_HOSTNAME.log\"\npidfile = \"$ACB_DIR/gpu-assigner.$CONTROLLER_HOSTNAME.pid\"\nport = $GPU_ASSIGNER_PORT\n\n[$EXECUTOR_HOSTNAME]\ntype = \"executor\"\nipaddr = \"$EXECUTOR_IP\"\nlogfile = \"$ACB_DIR/gpu-assigner.$EXECUTOR_HOSTNAME.log\"\npidfile = \"$ACB_DIR/gpu-assigner.$EXECUTOR_HOSTNAME.pid\"\nport = $GPU_ASSIGNER_PORT\nEOF\n\necho \"Generated: $ACB_DIR/multinode.toml\"\n</code></pre>"},{"location":"multinode-setup.html#22-update-system-hosts-file","title":"2.2 Update System Hosts File","text":"<p>Using the variables from step 2.1:</p> <p>On the controller node, add hostname mapping:</p> <pre><code># Remove any existing entries for this hostname\nsudo sed -i \"/^[[:space:]]*[0-9.]*[[:space:]]*$CONTROLLER_HOSTNAME[[:space:]]*$/d\" /etc/hosts\n\n# Add new hostname mapping\necho \"$CONTROLLER_IP $CONTROLLER_HOSTNAME\" | sudo tee -a /etc/hosts\n\n# Verify the entry was added\ngrep \"$CONTROLLER_HOSTNAME\" /etc/hosts\n</code></pre> <p>On the executor node, add hostname mapping:</p> <pre><code># Remove any existing entries for this hostname\nsudo sed -i \"/^[[:space:]]*[0-9.]*[[:space:]]*$EXECUTOR_HOSTNAME[[:space:]]*$/d\" /etc/hosts\n\n# Add new hostname mapping\necho \"$EXECUTOR_IP $EXECUTOR_HOSTNAME\" | sudo tee -a /etc/hosts\n\n# Verify the entry was added\ngrep \"$EXECUTOR_HOSTNAME\" /etc/hosts\n</code></pre>"},{"location":"multinode-setup.html#23-validate-configuration","title":"2.3 Validate Configuration","text":"<p>Verify the configuration files are correct:</p> <pre><code># Check if config.json is valid JSON\njq -e '.' config.json &gt;/dev/null &amp;&amp; echo \"config.json is valid\" || echo \"config.json is invalid\"\n\n# Check if multinode.toml was created\nls -la ~/.acb/multinode.toml\n\n# Display the generated configuration\ncat ~/.acb/multinode.toml\n</code></pre>"},{"location":"multinode-setup.html#step-3-start-gpu-assigner-services","title":"Step 3: Start GPU Assigner Services","text":"<p>IMPORTANT: The GPU Assigner services must be started in a specific order: Executor nodes first, then Controller node.</p>"},{"location":"multinode-setup.html#31-start-executor-node-service","title":"3.1 Start Executor Node Service","text":"<p>On the executor node, start the GPU Assigner service:</p> <pre><code># Extract port from configuration\nGPU_ASSIGNER_PORT=$(jq -r '.network.ports.gpu_assigner' config.json)\n\n# Start GPU Assigner service on executor node\ngpu_assigner --address 0.0.0.0 --port $GPU_ASSIGNER_PORT --multinode start --scheduler gpu-affinity\n\n# Verify the service started successfully\ngpu_assigner status\n\n# Check if the process is running\nps aux | grep gpu_assigner\n\n# Verify port is listening\nss -tlnp | grep $GPU_ASSIGNER_PORT\n</code></pre> <p>Expected output should show the GPU Assigner service running and listening on the specified port.</p>"},{"location":"multinode-setup.html#32-start-controller-node-service","title":"3.2 Start Controller Node Service","text":"<p>On the controller node (only after executor service is running), start the GPU Assigner service:</p> <pre><code># Extract port from configuration\nGPU_ASSIGNER_PORT=$(jq -r '.network.ports.gpu_assigner' config.json)\n\n# Start GPU Assigner service on controller node\ngpu_assigner --address 0.0.0.0 --port $GPU_ASSIGNER_PORT --multinode start --scheduler gpu-affinity\n\n# Verify the service started successfully\ngpu_assigner status\n\n# Check if the process is running\nps aux | grep gpu_assigner\n\n# Verify port is listening\nss -tlnp | grep $GPU_ASSIGNER_PORT\n</code></pre>"},{"location":"multinode-setup.html#33-verify-multi-node-communication","title":"3.3 Verify Multi-Node Communication","text":"<p>Test that the nodes can communicate with each other:</p> <pre><code># From controller node, test connection to executor\nEXECUTOR_IP=$(jq -r '.network.executor.ip' config.json)\nGPU_ASSIGNER_PORT=$(jq -r '.network.ports.gpu_assigner' config.json)\n\nnc -zv $EXECUTOR_IP $GPU_ASSIGNER_PORT\n\n# From executor node, test connection to controller\nCONTROLLER_IP=$(jq -r '.network.controller.ip' config.json)\n\nnc -zv $CONTROLLER_IP $GPU_ASSIGNER_PORT\n</code></pre>"},{"location":"multinode-setup.html#step-4-start-training-processes","title":"Step 4: Start Training Processes","text":"<p>IMPORTANT: Training processes must be started in a specific order: Controller node first, then Executor node(s).</p>"},{"location":"multinode-setup.html#41-set-environment-variables","title":"4.1 Set Environment Variables","text":"<p>On all nodes, set the required environment variables:</p> <pre><code># Extract controller information\nCONTROLLER_IP=$(jq -r '.network.controller.ip' config.json)\nGPU_ASSIGNER_PORT=$(jq -r '.network.ports.gpu_assigner' config.json)\n\n# Set AGA environment variables\nexport AGA_GPU_ALLOC_SERVER_ADDRESS=\"$CONTROLLER_IP\"\nexport AGA_GPU_ALLOC_SERVER_PORT=\"$GPU_ASSIGNER_PORT\"\n\n# Verify environment variables are set\necho \"AGA_GPU_ALLOC_SERVER_ADDRESS=$AGA_GPU_ALLOC_SERVER_ADDRESS\"\necho \"AGA_GPU_ALLOC_SERVER_PORT=$AGA_GPU_ALLOC_SERVER_PORT\"\n</code></pre>"},{"location":"multinode-setup.html#42-start-controller-training-process","title":"4.2 Start Controller Training Process","text":"<p>On the controller node (rank 0), start the training process:</p> <pre><code># Extract training configuration\nCONTROLLER_IP=$(jq -r '.network.controller.ip' config.json)\nMASTER_PORT=$(jq -r '.network.ports.rdzv' config.json)\nSCRIPT_PATH=$(jq -r '.training.script_path' config.json)\nEPOCHS=$(jq -r '.training.total_epochs' config.json)\nSAVE_EVERY=$(jq -r '.training.save_every' config.json)\nNNODES=$(jq -r '.training.nnodes' config.json)\nNPROC=$(jq -r '.training.nproc_per_node' config.json)\nRDZV_ID=$(jq -r '.training.rdzv_id' config.json)\n\n# Set node rank for controller\nRANK=0\n\necho \"Starting training on controller node (rank $RANK)...\"\n\n# Launch training with AGA\nagarun -- torchrun \\\n    --nproc_per_node=\"$NPROC\" \\\n    --nnodes=\"$NNODES\" \\\n    --node_rank=\"$RANK\" \\\n    --master_addr=\"$CONTROLLER_IP\" \\\n    --master_port=\"$MASTER_PORT\" \\\n    --rdzv_id=\"$RDZV_ID\" \\\n    \"$SCRIPT_PATH\" \\\n    \"$EPOCHS\" \\\n    \"$SAVE_EVERY\"\n</code></pre>"},{"location":"multinode-setup.html#43-start-executor-training-process","title":"4.3 Start Executor Training Process","text":"<p>On the executor node (rank 1), after the controller has started, start the training process:</p> <pre><code># Extract training configuration (same as controller)\nCONTROLLER_IP=$(jq -r '.network.controller.ip' config.json)\nMASTER_PORT=$(jq -r '.network.ports.rdzv' config.json)\nSCRIPT_PATH=$(jq -r '.training.script_path' config.json)\nEPOCHS=$(jq -r '.training.total_epochs' config.json)\nSAVE_EVERY=$(jq -r '.training.save_every' config.json)\nNNODES=$(jq -r '.training.nnodes' config.json)\nNPROC=$(jq -r '.training.nproc_per_node' config.json)\nRDZV_ID=$(jq -r '.training.rdzv_id' config.json)\n\n# Set node rank for executor\nRANK=1\n\necho \"Starting training on executor node (rank $RANK)...\"\n\n# Launch training with AGA\nagarun -- torchrun \\\n    --nproc_per_node=\"$NPROC\" \\\n    --nnodes=\"$NNODES\" \\\n    --node_rank=\"$RANK\" \\\n    --master_addr=\"$CONTROLLER_IP\" \\\n    --master_port=\"$MASTER_PORT\" \\\n    --rdzv_id=\"$RDZV_ID\" \\\n    \"$SCRIPT_PATH\" \\\n    \"$EPOCHS\" \\\n    \"$SAVE_EVERY\"\n</code></pre>"},{"location":"multinode-setup.html#step-5-monitoring-and-management","title":"Step 5: Monitoring and Management","text":""},{"location":"multinode-setup.html#51-monitor-training-progress","title":"5.1 Monitor Training Progress","text":"<p>Check training status on any node:</p> <pre><code># Check running training processes\nps aux | grep -E \"(torchrun|python.*training)\"\n\n# Monitor GPU usage\nnvidia-smi\n\n# Check AGA logs\ntail -f ~/.acb/gpu-assigner.*.log\n\n# Monitor network connections\nss -tlnp | grep -E \"(11234|29400)\"\n</code></pre>"},{"location":"multinode-setup.html#52-check-gpu-assigner-status","title":"5.2 Check GPU Assigner Status","text":"<p>Verify GPU Assigner service status:</p> <pre><code># Check service status\ngpu_assigner status\n\n# View detailed process information\nps aux | grep gpu_assigner\n\n# Check log files for errors\ntail -f ~/.acb/gpu-assigner.*.log\n</code></pre>"},{"location":"multinode-setup.html#step-6-shutdown-procedure","title":"Step 6: Shutdown Procedure","text":""},{"location":"multinode-setup.html#61-stop-training-processes","title":"6.1 Stop Training Processes","text":"<p>Training processes typically stop automatically when training completes. To manually stop:</p> <pre><code># Find training processes\nps aux | grep torchrun\n\n# Stop gracefully (replace PID with actual process ID)\nkill -TERM &lt;PID&gt;\n\n# Force stop if necessary (last resort)\nkill -9 &lt;PID&gt;\n</code></pre>"},{"location":"multinode-setup.html#62-stop-gpu-assigner-services","title":"6.2 Stop GPU Assigner Services","text":"<p>Stop GPU Assigner services in order: Controller first, then Executor(s):</p> <p>On controller node: <pre><code>gpu_assigner --multinode stop\n\n# Verify it stopped\ngpu_assigner status\n</code></pre></p> <p>On executor node: <pre><code>gpu_assigner --multinode stop\n\n# Verify it stopped\ngpu_assigner status\n</code></pre></p>"},{"location":"multinode-setup.html#63-clean-up-resources","title":"6.3 Clean Up Resources","text":"<p>Remove any temporary files if needed:</p> <pre><code># Optional: Clean up PID files\nrm -f ~/.acb/gpu-assigner.*.pid\n\n# Optional: Archive log files\nmv ~/.acb/gpu-assigner.*.log ~/.acb/logs/archive/\n</code></pre>"},{"location":"multinode-setup.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"multinode-setup.html#1-service-fails-to-start-with-port-already-in-use-error","title":"1. Service fails to start with \"Port already in use\" error","text":"<p>Likely cause: Port Conflicts</p> <p>Solution: <pre><code># Check what's using the port\nsudo lsof -i :11234\nsudo lsof -i :29400\n\n# Kill conflicting process (replace &lt;PID&gt; with actual process ID)\nsudo kill &lt;PID&gt;\n\n# Or change ports in config.json to use different port numbers\n</code></pre></p>"},{"location":"multinode-setup.html#2-nodes-cannot-connect-to-each-other","title":"2. Nodes cannot connect to each other","text":"<p>Likely cause: Connection Refused Errors</p> <p>Solution: <pre><code># Test network connectivity\nping &lt;target_node_ip&gt;\nnc -zv &lt;target_node_ip&gt; &lt;port&gt;\n\n# Check firewall settings\nsudo ufw status\nsudo iptables -L\n\n# Verify services are listening\nss -tlnp | grep &lt;port&gt;\n</code></pre></p>"},{"location":"multinode-setup.html#3-services-or-training-fail-to-start-or-coordinate","title":"3. Services or training fail to start or coordinate","text":"<p>Likely cause: Wrong Startup Order</p> <p>Solution:</p> <p>Step 1: GPU Assigner Services</p> <ol> <li>Start executor nodes first</li> <li>Start controller node second</li> </ol> <p>Step 2: Training Processes</p> <ol> <li>Start controller node first</li> <li>Start executor nodes second</li> </ol> <p>Recovery: Stop all services completely, then restart following steps above</p>"},{"location":"multinode-setup.html#4-services-fail-to-start-or-find-each-other","title":"4. Services fail to start or find each other","text":"<p>Likely cause: Configuration File Errors</p> <p>Solution: <pre><code># Validate JSON syntax\njq -e '.' config.json\n\n# Check required fields exist\njq -r '.network.controller.ip // \"MISSING\"' config.json\njq -r '.network.executor.ip // \"MISSING\"' config.json\n\n# Test IP addresses are reachable\nping $(jq -r '.network.controller.ip' config.json)\nping $(jq -r '.network.executor.ip' config.json)\n</code></pre></p>"},{"location":"multinode-setup.html#5-training-starts-but-cannot-allocate-gpus","title":"5. Training starts but cannot allocate GPUs","text":"<p>Likely cause: GPU Assignment Issues</p> <p>Solution: <pre><code># Check GPU status\nnvidia-smi\n\n# Verify environment variables\necho $AGA_GPU_ALLOC_SERVER_ADDRESS\necho $AGA_GPU_ALLOC_SERVER_PORT\n\n# Check service logs\ntail -f ~/.acb/gpu-assigner.*.log\n</code></pre></p>"},{"location":"multinode-setup.html#log-file-locations","title":"Log File Locations","text":"<p>Important log files for debugging:</p> <ul> <li>GPU Assigner Logs: <code>~/.acb/gpu-assigner.&lt;hostname&gt;.log</code></li> <li>Training Output: Console output from torchrun commands</li> <li>System Logs: <code>/var/log/syslog</code> (for system-level issues)</li> </ul>"},{"location":"multinode-setup.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered in this troubleshooting section:</p> <ol> <li>Check the log files for detailed error messages</li> <li>Verify all prerequisite software is installed correctly</li> <li>Ensure network connectivity between all nodes</li> <li>Confirm the correct startup sequence was followed</li> <li>Validate configuration file syntax and content</li> </ol> <p>This manual provides a comprehensive guide for setting up multi-node distributed training with ACB/AGA. Follow the steps in order and refer to the troubleshooting section if you encounter any issues.</p>"}]}